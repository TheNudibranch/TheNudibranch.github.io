[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ian Costley",
    "section": "",
    "text": "An Uhinged Introduction to State Space Models\n\n\n\nBayes\n\n\nR\n\n\nStan\n\n\n\nState Space model estimation using Stan as the inference engine\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPolychoric Correlation with Likert Data\n\n\n\nBayes\n\n\nR\n\n\nStan\n\n\n\nPolychoric correlation using a bayesian framework in Stan.\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Decision Analysis with Poisson Regression and Metropolis-Hastings\n\n\n\nBayes\n\n\nR\n\n\n\nBayesian poisson regression with a Metropolis Hastings sampler\n\n\n\n\n\n\nMar 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Beta-Binomial Model\n\n\n\nBayes\n\n\nJavascript\n\n\n\nProvide some priors/data and what you’re conjugate posteriors update in real-time!\n\n\n\n\n\n\nOct 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRock Paper Scissors Robots\n\n\n\nArduino\n\n\n\nRock paper scissors with a 3D printed wireless hand!\n\n\n\n\n\n\nMay 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow JS\n\n\n\nJavascript\n\n\n\nJust a simple webapp showing TF JS functionality for building simple models in the browser\n\n\n\n\n\n\nJul 27, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/state_space/index.html",
    "href": "posts/state_space/index.html",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "",
    "text": "Code\nlibrary(cmdstanr)\nlibrary(kableExtra)\noptions(mc.cores = parallel::detectCores())\n\nstate_space_utils &lt;- new.env()\nsource('state_space_model_utilities.R', state_space_utils)\n\n\nseason_length &lt;- 7; number_seasons &lt;- 10\nn_tot &lt;- season_length * number_seasons\nx1 &lt;- (cumsum(rnorm(n_tot, 0,0.5)) + rnorm(n_tot, 0,.01)) |&gt; abs()\nx2 &lt;- (cumsum(rnorm(n_tot, 0,0.5)) + rnorm(n_tot, 0,.01)) |&gt; abs()\nx_mat &lt;- cbind(x1,x2)\nbeta_vec &lt;- c(-1,1)\n\nss &lt;- cos(seq(0,2*pi, length.out=season_length))*1\nss &lt;- ss - mean(ss)\nss_vec &lt;- c(ss)[-length(ss)]\nss_var &lt;- 0.01^2;\n\nslope_start &lt;- 0; slope_var &lt;- 1e-30; slope_vec &lt;- c(slope_start)\n\nmu_start &lt;- rnorm(1,0,1); mu_var &lt;- 0.05^2; mu_vec &lt;- c(mu_start)\n\nnoise_var &lt;- 0.5\n\ny_vec &lt;- ss_comp &lt;-  c()\n# i &lt;- 1\nfor (i in 1:n_tot){\n  ss_comp &lt;- c(ss_vec[1], ss_comp)\n  y_vec &lt;- c(y_vec, mu_vec[i] + (x_mat %*% beta_vec)[i] + ss_vec[1] + rnorm(1,0,sqrt(noise_var)))\n  ss_vec &lt;- c(-sum(ss_vec) + rnorm(1, 0, sqrt(ss_var)), ss_vec[-length(ss_vec)])\n  slope_vec &lt;- c(slope_vec, slope_vec[i] + rnorm(1, 0, sqrt(slope_var)))\n  mu_vec &lt;- c(mu_vec, mu_vec[i] + slope_vec[i] + rnorm(1, 0,sqrt(mu_var)))\n}\n\n# Add 20% of series mean to the last 10 iterations\nis_new_prod_live &lt;- seq_along(y_vec) &gt;= length(y_vec) - 9 \ny_vec[is_new_prod_live] &lt;- abs(y_vec[is_new_prod_live])*0.5 + \n  y_vec[is_new_prod_live]\n\ndata.frame(leni=y_vec, abdominable=x1, bigfoot=x2) |&gt; \n  write.csv('state_space_data.csv', row.names = F)"
  },
  {
    "objectID": "posts/state_space/index.html#motivation-more-ice-please",
    "href": "posts/state_space/index.html#motivation-more-ice-please",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "Motivation: More Ice Please",
    "text": "Motivation: More Ice Please\nLet’s say you work for a drinkware company, we’ll call it something generic, maybe “Large bi-pEdal sNow cryptId”? Or LENI for short. LENI sells mugs, wine glasses, water glasses, and tumblers. The recent lack of innovation in the tumbler market has led to public outcry. As a response, you release Tumbler v2™. This exciting new technology features the same size tumblers you know an love, but with double the ice capacity. You even come up with a catchy slogan:\n\n“Double the ice, for the exact same price.” - LENI\n\nAs a result of your genius ideas, your tumbler sales being to pick up:\n\nplot(y_vec, type='n', lwd=3, cex.axis=1.2, cex=1.2, cex.main=1.5, main='Drinkware Company Sales',\n     ylab='', xlab='', xaxt='n', yaxt='n', ylim=range(c(y_vec, x1, x2, ss_comp)))\naxis(side=1, at=mean(seq_along(y_vec)), labels='Time', cex.axis=1.3, lwd.ticks = 0)\naxis(side=2, at=mean(y_vec), labels='Sales ($)', cex.axis=1.3, lwd.ticks = 0)\nlines(x1, lwd=3, col=adjustcolor('darkgoldenrod1', 0.5))\nlines(x2, lwd=3, col=adjustcolor('violet', 0.5))\nlines(y_vec, lwd=3, type='b', pch=16)\n# lines(ss_comp, lwd=3, pch=16, col='gold')\nbox(lwd=2)\nabline(v = length(y_vec)-9.5, lty=2, lwd=3)\nlegend('topleft', legend=c('LENI', 'Abominable Snowman', 'Bigfoot', 'Release of Tumbler v2™'), \n       lty=c(1,1,1,2), lwd=rep(3,4),\n       col=c('black', 'darkgoldenrod', 'violet', 'black'), bg=adjustcolor('white', 0.5), pch=c(16,rep(NA,3)))"
  },
  {
    "objectID": "posts/state_space/index.html#the-final-frontier",
    "href": "posts/state_space/index.html#the-final-frontier",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "The final frontier",
    "text": "The final frontier\nYour tumbler sales are putting all your rival drinkware companies, like “Abominable Snowman”, to shame. But just how much of your sales are incremental and aren’t just due to the usual seasonality, or positive overall industry growth. Can you quantify with some degree of certain the incremental sales you’ve realized from launching Tumbler v2™?\nTo approach this problem, we will use state space models. A class of models that assumes that our data generating process has some underlying state that gives rise to our actually observed variables. For instance, in the time series plot above, a state space approach would assume that each point in the time series is generated from some latent unobserved state which has its own distribution. For this time series analysis state space problem, we will used the widely adopted and used approach of (James Durbin and Koopman 2012). The governing equation for this system is given by:\n\\[\\begin{equation}\ny_t = Z_t\\alpha_t + \\varepsilon_t \\hspace{10mm} \\text{(1)}\n\\end{equation}\\] \\[\\begin{equation}\n\\alpha_{t+1} = T_t \\alpha_t + R_t \\eta_t \\hspace{6mm} \\text{(2)}\n\\end{equation}\\]\nwhere\n\\[\\varepsilon_t \\sim \\mathcal{N}(0,H_t) \\hspace{2mm} \\text{and} \\hspace{2mm} \\eta_t \\sim \\mathcal{N}(0,Q_t)\\] You might only recognize \\(y_t\\), the sales from our Tumbler v2, from the equation above. But allow me to introduce the full cast of characters:\n\n\nCode\nvec_var &lt;- c('$y_t$', '$\\\\alpha_t$', '$\\\\varepsilon_t$', '$\\\\eta_t$', '', '$a_1$')\nvec_dim &lt;- c('$p \\\\times 1$', '$m \\\\times 1$', '$p \\\\times 1$', '$r \\\\times 1$', '', '$m \\\\times 1$')\nvec_desc &lt;- c('Observations', 'State', 'Obs. Disturbance', 'State Disturbance', '', 'Initial State Expected Value')\nmat_vec &lt;- c('$Z_t$', '$T_t$', '$H_t$', '$R_t$', '$Q_t$', '$P_1$')\nmat_dim &lt;- c('$p \\\\times m$', '$m \\\\times m$', '$p \\\\times p$', '$m \\\\times r$', '$r \\\\times r$',\n             '$m \\\\times m$')\nmat_desc &lt;- c('Design Matrix', 'Transition Matrix', 'Obs. Covariance Matrix', 'State Disturbance Selection Matrix',\n              'State Covariance Matrix', 'Initial State Covariance Matrix')\ncbind(vec_var, vec_dim, vec_desc, mat_vec, mat_dim, mat_desc) %&gt;%\n  kable(col.names=NULL) %&gt;% kable_styling(position='center') %&gt;%\n  add_header_above(c('Vectors'=3, 'Matricies' = 3))\n\n\n\n\n\n\n\nVectors\n\n\n\n\nMatricies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(y_t\\)\n\n\n\\(p \\times 1\\)\n\n\nObservations\n\n\n\\(Z_t\\)\n\n\n\\(p \\times m\\)\n\n\nDesign Matrix\n\n\n\n\n\\(\\alpha_t\\)\n\n\n\\(m \\times 1\\)\n\n\nState\n\n\n\\(T_t\\)\n\n\n\\(m \\times m\\)\n\n\nTransition Matrix\n\n\n\n\n\\(\\varepsilon_t\\)\n\n\n\\(p \\times 1\\)\n\n\nObs. Disturbance\n\n\n\\(H_t\\)\n\n\n\\(p \\times p\\)\n\n\nObs. Covariance Matrix\n\n\n\n\n\\(\\eta_t\\)\n\n\n\\(r \\times 1\\)\n\n\nState Disturbance\n\n\n\\(R_t\\)\n\n\n\\(m \\times r\\)\n\n\nState Disturbance Selection Matrix\n\n\n\n\n\n\n\n\n\n\n\\(Q_t\\)\n\n\n\\(r \\times r\\)\n\n\nState Covariance Matrix\n\n\n\n\n\\(a_1\\)\n\n\n\\(m \\times 1\\)\n\n\nInitial State Expected Value\n\n\n\\(P_1\\)\n\n\n\\(m \\times m\\)\n\n\nInitial State Covariance Matrix\n\n\n\n\n.\n\n\nHere \\(\\alpha_t\\) is the underlying latent state that gives rise to our time series realizations \\(y_t\\). As stated before, the latent state \\(\\alpha_t\\) has its own distribution, which we can now deduce from the equation above is given by \\(\\alpha_t \\sim \\mathcal{N}(a_t, Q_t)\\). Notice from our table above that \\(\\alpha_t\\) is not the same size as \\(y_t\\). Our observations \\(y_t\\) is a vector of length \\(p\\), which will be 1 for our case here. Conversely, \\(\\alpha_t\\) is of size \\(m\\). This is because \\(\\alpha_t\\) will act as the conduit for all the effects and assumptions we impose on the model.\nThis is one of the best utilities provided by state space models. We have the ability to add smaller effects together and extract them at the end of the modeling process to further understand the structure of our time series. For example, you might want to impose a 52 week seasonality (1 year), positive industry trend growth, additive regression effects, and maybe even a AR(n) process to account for autocorrelation. All of this information is packed in \\(\\alpha_t\\) which we’ll demonstrate later.\nThe matrix \\(Z_t\\) provides the translation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^p\\), which our case is just one dimension. Therefore, \\(Z_t\\) reduces down to a vector of length \\(m\\), but this is not the case in general. Some of the data that \\(Z_t\\) might hold is the \\(x_t\\) for the regression component of our time series. Usually, \\(Z_t\\) is filled with 1s and 0s to select different components of \\(\\alpha_t\\) that are used for \\(y_t\\). As we’ll see later, there are many components of \\(\\alpha_t\\) that don’t directly impact \\(y_t\\), but are strategically placed to help \\(\\alpha_t\\) itself evolve over time.\nThis is a great segue to our transition matrix \\(T_t\\). The transition matrix is what evolves \\(\\alpha_t\\) to the next state \\(\\alpha_{t+1}\\). This again can take multiple forms. It might be the stage in the process where a positive industry trend is added to the overall series or where we move from season to season.\nFinally we have \\(R_t\\), the state disturbance selection matrix. Notice that everything up to this point has had a \\(t\\) subscript. While we are modeling a time series, it is not true that every element of \\(\\alpha_t\\) has to vary over time. In fact, most of the time the regression coefficients (if included) are usually static with respect to time. The way we tell this model to account for static elements of \\(\\alpha_t\\) is to not add any disturbance to them. Lets consider a very simple local linear trend model:\n\\[\\begin{align}\ny_t &= \\mu_t + \\varepsilon_t\\\\\n\\mu_{t + 1} &= \\mu_{t} + \\nu_{t} + \\eta_{t}\\\\\n\\nu_{t+1} &= \\nu_{t}\n\\end{align}\\]\nOur cast of characters now wear the following costumes:\n\\[\\alpha_t = \\begin{bmatrix} \\mu_t \\\\ \\nu_t\\end{bmatrix} \\hspace{10mm} Z_t = \\begin{bmatrix} 1 & 0\\\\ \\end{bmatrix}\\] \\[T_t = \\begin{bmatrix}1 & 1\\\\ 0 & 1 \\end{bmatrix} \\hspace{5mm} R_t = \\begin{bmatrix}1\\\\ 0\\end{bmatrix}\\] Clearly, \\(\\eta_t\\) is just a 1-D normal with variance \\(Q_t\\), and thus as \\(\\alpha_t\\) evolves over time its element \\(\\nu_{n+1} = \\nu_{n}\\) for all \\(n\\in [1,t]\\). The variable \\(\\nu\\) here could be considered a static industry trend and \\(\\mu_t\\) is the random walk it perturbs. I’m sure you can already see the powerful flexibility we’ve been afforded by this modeling system.\nNow, the only variables I have yet to mention are \\(a_1\\) and \\(P_1\\). These dictate the distribution of our starting state. That is \\(\\alpha_1 \\sim \\mathcal{N}(a_1, P_1)\\). These initial values are assumed to be known at the time of modeling. In reality these are rarely known and there is a large amount of theory to approximate the best starting value of these based on asymptotics. For more information, check out chapter 5 of (James Durbin and Koopman 2012). But, we are Bayesians after all are we not? If we don’t know something we just chuck a prior at it. Although we will abide by the suggestion in chapter 5 and set \\(a_1 = \\bf{0}\\).\nI don’t know if you can tell, but this notation is incredibly loaded, and unfortunately it is only going to get worse before it gets better. The most loaded of them all is understanding when we are talking about a state vector \\(\\alpha_t\\) or the expected value of a state, denoted by \\(a_t\\). To hopefully alleviate some of notation burden, here is a footnote that shows the definition for all \\(\\alpha_t\\) or \\(a_t\\) variations we might encounter.1"
  },
  {
    "objectID": "posts/state_space/index.html#i-prefer-autumn",
    "href": "posts/state_space/index.html#i-prefer-autumn",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "I prefer Autumn",
    "text": "I prefer Autumn\nOne of the key components we’ll utilize is seasonality. This means that for some repeating point in time, we want to see the same, or slightly modified effect. For example, we might observe seasonality in consumer purchasing behavior. Big spikes occur during the holidays when everyone is purchasing Tumbler v2s to give to their ice enthusiast friends at Christmas, but then sales slowly die off. It’s a tale as old as time that we’ll see again next Christmas.\nWe’ll go about this by considering a 4 season example. This might be data that is collected every quarter for a company. We’ll define four season parameters denoted by \\(\\tau_j\\), where \\(j \\in [1,4]\\). We’ll make the assumption that all seasonal effects will add to zero. This makes sense if we assume that a seasonal effect is just some perturbation against the underlying trend. Consider the following:\n\nn_ex1 &lt;- 4*5\nss_comp_ex1 &lt;- cos(seq(0,2*pi, length.out=4))*2\nss_ex1 &lt;- rep(ss_comp_ex1 - mean(ss_comp_ex1),5)\ntrend_ex1 &lt;- cumsum(rnorm(n_ex1) + 0.5)\n\nt_cex &lt;- 1.6\n\npar(mfrow=c(3,1))\n# layout_mat &lt;- matrix(c(1,2,3,3), nrow=2, byrow=T)\n# layout(layout_mat)\npar(mar=c(5,3,2,2)-0.5)\nplot(ss_ex1, type='b', lwd=3, pch=16, xlab='', ylab='',\n     main='Seasonality', xaxt='n', yaxt='n',cex.main=t_cex)\nbox(lwd=2)\nplot(trend_ex1, pch=16, type='b', lwd=3, xlab='', ylab='',\n     main='Underlying Trend', xaxt='n', yaxt='n', cex.main=t_cex)\nbox(lwd=2)\nplot(ss_ex1 + trend_ex1, lwd=3, pch=16, type='b', xlab='', ylab='', \n     main='Observed Trend', xaxt='n', yaxt='n', cex.main=t_cex)\nbox(lwd=2)\n\n\n\n\n\n\n\n\nLet’s write our system in plain Greek and forget about the matrices for a second:\n\\[\\begin{align}\ny_t &= \\mu_t + \\tau_t + \\varepsilon_t\\\\\n\\mu_{t + 1} &= \\mu_{t} + \\eta_{t}^\\mu\\\\\n\\tau_{t+1} &= -\\sum_{j=1}^{s-1}\\tau_{t + 1 - j} + \\eta_t^\\tau\n\\end{align}\\]\nFor some, that summation might have come out of left field. But recall that we defined our seasonality components to sum to zero. Therefore, for any \\(s\\) length seasonality trend, we only need \\(s-1\\) components to recover the full trend. For example if we have a seasonal trend of length 4 and trying to find the expected seasonal component of time \\(t+1\\), we can use the following:\n\\[\\begin{align}\n0 &= \\tau_{t+1} + \\tau_{t} + \\tau_{t-1} + \\tau_{t-2}\\\\\n\\tau_{t+1} &= -(\\tau_{t} + \\tau_{t-1} + \\tau_{t-2})\\\\\n\\tau_{t+1} &= -\\sum_{j=1}^{s-1}\\tau_{t + 1 - j}\\\\\n\\end{align}\\]\nSince we want to allow our seasonality to vary over time, we add a disturbance \\(\\eta_t^\\tau\\) every time the next expected seasonality component \\(\\tau_{t+1}\\) is calculated. Note, since we want both \\(\\mu_t\\) and \\(\\tau_t\\) to vary over time, \\(\\eta_t\\) will be a vector of length 2. Finally, we can write our state space system matrices as:\n\\[\\alpha_t = \\begin{bmatrix} \\mu_t \\\\ \\tau_{t} \\\\ \\tau_{t-1} \\\\ \\tau_{t-2}\\end{bmatrix} \\hspace{10mm} Z_t = \\begin{bmatrix} 1 & 1 & 0 & 0\\\\ \\end{bmatrix}\\] \\[T_t = \\begin{bmatrix}1 & 0 & 0 & 0\\\\ 0 & -1 & -1 & -1\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\end{bmatrix} \\hspace{5mm} R_t = \\begin{bmatrix}1 & 0\\\\ 0 & 1\\\\0 &0\\\\0&0\\end{bmatrix}\\] The 1s in the third and fourth row of \\(T_t\\) ensure that we carry the \\(s-2\\) most recent seasonal components with us to the next state. The 0s in the third and fourth row of \\(R_t\\) select our disturbance draw such that no variation will be applied to the \\(s-2\\) carryover seasonal states."
  },
  {
    "objectID": "posts/state_space/index.html#who-got-a-call-back",
    "href": "posts/state_space/index.html#who-got-a-call-back",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "Who got a call back?",
    "text": "Who got a call back?\nSo now that we’ve hosted the auditions, who should we call back to comprise our final ensemble? Looking at the sales trend line, we see that there are two main competitors: “Abominable Snowman” and “Bigfoot”. We’ll use those two trends as covariates for our state space model. We can omit the slope \\(\\nu_t\\) with our reasoning being that any industry trend will already be captured by including the covariates. Finally, a 7 day seasonality trend will be considered to account for any within week variation. In practice, this can usually be also be omitted if we assume that the covariates can explain this variation as well, but we’ll keep seasonality for demonstration.\nNow, let me introduce the stars of the play:\n\\[\\alpha_t = \\begin{bmatrix} \\mu_t \\\\ \\tau_{t} \\\\ \\tau_{t-1} \\\\ \\tau_{t-2}\\\\ \\tau_{t-3}\\\\ \\tau_{t-4}\\\\ \\tau_{t-5}\\\\ \\beta_a\\\\ \\beta_b\\end{bmatrix} \\hspace{10mm} Z_t = \\begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 & 0 & x_{a} & x_b\\\\ \\end{bmatrix}\\] where \\((\\beta_a, \\beta_b)\\) and \\((x_a, x_b)\\) are the regression coefficients and observed sales trend for “Abominable Snowman” and “Bigfoot”, respectively.\n\\[T_t = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & -1 & -1 & -1 & -1 & -1 & -1 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\\end{bmatrix} \\hspace{5mm}\nR_t = \\begin{bmatrix}1 & 0\\\\ 0 & 1\\\\0 &0\\\\0&0\\\\0&0\\\\0&0\\\\0&0\\\\0&0\\\\0&0\\end{bmatrix}\\]\nSince we are only varying the seasonality and \\(\\mu\\) over time, our disturbance \\(\\eta_t\\) covariance matrix is given by \\(Q_t = \\text{diag}(\\sigma_{\\mu}, \\sigma_{\\tau})\\). Furthermore, since \\(y_t\\) is univariate, our disturbance \\(\\varepsilon_t\\) has scalar variance \\(H_t = \\sigma_y\\)."
  },
  {
    "objectID": "posts/state_space/index.html#mine-is-a-brita",
    "href": "posts/state_space/index.html#mine-is-a-brita",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "Mine is a Brita",
    "text": "Mine is a Brita\nThe first step of our modeling process is filtering.\nFor sake of completeness, let’s review the assumptions2 we made about our data generating process by imposing this system of equations:\n\n\\(y_t\\) is a linear function of the latent state \\(\\alpha_t\\)\nThe disturbances of our data generating process are normally distributed\nThe disturbances of our latent state are normally distributed.\n\nThese unassuming3 assumptions allow us to now leverage the power of the Kalman filter. Without going into too much detail, filtering is just the process of updating what we expect given what we have observed. Assume that we have seen the first 4 realizations of some time series process. We are not in the dark about. We have some knowledge about what the 5th observation might be. If the first 4 had values between 1 and 10, we can be pretty sure that the 5th realization won’t be 1,000. This is of course a gross oversimplification, but if our process follows the assumptions outlined above, we can use the Kalman filter to quantify the uncertainty and expected value of our 5th observation.\nIf we had the time I would write out the derivation for you, but I already feel like I’m going to write too much. Instead, I refer you to page 82-85 of (James Durbin and Koopman 2012) or this cool blog post. Now, you’ve met the star cast but do you know their origin story:\n\\[\n\\begin{align*}\nv_t &= y_t - Z_ta_t & F_t &= Z_t P_t Z^\\prime_t + H_t \\\\\na_{t|t} &= a_t + P_t Z^\\prime_t F^{-1}_t v_t & P_{t|t} &= P_t - P_t Z^\\prime_t F^{-1}_t Z_t P_t \\\\\na_{t+1} &= T_t a_t + K_t v_t & P_{t+1} &= T_t P_t (T_t - K_t Z_t)^\\prime + R_t Q_t R^\\prime_t\n\\end{align*}\n\\] Looks like we picked up some side characters on the way. The matrix \\(K_t\\) is referred to as the Kalman Gain and is given by \\(K_t = T_tP_tZ^\\prime_tF^{-1}_t\\). It encodes how much we should trust a given observation \\(y_t\\). If the gain is high, then the observation uncertainty is low and the next predicted state has a high dependence on the previous observation. If the gain is low, there is high uncertainty in the observation and thus we put most of our weight in the previous predicted state \\(a_t\\). For a more detailed inerpretation, take a look at this stackoverflow post.\nThe matrix \\(F_t\\) is defined as \\(\\textrm{Var}[v_t|Y_{t-1}]\\) and the conditional expected value and variance of the state is defined as:\n\\[\n\\begin{align*}\na_{t|t}&=\\mathbb{E}[\\alpha_t|Y_t] & P_{t|t}&=\\textrm{Var}[\\alpha_t|Y_t] \\\\\n\\end{align*}\n\\] where \\(Y_t = \\{y_1,y_2,\\dots,y_t\\}\\). In practice, you can actually ignore the middle row (\\(*_{t|t}\\)) but we include it for completeness. Now, let’s apply the above equations to produce the filtered expected states \\(a_{t|t}\\). Since we are trying to understand the state of the system prior to our launch of the Tumbler v2™, we will exclude the data post launch from our analysis. Note, the functions used throughout this analysis are loaded into a environment variable state_space_utils in the first hidden block. The full source code for the various functions can be found here under the project directory.\n\ny_vec_no_prod &lt;- y_vec[!is_new_prod_live]\nk_obj &lt;- state_space_utils$forward_backward_pass(\n  y_vec_no_prod, a_1=rep(0,9), \n  P_1 = state_space_utils$iden(9)*0.5,\n  var_vec = c(mu_var, ss_var), \n  noise_var = c(noise_var),\n  params = list('include_slope'=F, x_mat=cbind(x1,x2)[!is_new_prod_live,], n_seasons=7)\n)\n\nfiltered &lt;- y_vec_no_prod - (k_obj$forw$v |&gt; unlist())\nsmoothed &lt;- y_vec_no_prod - (k_obj$back$v_smth |&gt; unlist())\n\nsim &lt;- state_space_utils$simulate_state(\n  k_obj$back$alpha_smth, a_1=rep(0,9), \n  P_1 = state_space_utils$iden(9)*0.5,\n  var_vec = c(mu_var, ss_var), noise_var = c(noise_var),\n  params = list('include_slope'=F, x_mat=cbind(x1,x2)[!is_new_prod_live,], n_seasons=7)\n)\n\nplot(y_vec_no_prod, type='n', lwd=3, cex.axis=1.2, cex=1.2, cex.main=1.5, \n     ylab='', xlab='', xaxt='n', yaxt='n', ylim=range(c(y_vec_no_prod, filtered, smoothed)))\naxis(side=1, at=mean(seq_along(y_vec_no_prod)), labels='Time', cex.axis=1.3, lwd.ticks = 0)\naxis(side=2, at=mean(y_vec_no_prod), labels='Sales ($)', cex.axis=1.3, lwd.ticks = 0)\nlines(filtered, lwd=3, col='darkred')\nlines(y_vec_no_prod, lwd=3, type='b', pch=16)\nbox(lwd=2)\nlegend('topleft', legend=c('LENI Sales', 'LENI Sales Filtered'), lwd=3,col=c('black', 'darkred'), pch=c(16,NA))"
  },
  {
    "objectID": "posts/state_space/index.html#dont-forget-to-moisturize",
    "href": "posts/state_space/index.html#dont-forget-to-moisturize",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "Don’t forget to Moisturize",
    "text": "Don’t forget to Moisturize\nSo you’ve done your filtering, but we don’t want dry flaky equations. For that you’re going to need to moisturize, or as the mathematicians call it: smoothing. You can think of filtering as the forward pass and smoothing as the backward pass. Filtering helped us get the finish line and find the next expected quantity given what we have seen so far. But now that we are at the final time step, we can go backwards and update our states with the entire series of observations, \\(Y_n\\). There are smoothing equations for the smoothed disturbances of \\(\\varepsilon_t\\) and \\(\\eta_t\\), which we will denote \\(\\hat{\\varepsilon}_t\\) and \\(\\hat{\\eta}_t\\) respectively, but we will omit those since we are only concerned with the smoothed state \\(\\hat{\\alpha}_t\\) inference.\nWe define \\(\\hat{\\alpha}_t = \\mathbb{E}[\\alpha_t|Y_n]\\). Note, this is different from \\(a_{t|t}\\) in filtering due to the condition. The series \\(Y_t\\) is defined as the series of observations up to time \\(t\\) given by \\(Y_t = \\{y_1, y_2, \\dots,y_t\\}\\), but \\(Y_n\\) is the entire series of \\(n\\) observations. As with filtering, I omit the proof of the smoothing equations and instead refer you to pages 88-91 of (James Durbin and Koopman 2012). The smoothing equations are relatively simpler, given by\n\\[\n\\begin{align*}\nr_{t-1} &= Z_t^\\prime F_t^{-1} v_t + L_t^\\prime r_t & \\hat{\\alpha_t} &= a_t + P_t r_{t-1}\n\\end{align*}\n\\] What fresh hell is this? We just keep getting more and more cast members? I promise, this was the last audition, they’ll be no more late additions. The matrix \\(L_t\\) is present only for convenience and is defined as \\(L_t = T_t - K_t Z_t\\). As I said before, this is the backward pass. When we start to compute our smoothed states, we will go in reverse following \\(t=n,\\dots,1\\), with \\(r_n = 0\\). Let’s see how our smoothed trend \\(\\hat{\\alpha}_t\\) compares to our filtered trend:\n\nplot(y_vec_no_prod, type='n', lwd=3, cex.axis=1.2, cex=1.2, cex.main=1.5,\n     ylab='', xlab='', xaxt='n', yaxt='n', ylim=range(c(y_vec_no_prod, filtered, smoothed)))\naxis(side=1, at=mean(seq_along(y_vec_no_prod)), labels='Time', cex.axis=1.3, lwd.ticks = 0)\naxis(side=2, at=mean(y_vec_no_prod), labels='Sales ($)', cex.axis=1.3, lwd.ticks = 0)\nlines(filtered, lwd=3, col='darkred')\nlines(smoothed, lwd=3, col='darkblue')\nlines(y_vec_no_prod, lwd=3, type='b', pch=16)\nbox(lwd=2)\nlegend('topleft', legend=c('LENI Sales', 'LENI Sales Filtered', 'LENI Sales Smoothed'), \n       lwd=3,col=c('black', 'darkred', 'darkblue'),\n        bg=adjustcolor('white', 0.5), pch=c(16,NA,NA))\n\n\n\n\n\n\n\n\nYou’ll notice that the largest deviations from filtered to smooth are towards the beginning of the series when the system has the least amount of information."
  },
  {
    "objectID": "posts/state_space/index.html#lots-and-lots-of-understudies",
    "href": "posts/state_space/index.html#lots-and-lots-of-understudies",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "Lots and Lots of Understudies",
    "text": "Lots and Lots of Understudies\nAlright, so we went forward, we went backward, what’s left before show time? Simulating. I’m not one for point estimates. If given the option I’d rather work with a distribution, which works nicely with our chosen Bayesian workflow. Our distribution of interest is \\(P(\\alpha|Y_n)\\), with individual draws denoted as \\(\\tilde{\\alpha}\\). How do we go about sampling this? Luckily, Durbin and Koopman have us covered once again (J. Durbin and Koopman 2002).\nThe first step in simulating is realizing that we have the blueprints to play the system forward from some starting state \\(\\alpha_1\\) if we are given \\(H_t\\), \\(Q_t\\), \\(a_1\\), and \\(P_1\\) (we’ll get to estimating those later). We’ll use the \\(*^+\\) notation for describing draws that have been generated from evolving our system forward with random initializations. For example, \\(\\alpha^+ = [\\alpha_1^+, \\alpha_2^+,\\dots,\\alpha_n^+]\\), where \\(\\alpha_1^+\\) was drawn from \\(\\mathcal{N}(a_1, P_1)\\) and the rest of the states simulated from recursively applying equations \\(\\text{(1)}\\) and \\(\\text{(2)}\\).4 Here are the steps fully laid out for simulating \\(*^+\\):\n\nDraw \\(\\alpha^+_1\\) from the distribution \\(\\mathcal{N}(a_1, P_1)\\)\nSet \\(t=1\\)\nDraw \\(\\varepsilon_t\\) from \\(\\mathcal{N}(0, H_t)\\) and \\(\\nu_t\\) from \\(\\mathcal{N}(0, Q_t)\\)\nSimulate the observation \\(y^+_t\\) using equation \\(\\text{(1)}\\)\nSimulate the next state \\(\\alpha^+_{t+1}\\) using equation \\(\\text{(2)}\\)\nSet \\(t = t+1\\)\nReturn to step 3\n\nNow that we have \\(y^+\\), we can calculate \\(\\hat{\\alpha}^+\\) defined by \\(E[\\alpha|y^+]\\). The equations used for finding \\(\\hat{\\alpha}^+\\) are the same ones we used for filtering and smoothing above. Finally, we may use the culmination of the (J. Durbin and Koopman 2002) paper to yield \\(\\tilde{\\alpha}\\), given by:\n\\[\\tilde{\\alpha} = \\alpha^+ - \\hat{\\alpha}^+ + \\hat{\\alpha}\\] Using the methodology outline above, we can now plot the simulated states:\n\ny_sim &lt;- list()\nalpha_sim &lt;- list()\nz_list &lt;- state_space_utils$gen_Z_list(length(y_vec_no_prod), include_slope=F,\n                                       x_mat=cbind(x1,x2)[!is_new_prod_live,], n_seasons=7)\n\nfor (i in 1:100){\n  sim &lt;- state_space_utils$simulate_state(\n    k_obj$back$alpha_smth, a_1=rep(0,9),\n    P_1 = state_space_utils$iden(9)*0.5,\n    var_vec = c(mu_var, ss_var), noise_var = c(noise_var),\n    params = list('include_slope'=F, x_mat=cbind(x1,x2)[!is_new_prod_live,], n_seasons=7)\n  )\n  alpha_sim[[i]] &lt;- sim\n  y_sim[[i]] &lt;- lapply(1:length(y_vec_no_prod), \\(x) z_list[[x]] %*% sim[[x]]) |&gt; unlist()\n}\n\nplot(y_vec_no_prod, type='n', lwd=3, cex.axis=1.2, cex=1.2, cex.main=1.5,\n     ylab='', xlab='', xaxt='n', yaxt='n', ylim=range(c(y_vec_no_prod, filtered, smoothed)))\naxis(side=1, at=mean(seq_along(y_vec_no_prod)), labels='Time', cex.axis=1.3, lwd.ticks = 0)\naxis(side=2, at=mean(y_vec_no_prod), labels='Sales ($)', cex.axis=1.3, lwd.ticks = 0)\nbox(lwd=2)\ny_sim |&gt; unlist() |&gt; matrix(ncol=length(y_vec_no_prod), byrow=T) |&gt; \n  state_space_utils$time_series_err()\nlines(y_vec_no_prod, type='b', lwd=3, pch=16)\nlegend('topleft', legend=c('LENI Sales', 'Simulated State'),\n       col=c('black', NA),\n       pch=c(16,22),\n       pt.cex=c(1,2),\n       lwd=c(3,NA),\n       lty=c(1,NA),\n       pt.bg=c(NA, adjustcolor('darkblue',0.2)),\n\n               bg=adjustcolor('white', 0.5))"
  },
  {
    "objectID": "posts/state_space/index.html#references",
    "href": "posts/state_space/index.html#references",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "References",
    "text": "References\n\n\nDurbin, James, and Siem Jan Koopman. 2012. Time Series Analysis by State Space Methods. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199641178.001.0001.\n\n\nDurbin, J., and S. J. Koopman. 2002. “A Simple and Efficient Simulation Smoother for State Space Time Series Analysis.” Biometrika 89 (3): 603–15. http://www.jstor.org/stable/4140605."
  },
  {
    "objectID": "posts/state_space/index.html#footnotes",
    "href": "posts/state_space/index.html#footnotes",
    "title": "An Uhinged Introduction to State Space Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(\\alpha_t\\): defined as the state as time t \\(a_t = \\mathbb{E}[\\alpha_t|Y_{t-1}]\\)  \\(a_{t|t} = \\mathbb{E}[\\alpha_t|Y_t]\\)  \\(\\hat{\\alpha}_t=\\mathbb{E}[\\alpha_t|Y_n]\\). This one irks me a bit. I’d prefer it be \\(a_{t|n}\\) or \\(\\hat{a}_t\\), but it makes sense if they wanted to be consistent with the sampling notation.  \\(\\tilde{\\alpha}_t\\): a draw from the distribution \\(P(\\alpha|Y_n)\\)  \\(\\alpha^+\\): a draw from the distribution \\(P(\\alpha)\\), the unconditional distribution  \\(\\hat{\\alpha}^+ = \\mathbb{E}[\\alpha|y^+]\\), where \\(y^+\\) are the observations from evolving the system forward given a random initialization drawn from the known disturbance variances and \\(\\alpha_1\\) distribution.↩︎\nYou can actually drop the normality assumptions of the disturbances if you’re willing to work with the minimum variance linear unbiased estimate (MVLUE) instead↩︎\nOr incredibly assuming↩︎\nDurbin & Koopman actually using \\(\\prime\\) when describing individual draws within the \\(*^+\\) vector, I have chosen to use \\(*^+_t\\) to keep the notation less overwhelming↩︎"
  },
  {
    "objectID": "posts/polychoric/index.html",
    "href": "posts/polychoric/index.html",
    "title": "Polychoric Correlation with Likert Data",
    "section": "",
    "text": "Look at you, you successful businessperson you! You own a company that sells two products: A and B. You run a short two question survey to determine whether your customers would recommend either products. For this example we’ll assume that all customers are using both products. The question style is the commonly used “Net Promoter Score” or likert scale format:\n\n“Provide your level of agreement to the following question: I would recommend this product to a friend”\n\nWhere the available choices are:\n\nStrongly Disagree\nSomewhat Disagree\nNeither Agree or Disagree\nSomewhat Agree\nStrongly Agree\n\nThere are of course limitations to this kind of survey design. For one, most people have a hard time discretizing their feelings or emotions into a single bucket. Perhaps the more appropriate question response would feature a slider that allows respondents to freely select their agreement on a continuous scale. Regardless, as this is the design chosen by thousands of companies and organizations, we’ll choose it as well. Though, we’ll recognize that agreement or sentiment in general is better categorized as a spectrum.\nEnough philosophy, now to the actual data. I’m going to show how the data is generated further down, but for now let’s say that we ran the survey and collected 1,000 responses. First, let’s start by loading in all the packages we’ll need for this analysis.\n\n\nCode\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggridges)\n\noptions(mc.cores=4)\n\nset.seed(1234)\n\nsigma &lt;- c(1,0.5,0.5,1) %&gt;% matrix(.,nrow=2,byrow=T)\ndat &lt;- mvtnorm::rmvnorm(500, sigma=sigma)\n\nc_points &lt;- list(\n  c(-2.69, -0.68, 0.07, 1.13),\n  c(-1.29, 0.62, 1.38, 1.65)\n)\n\ndiscrete_data &lt;- dat %&gt;% apply(., 1, \n                               \\(x) c(\n                                 sum(x[1] &gt; c_points[[1]]) + 1,\n                                 sum(x[2] &gt; c_points[[2]]) + 1)\n                               ) %&gt;% t()\n\n\nNext, we’ll take our discrete_data data frame which holds our survey responses and visualize it as a table of all unique responses. For example, the third row and second column will be the number of customers that responded 3 to question 1 and 2 to question 2.\n\ntable(discrete_data[,1], discrete_data[,2]) %&gt;%\n  kbl(row.names=1) %&gt;%\n  kable_styling() %&gt;%\n  column_spec(column = 1, bold=T)\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\n1\n2\n0\n0\n0\n\n\n2\n25\n102\n5\n0\n0\n\n\n3\n9\n108\n16\n3\n3\n\n\n4\n4\n96\n46\n9\n10\n\n\n5\n0\n29\n17\n5\n10\n\n\n\n\n\n\n\nFrom the table above, we can already see that there is a high degree of positive correlation between the questions. If we wanted to quantifying this correlation, we might naively use the cor function, but this produces biased results as our provided data is not continuous, which is assumed by the default Pearson correlation measure. There are other measures of correlation such as Spearman or Kendall which are non-parametric, but neither take into account the data generating process that aligns with our philosophy. For that, we will need to employ the polychoric correlation which we will further define below."
  },
  {
    "objectID": "posts/polychoric/index.html#motivation-its-likely-likert",
    "href": "posts/polychoric/index.html#motivation-its-likely-likert",
    "title": "Polychoric Correlation with Likert Data",
    "section": "",
    "text": "Look at you, you successful businessperson you! You own a company that sells two products: A and B. You run a short two question survey to determine whether your customers would recommend either products. For this example we’ll assume that all customers are using both products. The question style is the commonly used “Net Promoter Score” or likert scale format:\n\n“Provide your level of agreement to the following question: I would recommend this product to a friend”\n\nWhere the available choices are:\n\nStrongly Disagree\nSomewhat Disagree\nNeither Agree or Disagree\nSomewhat Agree\nStrongly Agree\n\nThere are of course limitations to this kind of survey design. For one, most people have a hard time discretizing their feelings or emotions into a single bucket. Perhaps the more appropriate question response would feature a slider that allows respondents to freely select their agreement on a continuous scale. Regardless, as this is the design chosen by thousands of companies and organizations, we’ll choose it as well. Though, we’ll recognize that agreement or sentiment in general is better categorized as a spectrum.\nEnough philosophy, now to the actual data. I’m going to show how the data is generated further down, but for now let’s say that we ran the survey and collected 1,000 responses. First, let’s start by loading in all the packages we’ll need for this analysis.\n\n\nCode\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggridges)\n\noptions(mc.cores=4)\n\nset.seed(1234)\n\nsigma &lt;- c(1,0.5,0.5,1) %&gt;% matrix(.,nrow=2,byrow=T)\ndat &lt;- mvtnorm::rmvnorm(500, sigma=sigma)\n\nc_points &lt;- list(\n  c(-2.69, -0.68, 0.07, 1.13),\n  c(-1.29, 0.62, 1.38, 1.65)\n)\n\ndiscrete_data &lt;- dat %&gt;% apply(., 1, \n                               \\(x) c(\n                                 sum(x[1] &gt; c_points[[1]]) + 1,\n                                 sum(x[2] &gt; c_points[[2]]) + 1)\n                               ) %&gt;% t()\n\n\nNext, we’ll take our discrete_data data frame which holds our survey responses and visualize it as a table of all unique responses. For example, the third row and second column will be the number of customers that responded 3 to question 1 and 2 to question 2.\n\ntable(discrete_data[,1], discrete_data[,2]) %&gt;%\n  kbl(row.names=1) %&gt;%\n  kable_styling() %&gt;%\n  column_spec(column = 1, bold=T)\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\n1\n2\n0\n0\n0\n\n\n2\n25\n102\n5\n0\n0\n\n\n3\n9\n108\n16\n3\n3\n\n\n4\n4\n96\n46\n9\n10\n\n\n5\n0\n29\n17\n5\n10\n\n\n\n\n\n\n\nFrom the table above, we can already see that there is a high degree of positive correlation between the questions. If we wanted to quantifying this correlation, we might naively use the cor function, but this produces biased results as our provided data is not continuous, which is assumed by the default Pearson correlation measure. There are other measures of correlation such as Spearman or Kendall which are non-parametric, but neither take into account the data generating process that aligns with our philosophy. For that, we will need to employ the polychoric correlation which we will further define below."
  },
  {
    "objectID": "posts/polychoric/index.html#data-generating-process",
    "href": "posts/polychoric/index.html#data-generating-process",
    "title": "Polychoric Correlation with Likert Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\nWe assumed that our data was generated by customers that were forced to discretize their agreement with a given question. If we wanted to properly model this, we might want to assume that our data is first generated from some latent multivariate continuous distribution, and then discretized using a set of p - 1 “cut-points” for each dimension of the latent space, where p is the number of possible choices in the questionnaire. The code to generate data from this process is shown below:\n\nsigma &lt;- c(1,0.5,0.5,1) %&gt;% matrix(.,nrow=2,byrow=T)\ndat &lt;- mvtnorm::rmvnorm(500, sigma=sigma)\n\nc_points &lt;- list(\n  c(-2.69, -0.68, 0.07, 1.13),\n  c(-1.29, 0.62, 1.38, 1.65)\n)\n\ndiscrete_data &lt;- dat %&gt;% apply(., 1, \n                               \\(x) c(\n                                 sum(x[1] &gt; c_points[[1]]) + 1,\n                                 sum(x[2] &gt; c_points[[2]]) + 1)\n                               ) %&gt;% t()\n\nWe use a bivariate gaussian as our latent distribution with a mean of 0 and variance, or equivalently in this case, a correlation matrix of\n\\[\n\\begin{bmatrix}\n1 & 0.5\\\\\n0.5 & 1\n\\end{bmatrix}\n\\] If we plot the latent variables with the cut-points we obtain:\n\nplot(dat,pch=16, xlab = 'Question 1', ylab='Question 2', main='Latent Space Representation')\nfor (i in 1:length(c_points[[1]])){\n  abline(v=c_points[[1]][i], lwd=2, col='darkred')\n  abline(h=c_points[[2]][i], lwd=2, col='darkblue')\n}\n\n\n\n\n\n\n\n\nSo a customer that answered 3 for the first question and 2 for the second question would have had their latent representation in the below region:\n\n\nCode\nplot(dat, pch=16, xlab = 'Question 1', ylab='Question 2', main='Latent Space Representation')\npolygon(x=c(c_points[[1]][2], c_points[[1]][3], c_points[[1]][3], c_points[[1]][2]),\n        y=c(c_points[[2]][1], c_points[[2]][1], c_points[[2]][2], c_points[[2]][2]),\n        col=adjustcolor('purple', 0.5))\nfor (i in 1:length(c_points[[1]])){\n  abline(v=c_points[[1]][i], lwd=2, col='darkred')\n  abline(h=c_points[[2]][i], lwd=2, col='darkblue')\n}"
  },
  {
    "objectID": "posts/polychoric/index.html#modeling",
    "href": "posts/polychoric/index.html#modeling",
    "title": "Polychoric Correlation with Likert Data",
    "section": "Modeling",
    "text": "Modeling\n\nPriors and Likelihood\nOur model is almost fully specified with the data generating process outlined above, but we still need to incorporate our priors. For the correlation matrix, we will reparametrize using the Cholesky decomposition and use a LKJ prior. The cut-points are a little trickier, but notice that the marginals of our latent distribution are standard normals. We can use this to our advantage by reparametrizing the cut-points as a vector of probabilities where each entry is the probability allocated to the interval on the standard normal distribution between two adjacent cut-points \\(c_i\\) and \\(c_j\\). Note that for \\(p-1\\) cutpoints, there will be \\(p\\) entries in our probability vector. Thus, we can write:\n\\[\n\\begin{align}\n\\begin{bmatrix}\nc_1\\\\\nc_2 \\\\\n\\vdots\\\\\nc_{p-1}\n\\end{bmatrix}\n& \\rightarrow \\begin{bmatrix}\n\\theta_1\\\\\n\\theta_2\\\\\n\\vdots\\\\\n\\theta_{p}\n\\end{bmatrix} \\\\ \\\\\n&= \\begin{bmatrix}\n\\Phi(c_1) \\\\\n\\Phi(c_2) - \\Phi(c_1) ] \\\\\n\\vdots \\\\\n1 - \\Phi(c_{p-1})\n\\end{bmatrix}\n\\end{align}\n\\] Since our probability interval vector must sum to one, we can use a dirichlet distribution as the prior. The stan code to specify this prior and perform the reparametrization is below:\nreal induced_dirichlet_lpdf(vector c, vector alpha, real gamma){\n    int K = num_elements(c) + 1;\n    vector[K - 1] cuml = Phi(c - gamma);\n    vector[K] p;\n    matrix[K,K] J = rep_matrix(0,K,K);\n\n    p[1] = cuml[1];\n    for (k in 2:(K-1)){\n        p[k] = cuml[k] - cuml[k-1];\n    }\n    p[K] = 1 - cuml[K-1];\n\n    for (k in 1:K) J[k,1] = 1;\n\n    for (k in 2:K){\n        real rho = exp(std_normal_lpdf(c[k-1] - gamma));\n        J[k,k] = -rho;\n        J[k - 1, k] = rho;\n    }\n    return dirichlet_lpdf(p | alpha) + log_determinant(J);\n}\nThe stan code is a bit more involved, and includes the Jacobian calculations since we are specifying a prior on the transformed parameters. For more detail about the reparametrization and Jacobian calculations I suggest reading Michael Betancourt’s ordinal regression tutorial. Be aware that his model uses a latent logistic distribution (different than our standard normal).\nFinally, we need to consider the likelihood of the data, conditioned on our parameters. To model this, we will extend the multivariate probit model to the case where an arbitrary number of ordinal categories are observed. Without going into too much detail, the multivariate probit is used to model a bernoulli distributed random vector, where the data is assumed to have been generated from a latent multivariate normal distribution. For example, if you consider our data generating process above but instead only have one cut-point per dimension, then the data generated would be a bernoulli random vector. The stan code used to define this extenstion of the multivariate probit likelihood is here, along with the full stan code for the model.\nThe full derivation for the likelihood, and therefore stan code, is beyond the scope of this blog post, but I refer you to these two other resources to learn more if you are interested:\n\nBen Goodrich’s Truncated Normal Sampler in STAN\nGHK Algorithm, which is what Ben’s implementation is based on\n\nThe parameter, model, and generated quantities block is shown below:\nparameters {\n  cholesky_factor_corr[D] L_Omega;\n  array[N,D] real&lt;lower=0, upper=1&gt; u;\n  array[D] ordered[n_cut] c_points;\n}\nmodel {\n    L_Omega ~ lkj_corr_cholesky(4);\n    for (d in 1:D) target += induced_dirichlet_lpdf(c_points[d] | rep_vector(1, n_cut + 1), 0);\n\n    for (n in 1:N) target += trunc_norm(y[n], c_points, L_Omega, u[n], D, y_min, y_max);\n}\ngenerated quantities {\n   corr_matrix[D] Omega;\n   Omega = multiply_lower_tri_self_transpose(L_Omega);\n}\nThe parameters of interest here are L_Omega, which will give us the correlation matrix for our latent gaussian and the c_points array which determines the cut-points that generated our data. Ignore the u parameter, as it is a nuisance parameter and is only used to help define the likelihood of our data.\n\n\nSampling and Posterior Exploration\nNow that are model is fully defined, we can used the cmdstanr package to sample our posterior. The full stan code can found here. Note, during model fitting we are expecting some rejected samples due to the highly constrained values of the correlation matrix. When fitting this model with a higher dimension latent\n\n# fp &lt;- file.path('PATH TO YOUR STAN MODEL CODE')\nmod &lt;- cmdstan_model(fp)\ndata &lt;- list(\n  D = ncol(discrete_data),\n  N = nrow(discrete_data),\n  y = discrete_data,\n  y_min = min(discrete_data),\n  y_max = max(discrete_data)\n)\n\npoly_cor &lt;- mod$sample(data = data, seed = 1234, chains = 4, parallel_chains = 2,\n                       iter_warmup = 2000,iter_sampling = 2000)\n\nLet’s take a look out some diagnotics to make sure we had adequate posterior sampling:\n\npoly_cor$summary(c(paste0('c_points[1,', 1:4,']'), paste0('c_points[2,', 1:4,']'), 'Omega[2,1]')) %&gt;%\n  select(-c(median, q5, q95, mad)) %&gt;%\n  map(., \\(x) if(is.character(x)) x else round(x,2)) %&gt;% as.data.frame() %&gt;%\n  kbl() %&gt;% kable_styling()\n\n\n\n\nvariable\nmean\nsd\nrhat\ness_bulk\ness_tail\n\n\n\n\nc_points[1,1]\n-2.44\n0.19\n1\n9841.39\n5775.21\n\n\nc_points[1,2]\n-0.70\n0.06\n1\n9717.82\n7102.99\n\n\nc_points[1,3]\n-0.02\n0.06\n1\n9485.35\n7512.77\n\n\nc_points[1,4]\n1.04\n0.07\n1\n11822.11\n6832.92\n\n\nc_points[2,1]\n-1.36\n0.08\n1\n12008.87\n6858.70\n\n\nc_points[2,2]\n0.66\n0.06\n1\n9789.17\n6537.07\n\n\nc_points[2,3]\n1.44\n0.08\n1\n10790.98\n6061.38\n\n\nc_points[2,4]\n1.71\n0.10\n1\n12190.28\n6611.51\n\n\nOmega[2,1]\n0.51\n0.04\n1\n10775.66\n6575.90\n\n\n\n\n\n\n\nOur Rhat, ess_bulk, and ess_tail look good! Let’s take a look at our posteriors for the c_points parameter. The ggplot code that adds line segments to the ridge plot is adapted from this stackoverflow post.\n\n# Function to extract Cut Points from stan model\nget_cut_point &lt;- function(dim, n){\n  c_name &lt;- paste0('c_points[', dim, ',', n, ']')\n  poly_cor$draws(c_name) %&gt;% \n    .[,,1,drop=T] %&gt;%\n    as.vector() %&gt;% as.data.frame() %&gt;%\n    cbind(., c_name, dim)\n}\n\ncut_point_draws &lt;- lapply(4:1, \\(x) get_cut_point(1,x)) %&gt;% \n  append(., lapply(4:1, \\(x) get_cut_point(2,x))) %&gt;%\n  do.call('rbind', .) %&gt;% set_names(c('value', 'name', 'dim')) %&gt;% \n  mutate(dim=as.factor(dim))\n\n# Plot the Cut Point Ridge plot\ncomp &lt;- ggplot(cut_point_draws, aes(x=value, y=name)) + geom_density_ridges()\ningredients &lt;- ggplot_build(comp) %&gt;% purrr::pluck(\"data\", 1)\n\ndensity_lines &lt;- ingredients %&gt;% group_by(group) %&gt;% \n  mutate(a = c_points[[floor(max(group)/5) + 1]][[((max(group) - 1) %% 4) + 1]]) %&gt;% \n  slice(which.min(abs(x-a)))\n\nggplot(cut_point_draws, aes(x=value, y=name)) + \n  geom_density_ridges(aes(fill=dim), rel_min_height = 0.01, lwd=1.2, alpha=0.3) +\n  scale_x_continuous(breaks = seq(-3.5,2.5,0.5),limits = c(-3.5,2), expand=c(0,0)) +\n  geom_segment(data = density_lines, col='darkred',lwd=1.2,\n               aes(x = x, y = ymin, xend = x, \n                   yend = ymin+density*scale*iscale, linetype='Actual Cut-Point Value')) +\n  scale_linetype_manual(\"\",values=c('Actual Cut-Point Value'=1)) +\n  scale_fill_manual(\"Cut-point Dimension\", \n                    values=c('darkblue', 'darkred', 'yellow'), \n                    labels=c('Dim. 1', 'Dim. 2')) +\n  labs(title='Cut Point Parameter Estimates') +\n  theme_ridges() + \n  theme(axis.title.y = element_blank(), axis.title.x = element_blank(), \n        axis.text = element_text(size=20), plot.title = element_text(size=30),\n        legend.text = element_text(size=20), legend.title = element_text(size=30))\n\n\n\n\n\n\n\n\nIt looks like the actual value of each cut-point is captured within the range of each estimated parameter. Now we can look at our originally requested quantity; the correlation.\n\npoly_cor$draws('Omega[2,1]') %&gt;% .[,,1,drop=T] %&gt;% as.vector() %&gt;% \n  {as.data.frame(list(x=.))} %&gt;%\n  ggplot(aes(x=x)) + geom_histogram(aes(y=..density..), fill='darkblue', col='black', alpha=0.3) + \n  geom_density(lwd=1.3) +     \n  geom_vline(aes(xintercept=0.5, color='Actual Value'), size=1.5, linetype='twodash') +\n  scale_colour_manual(\"\", values=\"darkred\") +\n  labs(title='Correlation Parameter Estimate') +\n  theme_minimal() +\n  theme(axis.title.y = element_blank(), axis.title.x = element_blank(), legend.text = element_text(size=13),\n        axis.text = element_text(size=13), plot.title = element_text(size=17))"
  },
  {
    "objectID": "posts/polychoric/index.html#conclusion",
    "href": "posts/polychoric/index.html#conclusion",
    "title": "Polychoric Correlation with Likert Data",
    "section": "Conclusion",
    "text": "Conclusion\nIt looks like our model fits the data well and is able to adequately identify the parameters. While the polychoric correlation is a little more involved than a simple pearson correlation, it aligns more with our original data generating process philosophy. Since this model was fit using a bayesian framework we have samples from our posterior which we can use to perform decision analysis, generating pseudocustomers, or probabilistic PCA from the posterior of the correlation matrix.\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggridges_0.5.6   kableExtra_1.4.0 knitr_1.49       cmdstanr_0.8.1  \n [5] lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4     \n [9] purrr_1.0.2      readr_2.1.5      tidyr_1.3.1      tibble_3.2.1    \n[13] ggplot2_3.5.1    tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] tensorA_0.36.2.1     utf8_1.2.4           generics_0.1.3      \n [4] xml2_1.3.6           stringi_1.8.4        hms_1.1.3           \n [7] digest_0.6.37        magrittr_2.0.3       evaluate_1.0.1      \n[10] grid_4.4.2           timechange_0.3.0     mvtnorm_1.3-2       \n[13] fastmap_1.2.0        jsonlite_1.8.9       processx_3.8.4      \n[16] backports_1.5.0      ps_1.8.1             fansi_1.0.6         \n[19] viridisLite_0.4.2    scales_1.3.0         abind_1.4-8         \n[22] cli_3.6.3            rlang_1.1.4          munsell_0.5.1       \n[25] withr_3.0.2          yaml_2.3.10          tools_4.4.2         \n[28] tzdb_0.4.0           checkmate_2.3.2      colorspace_2.1-1    \n[31] vctrs_0.6.5          posterior_1.6.0      R6_2.5.1            \n[34] matrixStats_1.4.1    lifecycle_1.0.4      htmlwidgets_1.6.4   \n[37] pkgconfig_2.0.3      pillar_1.9.0         gtable_0.3.6        \n[40] glue_1.8.0           systemfonts_1.1.0    xfun_0.49           \n[43] tidyselect_1.2.1     rstudioapi_0.17.1    farver_2.1.2        \n[46] htmltools_0.5.8.1    labeling_0.4.3       svglite_2.1.3       \n[49] rmarkdown_2.29       compiler_4.4.2       distributional_0.5.0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Ian, I am the Technical Lead Data Scientist working in the Customer Insights group at Wegmans. My main interests lie in analyzing consumer behavior using Bayesian statistics and classical machine learning techniques. When my sampler is running you can find me outside looking at mushrooms or inside reading if it’s too rainy."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nWegmans Food Markets | Data Scientist - Technical Lead | Jan 2021 - present\nWegmans Food Markets | Data Science Co-Op | May 2019 - Jan 2021\nWorcester Polytechnic Institute | Data Science Undergraduate Research Assistant | May 2018 - Aug 2018"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Rochester | Rochester, NY\nM.S. in Data Science | Aug 2019 - Jan 2021\nState University of New York at Geneseo | Geneseo, NY\nB.S. in Applied Physics, B.A. in Mathematics | Aug 2015 - May 2019"
  },
  {
    "objectID": "posts/poisson_metrop/index.html",
    "href": "posts/poisson_metrop/index.html",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "",
    "text": "Allow me to paint a picture:\nYou’re an avid salmon fisherman and statistician. You’ve fished 100 ponds in the surrounding area and have recorded the hourly rate at which you catch salmon while on the water. During your time on the water you also record a couple of measurable variables for each pond: pond depth and dissolved oxygen.\nNow, your friend Danny wants go fishing with you this week and suggests two ponds you haven’t fished a before. Also, since Danny is a good friend, they have kindly measured both depth and dissolved oxygen of each pond. Very thoughtful! The question now stands: which pond will you catch more salmon at?"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#a-fishy-situation",
    "href": "posts/poisson_metrop/index.html#a-fishy-situation",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "",
    "text": "Allow me to paint a picture:\nYou’re an avid salmon fisherman and statistician. You’ve fished 100 ponds in the surrounding area and have recorded the hourly rate at which you catch salmon while on the water. During your time on the water you also record a couple of measurable variables for each pond: pond depth and dissolved oxygen.\nNow, your friend Danny wants go fishing with you this week and suggests two ponds you haven’t fished a before. Also, since Danny is a good friend, they have kindly measured both depth and dissolved oxygen of each pond. Very thoughtful! The question now stands: which pond will you catch more salmon at?"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#data-generation",
    "href": "posts/poisson_metrop/index.html#data-generation",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "Data generation",
    "text": "Data generation\nLets assume that the number of salmon caught \\(Y\\) is poisson distributed with rate \\(\\lambda\\). \\[Y \\sim Poi(\\lambda)\\]\nLet us also assume that the log rate of catching salmon is a linear function of the two pond variables:\n\\[\\log(\\lambda) =  \\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0\\] Finally, we may write the probability of catching \\(y\\) salmon during one hour of fishing given the above parameters:\n\\[P(y|x_i;\\beta_{depth},\\beta_{oxy},\\beta_0)=\\frac{\\lambda^y}{y!}e^{-\\lambda}, \\hspace{4mm} \\lambda=\\exp(\\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0)\\] Now, let’s generate some data!\n\n\nCode\nlibrary(dplyr)\nlibrary(rstan)\n\n\n\n# Function to standardize data\nstandardize &lt;- function(x){\n  return((x - mean(x)) / sd(x))\n}\nset.seed(1234)\n\nN &lt;- 200 # Number of ponds fished at\nx_oxy &lt;- rnorm(N, 5, 1) # Dissolved oxygen in mg per Liter\nx_depth &lt;- abs(rnorm(N, 30, 10)) # Pond depth\n\nb_oxy &lt;- 0.8\nb_depth &lt;- -0.6\nb_int &lt;- 0.4\nlambda &lt;- exp(b_oxy*standardize(x_oxy) + b_depth*standardize(x_depth) + b_int)\ny &lt;- rpois(length(lambda), lambda = lambda)\n\npar(mfrow=c(1,2))\nplot(x_oxy, y, pch=16, col='darkblue', ylab='Fish Caught Per Hour', xlab='Dissolved Oxygen (mg/L)')\nplot(x_depth, y, pch=16, col='darkred', ylab='Fish Caught Per Hour', xlab='Pond Depth (m)')"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#mcmc-with-metropolis-hastings",
    "href": "posts/poisson_metrop/index.html#mcmc-with-metropolis-hastings",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "MCMC with Metropolis Hastings",
    "text": "MCMC with Metropolis Hastings\n\nTheory\n\nModel Definition\nFirst, we will assume that the parameters \\(\\beta_{depth}\\), \\(\\beta_{oxy}\\), and \\(\\beta_0\\) are independent of each other such that the joint probability distribution for the parameters can be expressed as:\n\\[P(\\beta_{depth}, \\beta_{oxy}, \\beta_0 ) = P(\\beta_{depth} ) \\cdot P(\\beta_{oxy})\\cdot P(\\beta_0)\\]\nNow, observing the data from the plots above, we can assume that \\(\\beta_{depth}\\) will be positive and \\(\\beta_{oxy}\\) to be negative. Thus, we can set out priors to reflect this observation:\n\\[\\begin{align}\nY &\\sim Poi(\\lambda) \\\\\n\\log(\\lambda) &=  \\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0 \\\\\n\\beta_{depth} &\\sim N(0.5, 0.5) \\\\\n\\beta_{oxy} &\\sim N(-0.5,0.5) \\\\\n\\beta_0 &\\sim N(0, 0.5)\n\\end{align}\\]\nLet’s now define our likelihood and priors in R:\n\nlog_poi_liklihood &lt;- function(params){\n  lam_cands &lt;- exp(params[['b_oxy_cand']]*standardize(x_oxy) + \n                     params[['b_depth_cand']]*standardize(x_depth) +\n                     params[['b_int_cand']])\n  return(sum(dpois(y, lam_cands, log=T)))\n}\n\nlog_prior &lt;- function(params){\n  return(\n    dnorm(params[['b_oxy_cand']], 0.5, 0.5, log=T) +\n    dnorm(params[['b_depth_cand']], -0.5, 0.5, log=T) +\n    dnorm(params[['b_int_cand']], 0, 0.5, log=T)\n  )\n}\n\nlog_posterior_prob &lt;- function(params){\n  return(log_prior(params) + log_poi_liklihood(params))\n}\n\nWhile defining our priors it is always good practice to make sure that they make sense by checking the prior predictive distribution. We can do this by taking some samples from our prior distributions and using them in place of our model.\n\nn_prior_samples &lt;- 1e3\nsample_priors &lt;- cbind(rnorm(n_prior_samples, 0.5, 0.5),\n                       rnorm(n_prior_samples, -0.5, 0.5),\n                       rnorm(n_prior_samples, 0,0.5))\n\nprior_predicitive &lt;- cbind(standardize(x_oxy), standardize(x_depth)) %&gt;% apply(., 1, function(x) \n  rpois(n=n_prior_samples, exp(x[1]*sample_priors[,1] + x[2]*sample_priors[,2] + sample_priors[,3]) ))\n\nhist(prior_predicitive %&gt;% log(),prob=T,ylim=c(0,0.8), col=adjustcolor('darkblue', alpha.f = 0.5),\n     main='Log of Prior Predictive and y', xlab='', ylab='')\nhist(y %&gt;% log(), prob=T, add=T, col=adjustcolor('darkred', alpha.f = 0.5))\nlegend('topright', c('Prior Predidictive','y'), fill=c(adjustcolor('darkblue', alpha.f = 0.5),\n                                                       adjustcolor('darkred', alpha.f = 0.5)))\n\n\n\n\n\n\n\n\n\n\nMCMC\nMetropolis Hastings is rejection sampler defined as the following.\nSuppose we have some sampled value \\(x_t\\) and some function \\(P(x)\\) the returns the probability of a given \\(x\\). We also have some proposal function that generates a new \\(x_{t+1}\\) given a previously sampled \\(x_t\\) defined by \\(g(x_{t+1}|x_t)\\).\nNow, we need some way to “reject” or “accept” some newly generated \\(x_{t+1}\\) value from our function \\(g\\). Define this probability of acceptance to be\n\\[a=\\frac{P(x_{t+1})g(x_t|x_{t+1})}{P(x_t)g(x_{t+1}|x_t)}\\] Usually (and for our case today), we’ll choose a function \\(g\\) such that \\(g\\) is symmetric, or \\(g(x_t|x_{t+1})=g(x_{t+1}|x_t)\\). A common choice to achieve this property would be to assume \\(g\\) is normal with mean equal to the given point. In other words \\[g(x_t|x_{t+1})\\sim N(x_{t+1},\\sigma)\\]\nNote, here \\(P(x)\\) will be the probability of our sampled \\(\\{\\beta_{depth}, \\beta_{oxy}, \\beta_0 \\}\\) given our data, as that is the posterior we which to rejection sample from.\nNow, let’s write our MCMC algorithm and sample from our posterior! We run 4 different chains to get the best estimate of our posterior.\n\nN_sim &lt;- 5e4\nN_chains &lt;- 4\n\nmcmc_chain &lt;- function(N_sim, explore_param){\n  curr_params &lt;-  list(\n    b_oxy_cand = rnorm(1, 0, 4),\n    b_depth_cand = rnorm(1, 0, 4),\n    b_int_cand = rnorm(1, 0, 4)\n  )\n  chain &lt;- matrix(NA, nrow=N_sim, ncol=3)\n  for (i in 1:N_sim){\n    cand_params &lt;- list(\n      b_oxy_cand = rnorm(1, curr_params[['b_oxy_cand']], explore_param),\n      b_depth_cand = rnorm(1, curr_params[['b_depth_cand']], explore_param),\n      b_int_cand = rnorm(1, curr_params[['b_int_cand']], explore_param)\n    )\n    a &lt;- min(1, exp(log_posterior_prob(cand_params) - \n                      log_posterior_prob(curr_params)))\n    u &lt;- runif(1)\n    if (u &lt;= a){\n      chain[i,] &lt;- unlist(cand_params)\n      curr_params &lt;- cand_params\n    }\n    else{\n      chain[i,] &lt;- unlist(curr_params)\n    }\n  }\n  return(chain)\n}\n\nsimulation &lt;- list()\nfor (i in 1:N_chains){\n  simulation[[paste0('chain_',i)]] &lt;- mcmc_chain(N_sim, explore_param = 0.01)\n}\n\nburn &lt;- 1e4\n\n\n\n\nAnalyzing our chains\nLet’s see how well our posteriors match the actual values:\n\n\nCode\npar(mfrow=c(1,3))\nposterior_oxy &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),1])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_oxy %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Dissolved Oxygen')\npolygon(posterior_oxy %&gt;% density(), col=adjustcolor('darkgreen', 0.5))\nabline(v=0.8, col='red', lwd=4)\n\n\nposterior_depth &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),2])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_depth %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Depth')\npolygon(posterior_depth %&gt;% density(), col=adjustcolor('darkblue', 0.5))\nabline(v=-0.6, col='red', lwd=4)\n\n\nposterior_int &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),3])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_int %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Beta_0')\npolygon(posterior_int %&gt;% density(), col=adjustcolor('darkred', 0.5))\nabline(v=0.4, col='red', lwd=4)\n\n\n\n\n\n\n\n\n\nFocusing on \\(\\beta_{oxy}\\), let’s see how well our chain converged using rank plots:\n\na &lt;- simulation %&gt;% lapply(., function(x) x[-seq(1,burn),1]) %&gt;% unlist() %&gt;% rank() %&gt;% \n  matrix(., ncol=4)\npar(mfrow=c(2,2))\nfor (i in 1:4) hist(a[,i], col=adjustcolor('darkblue', alpha.f = 0.5), main=paste0('Chain_',i),\n                    xlab='', ylab='')\n\n\n\n\n\n\n\n\nRank plots are calculated by combining all our MCMC samples and finding each samples respective rank. The resulting ranks are separated back into their respective chains and plotted as histograms. If the MCMC sampler converged and the chains mixed well without a high degree of autocorrelation, we can expected uniform distributions for each rank plot.\nThere a couple of metrics we can look at to assess the convergence of our MCMC sampling. One main metric is \\(\\hat{R}\\). It tells us how well all our chains converged and mixed. A good rule of thumb is to have \\(\\hat{R}\\) under 1.05.\nThe other metrics have to do with effective sample size (ESS). In MCMC sampling, we are assuming a level of independence for samples not directly adjacent. In other words, we are hoping for a low degree of autocorrelation. Simply put, if we have a high degree of autocorrelation in our samples then we effectively have less information describing our posterior. This is what ESS measures, the degree of autocorrelation in our chains. The first, bulk-ESS tells us how well the center or bulk of our posterior has been sampled. The second is tail-ESS, which tells us how well our posterior tails were sampled. A good rule of thumb is to have a bulk-ESS and tail-ESS greater than 400.\n\nmetric_mat &lt;- matrix(NA, nrow=3, ncol=3)\nfor (i in 1:3){\n  metric_mat[1,i] &lt;- Rhat(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                            as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,2)\n  metric_mat[2,i] &lt;- ess_bulk(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                                as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,1)\n  metric_mat[3,i] &lt;- ess_tail(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                                as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,1)\n}\ncolnames(metric_mat) &lt;- c('b_oxy', 'b_depth', 'b_0')\nrow.names(metric_mat) &lt;- c('r_hat', 'bulk_ess', 'tail_ess')\nknitr::kable(metric_mat, align = 'ccc')\n\n\n\n\n\nb_oxy\nb_depth\nb_0\n\n\n\n\nr_hat\n1.0\n1.0\n1.0\n\n\nbulk_ess\n1359.1\n1168.9\n834.9\n\n\ntail_ess\n2638.3\n2752.3\n1573.2\n\n\n\n\n\nA good measure to determine how well model fits our data is to plot the posterior predictive against the observed data.\n\nposterior_predictive &lt;- cbind(standardize(x_oxy), standardize(x_depth)) %&gt;% apply(., 1, function(x) \n  rpois(n=n_prior_samples, exp(x[1]*posterior_oxy + x[2]*posterior_depth + posterior_int))) %&gt;% c()\n\nhist(posterior_predictive,prob=T, col=adjustcolor('darkblue', alpha.f = 0.5),\n     breaks=length(unique(posterior_predictive)),\n     main='Posterior Predictive and y', ylab='', xlab='')\n\nhist(y, prob=T, add=T, col=adjustcolor('darkred', alpha.f = 0.5), breaks=34)\nlegend('topright', c('Posterior Predictive','y'), fill=c(adjustcolor('darkblue', alpha.f = 0.5),\n                                                       adjustcolor('darkred', alpha.f = 0.5)))"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#decision-analysis",
    "href": "posts/poisson_metrop/index.html#decision-analysis",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "Decision Analysis",
    "text": "Decision Analysis\nBack to the question at hand: what pond should you fish at? Let’s say your friend Danny has the following measurements for the two ponds in question.\n\nponds &lt;- matrix(c(8, 7, 40, 20), ncol=2) %&gt;% as.data.frame()\ncolnames(ponds) &lt;- c('Dissolved Oxygen', 'Pond Depth')\nrow.names(ponds) &lt;- c('Pond A', 'Pond B')\nknitr::kable(ponds, align='ccc')\n\n\n\n\n\nDissolved Oxygen\nPond Depth\n\n\n\n\nPond A\n8\n40\n\n\nPond B\n7\n20\n\n\n\n\n\nFrom our posterior samples, we can obtain distributions representing our uncertainty for the fish we will catch at each of the ponds in question.\n\npar(mfrow=c(1,2))\npond_a &lt;- ((ponds$`Dissolved Oxygen`[1] - mean(x_oxy)) / sd(x_oxy)) * posterior_oxy + \n  ((ponds$`Pond Depth`[1] - mean(x_depth)) / sd(x_depth)) * posterior_depth + posterior_int\npond_a &lt;- rpois(length(pond_a), exp(pond_a))\nhist(pond_a, breaks=length(unique(pond_a)), prob=T, col=adjustcolor('darkgreen', alpha.f = 0.5),\n     main='Pond A', xlab='Fish Caught', ylab='')\n\npond_b &lt;- ((ponds$`Dissolved Oxygen`[2] - mean(x_oxy)) / sd(x_oxy)) * posterior_oxy + \n  ((ponds$`Pond Depth`[2] - mean(x_depth)) / sd(x_depth)) * posterior_depth + posterior_int\npond_b &lt;- rpois(length(pond_b), exp(pond_b))\nhist(pond_b, breaks=length(unique(pond_b)), prob=T, col=adjustcolor('yellow', alpha.f = 0.5),\n     main='Pond B', xlab='Fish Caught', ylab='')\n\n\n\n\n\n\n\n\nNow, we can take the difference of the two distributions and come to our conclusion.\n\npond_diff &lt;- (pond_b - pond_a)\nhist(pond_diff, prob=T, main='Pond B - Pond A', xlab='Difference in fish caught between ponds', ylab='',\n      col=adjustcolor('purple', alpha.f = 0.5))\n\n\n\n\n\n\n\n\nIf we want to find the expected increase in fish in choosing pond B of pond A, it’s as simple as taking the average of our above distribution.\n\nmean(pond_b - pond_a)\n\n[1] 3.11315\n\n\nHence, after our extensive analysis we can come to the conclusion that it is best to choose Pond B over A. Although, maybe your friend Danny has left by now!\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] rstan_2.32.6        StanHeaders_2.32.10 dplyr_1.1.4        \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_1.8.9     compiler_4.4.2     tidyselect_1.2.1  \n [5] Rcpp_1.0.13-1      parallel_4.4.2     gridExtra_2.3      scales_1.3.0      \n [9] yaml_2.3.10        fastmap_1.2.0      ggplot2_3.5.1      R6_2.5.1          \n[13] generics_0.1.3     knitr_1.49         htmlwidgets_1.6.4  tibble_3.2.1      \n[17] munsell_0.5.1      pillar_1.9.0       rlang_1.1.4        utf8_1.2.4        \n[21] inline_0.3.20      xfun_0.49          RcppParallel_5.1.9 cli_3.6.3         \n[25] magrittr_2.0.3     digest_0.6.37      grid_4.4.2         rstudioapi_0.17.1 \n[29] lifecycle_1.0.4    vctrs_0.6.5        evaluate_1.0.1     glue_1.8.0        \n[33] QuickJSR_1.4.0     codetools_0.2-20   stats4_4.4.2       pkgbuild_1.4.5    \n[37] fansi_1.0.6        colorspace_2.1-1   rmarkdown_2.29     matrixStats_1.4.1 \n[41] tools_4.4.2        loo_2.8.0          pkgconfig_2.0.3    htmltools_0.5.8.1"
  }
]