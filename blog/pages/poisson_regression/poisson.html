<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
	<a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a>
  <div class="container">
  	<!-- <a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a> -->
    <div class="navbar-header">
      <!-- NOTE: add "navbar-inverse" class for an alternate navbar background -->
      <!-- <a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a> -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li><a href="https://github.com/TheNudibranch">GitHub</a></li>
        <li><a href="https://www.linkedin.com/in/ian-s-costley/">LinkedIn</a></li>
        <li><a href="../../blog.html">B<sub>natural</sub>LOG</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings</h1>

</div>


<div id="a-fishy-situation" class="section level2">
<h2>A fishy situation</h2>
<p>Allow me to paint a picture:</p>
<p>You’re an avid salmon fisherman and statistician. You’ve fished 100 ponds in the surrounding area and have recorded the hourly rate at which you catch salmon while on the water. During your time on the water you also record a couple of measurable variables for each pond: pond depth and dissolved oxygen.</p>
<p>Now, your friend Danny wants go fishing with you this week and suggests two ponds you haven’t fished a before. Also, since Danny is a good friend, they have kindly measured both depth and dissolved oxygen of each pond. Very thoughtful! The question now stands: which pond will you catch more salmon at?</p>
</div>
<div id="data-generation" class="section level2">
<h2>Data generation</h2>
<p>Lets assume that the number of salmon caught <span class="math inline">\(Y\)</span> is poisson distributed with rate <span class="math inline">\(\lambda\)</span>. <span class="math display">\[Y \sim Poi(\lambda)\]</span></p>
<p>Let us also assume that the log rate of catching salmon is a linear function of the two pond variables:</p>
<p><span class="math display">\[\log(\lambda) =  \beta_{depth}\cdot x_{depth} + \beta_{oxy}\cdot x_{oxy} + \beta_0\]</span> Finally, we may write the probability of catching <span class="math inline">\(y\)</span> salmon during one hour of fishing given the above parameters:</p>
<p><span class="math display">\[P(y|x_i;\beta_{depth},\beta_{oxy},\beta_0)=\frac{\lambda^y}{y!}e^{-\lambda}, \hspace{4mm} \lambda=\exp(\beta_{depth}\cdot x_{depth} + \beta_{oxy}\cdot x_{oxy} + \beta_0)\]</span> Now, let’s generate some data!</p>
<pre class="r"><code>library(dplyr)
library(rstan)</code></pre>
<pre class="r"><code># Function to standardize data
standardize &lt;- function(x){
  return((x - mean(x)) / sd(x))
}
set.seed(1234)

N &lt;- 200 # Number of ponds fished at
x_oxy &lt;- rnorm(N, 5, 1) # Dissolved oxygen in mg per Liter
x_depth &lt;- abs(rnorm(N, 30, 10)) # Pond depth

b_oxy &lt;- 0.8
b_depth &lt;- -0.6
b_int &lt;- 0.4
lambda &lt;- exp(b_oxy*standardize(x_oxy) + b_depth*standardize(x_depth) + b_int)
y &lt;- rpois(length(lambda), lambda = lambda)

par(mfrow=c(1,2))
plot(x_oxy, y, pch=16, col=&#39;darkblue&#39;, ylab=&#39;Fish Caught Per Hour&#39;, xlab=&#39;Dissolved Oxygen (mg/L)&#39;)
plot(x_depth, y, pch=16, col=&#39;darkred&#39;, ylab=&#39;Fish Caught Per Hour&#39;, xlab=&#39;Pond Depth (m)&#39;)</code></pre>
<p><img src="poisson_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="mcmc-with-metropolis-hastings" class="section level2">
<h2>MCMC with Metropolis Hastings</h2>
<div id="theory" class="section level3">
<h3>Theory</h3>
<div id="model-definition" class="section level4">
<h4>Model Definition</h4>
<p>First, we will assume that the parameters <span class="math inline">\(\beta_{depth}\)</span>, <span class="math inline">\(\beta_{oxy}\)</span>, and <span class="math inline">\(\beta_0\)</span> are independent of each other such that the joint probability distribution for the parameters can be expressed as:</p>
<p><span class="math display">\[P(\beta_{depth}, \beta_{oxy}, \beta_0 ) = P(\beta_{depth} ) \cdot P(\beta_{oxy})\cdot P(\beta_0)\]</span></p>
<p>Now, observing the data from the plots above, we can assume that <span class="math inline">\(\beta_{depth}\)</span> will be positive and <span class="math inline">\(\beta_{oxy}\)</span> to be negative. Thus, we can set out priors to reflect this observation:</p>
<p><span class="math display">\[\begin{align}
Y &amp;\sim Poi(\lambda) \\
\log(\lambda) &amp;=  \beta_{depth}\cdot x_{depth} + \beta_{oxy}\cdot x_{oxy} + \beta_0 \\
\beta_{depth} &amp;\sim N(0.5, 0.5) \\
\beta_{oxy} &amp;\sim N(-0.5,0.5) \\
\beta_0 &amp;\sim N(0, 0.5)
\end{align}\]</span></p>
<p>Let’s now define our likelihood and priors in R:</p>
<pre class="r"><code>log_poi_liklihood &lt;- function(params){
  lam_cands &lt;- exp(params[[&#39;b_oxy_cand&#39;]]*standardize(x_oxy) + 
                     params[[&#39;b_depth_cand&#39;]]*standardize(x_depth) +
                     params[[&#39;b_int_cand&#39;]])
  return(sum(dpois(y, lam_cands, log=T)))
}

log_prior &lt;- function(params){
  return(
    dnorm(params[[&#39;b_oxy_cand&#39;]], 0.5, 0.5, log=T) +
    dnorm(params[[&#39;b_depth_cand&#39;]], -0.5, 0.5, log=T) +
    dnorm(params[[&#39;b_int_cand&#39;]], 0, 0.5, log=T)
  )
}

log_posterior_prob &lt;- function(params){
  return(log_prior(params) + log_poi_liklihood(params))
}</code></pre>
<p>While defining our priors it is always good practice to make sure that they make sense by checking the prior predictive distribution. We can do this by taking some samples from our prior distributions and using them in place of our model.</p>
<pre class="r"><code>n_prior_samples &lt;- 1e3
sample_priors &lt;- cbind(rnorm(n_prior_samples, 0.5, 0.5),
                       rnorm(n_prior_samples, -0.5, 0.5),
                       rnorm(n_prior_samples, 0,0.5))

prior_predicitive &lt;- cbind(standardize(x_oxy), standardize(x_depth)) %&gt;% apply(., 1, function(x) 
  rpois(n=n_prior_samples, exp(x[1]*sample_priors[,1] + x[2]*sample_priors[,2] + sample_priors[,3]) ))

hist(prior_predicitive %&gt;% log(),prob=T,ylim=c(0,0.8), col=adjustcolor(&#39;darkblue&#39;, alpha.f = 0.5),
     main=&#39;Log of Prior Predictive and y&#39;, xlab=&#39;&#39;, ylab=&#39;&#39;)
hist(y %&gt;% log(), prob=T, add=T, col=adjustcolor(&#39;darkred&#39;, alpha.f = 0.5))
legend(&#39;topright&#39;, c(&#39;Prior Predidictive&#39;,&#39;y&#39;), fill=c(adjustcolor(&#39;darkblue&#39;, alpha.f = 0.5),
                                                       adjustcolor(&#39;darkred&#39;, alpha.f = 0.5)))</code></pre>
<p><img src="poisson_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="mcmc" class="section level4">
<h4>MCMC</h4>
<p>Metropolis Hastings is rejection sampler defined as the following.</p>
<p>Suppose we have some sampled value <span class="math inline">\(x_t\)</span> and some function <span class="math inline">\(P(x)\)</span> the returns the probability of a given <span class="math inline">\(x\)</span>. We also have some proposal function that generates a new <span class="math inline">\(x_{t+1}\)</span> given a previously sampled <span class="math inline">\(x_t\)</span> defined by <span class="math inline">\(g(x_{t+1}|x_t)\)</span>.</p>
<p>Now, we need some way to “reject” or “accept” some newly generated <span class="math inline">\(x_{t+1}\)</span> value from our function <span class="math inline">\(g\)</span>. Define this probability of acceptance to be</p>
<p><span class="math display">\[a=\frac{P(x_{t+1})g(x_t|x_{t+1})}{P(x_t)g(x_{t+1}|x_t)}\]</span> Usually (and for our case today), we’ll choose a function <span class="math inline">\(g\)</span> such that <span class="math inline">\(g\)</span> is symmetric, or <span class="math inline">\(g(x_t|x_{t+1})=g(x_{t+1}|x_t)\)</span>. A common choice to achieve this property would be to assume <span class="math inline">\(g\)</span> is normal with mean equal to the given point. In other words <span class="math display">\[g(x_t|x_{t+1})\sim N(x_{t+1},\sigma)\]</span></p>
<p>Note, here <span class="math inline">\(P(x)\)</span> will be the probability of our sampled <span class="math inline">\(\{\beta_{depth}, \beta_{oxy}, \beta_0 \}\)</span> given our data, as that is the posterior we which to rejection sample from.</p>
<p>Now, let’s write our MCMC algorithm and sample from our posterior! We run 4 different chains to get the best estimate of our posterior.</p>
<pre class="r"><code>N_sim &lt;- 1e5
N_chains &lt;- 4

mcmc_chain &lt;- function(N_sim, explore_param){
  curr_params &lt;-  list(
    b_oxy_cand = rnorm(1, 0, 4),
    b_depth_cand = rnorm(1, 0, 4),
    b_int_cand = rnorm(1, 0, 4)
  )
  chain &lt;- matrix(NA, nrow=N_sim, ncol=3)
  for (i in 1:N_sim){
    cand_params &lt;- list(
      b_oxy_cand = rnorm(1, curr_params[[&#39;b_oxy_cand&#39;]], explore_param),
      b_depth_cand = rnorm(1, curr_params[[&#39;b_depth_cand&#39;]], explore_param),
      b_int_cand = rnorm(1, curr_params[[&#39;b_int_cand&#39;]], explore_param)
    )
    a &lt;- min(1, exp(log_posterior_prob(cand_params) - 
                      log_posterior_prob(curr_params)))
    u &lt;- runif(1)
    if (u &lt;= a){
      chain[i,] &lt;- unlist(cand_params)
      curr_params &lt;- cand_params
    }
    else{
      chain[i,] &lt;- unlist(curr_params)
    }
  }
  return(chain)
}

simulation &lt;- list()
for (i in 1:N_chains){
  simulation[[paste0(&#39;chain_&#39;,i)]] &lt;- mcmc_chain(N_sim, explore_param = 0.01)
}

burn &lt;- 3e4</code></pre>
</div>
</div>
<div id="analyzing-our-chains" class="section level3">
<h3>Analyzing our chains</h3>
<p>Let’s see how well our posteriors match the actual values:</p>
<p><img src="poisson_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Focusing on <span class="math inline">\(\beta_{oxy}\)</span>, let’s see how well our chain converged using rank plots:</p>
<pre class="r"><code>a &lt;- simulation %&gt;% lapply(., function(x) x[-seq(1,burn),1]) %&gt;% unlist() %&gt;% rank() %&gt;% 
  matrix(., ncol=4)
par(mfrow=c(2,2))
for (i in 1:4) hist(a[,i], col=adjustcolor(&#39;darkblue&#39;, alpha.f = 0.5), main=paste0(&#39;Chain_&#39;,i),
                    xlab=&#39;&#39;, ylab=&#39;&#39;)</code></pre>
<p><img src="poisson_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /> Rank plots are calculated by combining all our MCMC samples and finding each samples respective rank. The resulting ranks are separated back into their respective chains and plotted as histograms. If the MCMC sampler converged and the chains mixed well without a high degree of autocorrelation, we can expected uniform distributions for each rank plot.</p>
<p>There a couple of metrics we can look at to assess the convergence of our MCMC sampling. One main metric is <span class="math inline">\(\hat{R}\)</span>. It tells us how well all our chains converged and mixed. A good rule of thumb is to have <span class="math inline">\(\hat{R}\)</span> under 1.05.</p>
<p>The other metrics have to do with effective sample size (ESS). In MCMC sampling, we are assuming a level of independence for samples not directly adjacent. In other words, we are hoping for a low degree of autocorrelation. Simply put, if we have a high degree of autocorrelation in our samples then we effectively have less information describing our posterior. This is what ESS measures, the degree of autocorrelation in our chains. The first, bulk-ESS tells us how well the center or bulk of our posterior has been sampled. The second is tail-ESS, which tells us how well our posterior tails were sampled. A good rule of thumb is to have a bulk-ESS and tail-ESS greater than 400.</p>
<pre class="r"><code>metric_mat &lt;- matrix(NA, nrow=3, ncol=3)
for (i in 1:3){
  metric_mat[1,i] &lt;- Rhat(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% 
                            as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,2)
  metric_mat[2,i] &lt;- ess_bulk(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% 
                                as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,1)
  metric_mat[3,i] &lt;- ess_tail(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% 
                                as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,1)
}
colnames(metric_mat) &lt;- c(&#39;b_oxy&#39;, &#39;b_depth&#39;, &#39;b_0&#39;)
row.names(metric_mat) &lt;- c(&#39;r_hat&#39;, &#39;bulk_ess&#39;, &#39;tail_ess&#39;)
knitr::kable(metric_mat, align = &#39;ccc&#39;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">b_oxy</th>
<th align="center">b_depth</th>
<th align="center">b_0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">r_hat</td>
<td align="center">1.0</td>
<td align="center">1.0</td>
<td align="center">1.0</td>
</tr>
<tr class="even">
<td align="left">bulk_ess</td>
<td align="center">2031.8</td>
<td align="center">2014.0</td>
<td align="center">1193.1</td>
</tr>
<tr class="odd">
<td align="left">tail_ess</td>
<td align="center">4640.0</td>
<td align="center">4357.2</td>
<td align="center">2468.5</td>
</tr>
</tbody>
</table>
<p>A good measure to determine how well model fits our data is to plot the posterior predictive against the observed data.</p>
<pre class="r"><code>posterior_predictive &lt;- cbind(standardize(x_oxy), standardize(x_depth)) %&gt;% apply(., 1, function(x) 
  rpois(n=n_prior_samples, exp(x[1]*posterior_oxy + x[2]*posterior_depth + posterior_int))) %&gt;% c()

hist(posterior_predictive,prob=T, col=adjustcolor(&#39;darkblue&#39;, alpha.f = 0.5),
     breaks=length(unique(posterior_predictive)),
     main=&#39;Posterior Predictive and y&#39;, ylab=&#39;&#39;, xlab=&#39;&#39;)

hist(y, prob=T, add=T, col=adjustcolor(&#39;darkred&#39;, alpha.f = 0.5), breaks=34)
legend(&#39;topright&#39;, c(&#39;Posterior Predictive&#39;,&#39;y&#39;), fill=c(adjustcolor(&#39;darkblue&#39;, alpha.f = 0.5),
                                                       adjustcolor(&#39;darkred&#39;, alpha.f = 0.5)))</code></pre>
<p><img src="poisson_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="decision-analysis" class="section level2">
<h2>Decision Analysis</h2>
<p>Back to the question at hand: what pond should you fish at? Let’s say your friend Danny has the following measurements for the two ponds in question.</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Dissolved Oxygen</th>
<th align="center">Pond Depth</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Pond A</td>
<td align="center">8</td>
<td align="center">40</td>
</tr>
<tr class="even">
<td align="left">Pond B</td>
<td align="center">7</td>
<td align="center">20</td>
</tr>
</tbody>
</table>
<p>From our posterior samples, we can obtain distributions representing our uncertainty for the fish we will catch at each of the ponds in question.</p>
<pre class="r"><code>par(mfrow=c(1,2))
pond_a &lt;- ((ponds$`Dissolved Oxygen`[1] - mean(x_oxy)) / sd(x_oxy)) * posterior_oxy + 
  ((ponds$`Pond Depth`[1] - mean(x_depth)) / sd(x_depth)) * posterior_depth + posterior_int
pond_a &lt;- rpois(length(pond_a), exp(pond_a))
hist(pond_a, breaks=length(unique(pond_a)), prob=T, col=adjustcolor(&#39;darkgreen&#39;, alpha.f = 0.5),
     main=&#39;Pond A&#39;, xlab=&#39;Fish Caught&#39;, ylab=&#39;&#39;)

pond_b &lt;- ((ponds$`Dissolved Oxygen`[2] - mean(x_oxy)) / sd(x_oxy)) * posterior_oxy + 
  ((ponds$`Pond Depth`[2] - mean(x_depth)) / sd(x_depth)) * posterior_depth + posterior_int
pond_b &lt;- rpois(length(pond_b), exp(pond_b))
hist(pond_b, breaks=length(unique(pond_b)), prob=T, col=adjustcolor(&#39;yellow&#39;, alpha.f = 0.5),
     main=&#39;Pond B&#39;, xlab=&#39;Fish Caught&#39;, ylab=&#39;&#39;)</code></pre>
<p><img src="poisson_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now, we can take the difference of the two distributions and come to our conclusion.</p>
<pre class="r"><code>pond_diff &lt;- (pond_b - pond_a)
hist(pond_diff, prob=T, main=&#39;Pond B - Pond A&#39;, xlab=&#39;Difference in fish caught between ponds&#39;, ylab=&#39;&#39;,
      col=adjustcolor(&#39;purple&#39;, alpha.f = 0.5))</code></pre>
<p><img src="poisson_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If we want to find the expected increase in fish in choosing pond B of pond A, it’s as simple as taking the average of our above distribution.</p>
<pre><code>## [1] 3.110529</code></pre>
<p>Hence, after our extensive analysis we can come to the conclusion that it is best to choose Pond B over A. Although, maybe your friend Danny has left by now!</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.1.1 (2021-08-10)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rstan_2.21.3         ggplot2_3.3.5        StanHeaders_2.21.0-7
## [4] bayesplot_1.8.1      dplyr_1.0.9         
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.1   xfun_0.27          bslib_0.3.1        purrr_0.3.4       
##  [5] colorspace_2.0-2   vctrs_0.4.1        generics_0.1.1     htmltools_0.5.2   
##  [9] stats4_4.1.1       loo_2.4.1          yaml_2.2.1         utf8_1.2.2        
## [13] rlang_1.0.3        pkgbuild_1.3.1     jquerylib_0.1.4    pillar_1.6.4      
## [17] glue_1.6.2         withr_2.5.0        DBI_1.1.3          matrixStats_0.61.0
## [21] lifecycle_1.0.1    plyr_1.8.6         stringr_1.4.0      munsell_0.5.0     
## [25] gtable_0.3.0       codetools_0.2-18   evaluate_0.14      inline_0.3.19     
## [29] knitr_1.36         callr_3.7.0        fastmap_1.1.0      ps_1.6.0          
## [33] parallel_4.1.1     fansi_0.5.0        Rcpp_1.0.7         scales_1.1.1      
## [37] RcppParallel_5.1.4 jsonlite_1.7.2     gridExtra_2.3      digest_0.6.28     
## [41] stringi_1.7.5      processx_3.5.2     grid_4.1.1         cli_3.3.0         
## [45] tools_4.1.1        magrittr_2.0.1     sass_0.4.0         tibble_3.1.6      
## [49] crayon_1.4.2       pkgconfig_2.0.3    ellipsis_0.3.2     prettyunits_1.1.1 
## [53] ggridges_0.5.3     assertthat_0.2.1   rmarkdown_2.11     rstudioapi_0.13   
## [57] R6_2.5.1           compiler_4.1.1</code></pre>
<style type="text/css">
pre {
  max-height: 200px;
  overflow-y: auto;
}

pre[class] {
  max-height: 400px;
}

.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
</style>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
