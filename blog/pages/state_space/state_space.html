<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>An Unhinged Introduction to State Space Models with STAN - Theory</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
	<a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a>
  <div class="container">
  	<!-- <a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a> -->
    <div class="navbar-header">
      <!-- NOTE: add "navbar-inverse" class for an alternate navbar background -->
      <!-- <a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a> -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li><a href="https://github.com/TheNudibranch">GitHub</a></li>
        <li><a href="https://www.linkedin.com/in/ian-s-costley/">LinkedIn</a></li>
        <li><a href="../../blog.html">B<sub>natural</sub>LOG</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">An Unhinged Introduction to State Space
Models with STAN - Theory</h1>

</div>


<style type="text/css">
pre {
  max-height: 200px;
  overflow-y: auto;
}

pre[class] {
  max-height: 500px;
}

.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
</style>
<pre class="r"><code>library(cmdstanr)
library(kableExtra)
knitr::opts_chunk$set(cache = F)
options(mc.cores = parallel::detectCores())

state_space_utils &lt;- new.env()
source(&#39;state_space_model_utilities.R&#39;, state_space_utils)


season_length &lt;- 7; number_seasons &lt;- 10
n_tot &lt;- season_length * number_seasons
x1 &lt;- (cumsum(rnorm(n_tot, 0,0.5)) + rnorm(n_tot, 0,.01)) |&gt; abs()
x2 &lt;- (cumsum(rnorm(n_tot, 0,0.5)) + rnorm(n_tot, 0,.01)) |&gt; abs()
x_mat &lt;- cbind(x1,x2)
beta_vec &lt;- c(-1,1)

ss &lt;- cos(seq(0,2*pi, length.out=season_length))*1
ss &lt;- ss - mean(ss)
ss_vec &lt;- c(ss)[-length(ss)]
ss_var &lt;- 0.01^2;

slope_start &lt;- 0; slope_var &lt;- 1e-30; slope_vec &lt;- c(slope_start)

mu_start &lt;- rnorm(1,0,1); mu_var &lt;- 0.05^2; mu_vec &lt;- c(mu_start)

noise_var &lt;- 0.5

y_vec &lt;- ss_comp &lt;-  c()
# i &lt;- 1
for (i in 1:n_tot){
  ss_comp &lt;- c(ss_vec[1], ss_comp)
  y_vec &lt;- c(y_vec, mu_vec[i] + (x_mat %*% beta_vec)[i] + ss_vec[1] + rnorm(1,0,sqrt(noise_var)))
  ss_vec &lt;- c(-sum(ss_vec) + rnorm(1, 0, sqrt(ss_var)), ss_vec[-length(ss_vec)])
  slope_vec &lt;- c(slope_vec, slope_vec[i] + rnorm(1, 0, sqrt(slope_var)))
  mu_vec &lt;- c(mu_vec, mu_vec[i] + slope_vec[i] + rnorm(1, 0,sqrt(mu_var)))
}

# Add 20% of series mean to the last 10 iterations
is_new_prod_live &lt;- seq_along(y_vec) &gt;= length(y_vec) - 9 
y_vec[is_new_prod_live] &lt;- abs(y_vec[is_new_prod_live])*0.5 + 
  y_vec[is_new_prod_live]

data.frame(leni=y_vec, abdominable=x1, bigfoot=x2) |&gt; 
  write.csv(&#39;state_space_data.csv&#39;, row.names = F)</code></pre>
<div id="casting-calls" class="section level1">
<h1>Casting Calls</h1>
<div id="motivation-more-ice-please" class="section level2">
<h2>Motivation: More Ice Please</h2>
<p>Let’s say you work for a drinkware company, we’ll call it something
generic, maybe “<strong>L</strong>arge bi-p<strong>E</strong>dal
s<strong>N</strong>ow crypt<strong>I</strong>d”? Or LENI for short. LENI
sells mugs, wine glasses, water glasses, and tumblers. The recent lack
of innovation in the tumbler market has led to public outcry. As a
response, you release Tumbler v2™. This exciting new technology features
the same size tumblers you know an love, but with double the ice
capacity. You even come up with a catchy slogan:</p>
<center>
<em>“Double the ice, for the exact same price.” - LENI</em>
</center>
<p>As a result of your genius ideas, your tumbler sales being to pick
up:</p>
<pre class="r"><code>plot(y_vec, type=&#39;n&#39;, lwd=3, cex.axis=1.2, cex=1.2, cex.main=1.5, main=&#39;Drinkware Company Sales&#39;,
     ylab=&#39;&#39;, xlab=&#39;&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, ylim=range(c(y_vec, x1, x2, ss_comp)))
axis(side=1, at=mean(seq_along(y_vec)), labels=&#39;Time&#39;, cex.axis=1.3, lwd.ticks = 0)
axis(side=2, at=mean(y_vec), labels=&#39;Sales ($)&#39;, cex.axis=1.3, lwd.ticks = 0)
lines(x1, lwd=3, col=adjustcolor(&#39;darkgoldenrod1&#39;, 0.5))
lines(x2, lwd=3, col=adjustcolor(&#39;violet&#39;, 0.5))
lines(y_vec, lwd=3, type=&#39;b&#39;, pch=16)
# lines(ss_comp, lwd=3, pch=16, col=&#39;gold&#39;)
box(lwd=2)
abline(v = length(y_vec)-9.5, lty=2, lwd=3)
legend(&#39;topleft&#39;, legend=c(&#39;LENI&#39;, &#39;Abominable Snowman&#39;, &#39;Bigfoot&#39;, &#39;Release of Tumbler v2™&#39;), 
       lty=c(1,1,1,2), lwd=rep(3,4),
       col=c(&#39;black&#39;, &#39;darkgoldenrod&#39;, &#39;violet&#39;, &#39;black&#39;), bg=adjustcolor(&#39;white&#39;, 0.5), pch=c(16,rep(NA,3)))</code></pre>
<p><img src="state_space_files/figure-html/unnamed-chunk-3-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="the-final-frontier" class="section level2">
<h2>The final frontier</h2>
<p>Your tumbler sales are putting all your rival drinkware companies,
like “Abominable Snowman”, to shame. But just how much of your sales are
incremental and aren’t just due to the usual seasonality, or positive
overall industry growth. Can you quantify with some degree of certain
the incremental sales you’ve realized from launching Tumbler v2™?</p>
<p>To approach this problem, we will use state space models. A class of
models that assumes that our data generating process has some underlying
state that gives rise to our actually observed variables. For instance,
in the time series plot above, a state space approach would assume that
each point in the time series is generated from some latent unobserved
state which has its own distribution. For this time series analysis
state space problem, we will used the widely adopted and used approach
of <span class="citation">(James Durbin and Koopman 2012)</span>. The
governing equation for this system is given by:</p>
<p><span class="math display">\[\begin{equation}
y_t = Z_t\alpha_t + \varepsilon_t \hspace{10mm} \text{(1)}
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\alpha_{t+1} = T_t \alpha_t + R_t \eta_t \hspace{6mm} \text{(2)}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\varepsilon_t \sim \mathcal{N}(0,H_t)
\hspace{2mm} \text{and} \hspace{2mm} \eta_t \sim
\mathcal{N}(0,Q_t)\]</span> You might only recognize <span
class="math inline">\(y_t\)</span>, the sales from our Tumbler v2, from
the equation above. But allow me to introduce the full cast of
characters:</p>
<pre class="r"><code>vec_var &lt;- c(&#39;$y_t$&#39;, &#39;$\\alpha_t$&#39;, &#39;$\\varepsilon_t$&#39;, &#39;$\\eta_t$&#39;, &#39;&#39;, &#39;$a_1$&#39;)
vec_dim &lt;- c(&#39;$p \\times 1$&#39;, &#39;$m \\times 1$&#39;, &#39;$p \\times 1$&#39;, &#39;$r \\times 1$&#39;, &#39;&#39;, &#39;$m \\times 1$&#39;)
vec_desc &lt;- c(&#39;Observations&#39;, &#39;State&#39;, &#39;Obs. Disturbance&#39;, &#39;State Disturbance&#39;, &#39;&#39;, &#39;Initial State Expected Value&#39;)
mat_vec &lt;- c(&#39;$Z_t$&#39;, &#39;$T_t$&#39;, &#39;$H_t$&#39;, &#39;$R_t$&#39;, &#39;$Q_t$&#39;, &#39;$P_1$&#39;)
mat_dim &lt;- c(&#39;$p \\times m$&#39;, &#39;$m \\times m$&#39;, &#39;$p \\times p$&#39;, &#39;$m \\times r$&#39;, &#39;$r \\times r$&#39;,
             &#39;$m \\times m$&#39;)
mat_desc &lt;- c(&#39;Design Matrix&#39;, &#39;Transition Matrix&#39;, &#39;Obs. Covariance Matrix&#39;, &#39;State Disturbance Selection Matrix&#39;,
              &#39;State Covariance Matrix&#39;, &#39;Initial State Covariance Matrix&#39;)
cbind(vec_var, vec_dim, vec_desc, mat_vec, mat_dim, mat_desc) %&gt;%
  kable(col.names=NULL) %&gt;% kable_styling(position=&#39;center&#39;) %&gt;%
  add_header_above(c(&#39;Vectors&#39;=3, &#39;Matricies&#39; = 3))</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Vectors
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Matricies
</div>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(p \times 1\)</span>
</td>
<td style="text-align:left;">
Observations
</td>
<td style="text-align:left;">
<span class="math inline">\(Z_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(p \times m\)</span>
</td>
<td style="text-align:left;">
Design Matrix
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\alpha_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(m \times 1\)</span>
</td>
<td style="text-align:left;">
State
</td>
<td style="text-align:left;">
<span class="math inline">\(T_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(m \times m\)</span>
</td>
<td style="text-align:left;">
Transition Matrix
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\varepsilon_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(p \times 1\)</span>
</td>
<td style="text-align:left;">
Obs. Disturbance
</td>
<td style="text-align:left;">
<span class="math inline">\(H_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(p \times p\)</span>
</td>
<td style="text-align:left;">
Obs. Covariance Matrix
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\eta_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(r \times 1\)</span>
</td>
<td style="text-align:left;">
State Disturbance
</td>
<td style="text-align:left;">
<span class="math inline">\(R_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(m \times r\)</span>
</td>
<td style="text-align:left;">
State Disturbance Selection Matrix
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
<span class="math inline">\(Q_t\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(r \times r\)</span>
</td>
<td style="text-align:left;">
State Covariance Matrix
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(a_1\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(m \times 1\)</span>
</td>
<td style="text-align:left;">
Initial State Expected Value
</td>
<td style="text-align:left;">
<span class="math inline">\(P_1\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(m \times m\)</span>
</td>
<td style="text-align:left;">
Initial State Covariance Matrix
</td>
</tr>
</tbody>
</table>
<p>Here <span class="math inline">\(\alpha_t\)</span> is the underlying
latent state that gives rise to our time series realizations <span
class="math inline">\(y_t\)</span>. As stated before, the latent state
<span class="math inline">\(\alpha_t\)</span> has its own distribution,
which we can now deduce from the equation above is given by <span
class="math inline">\(\alpha_t \sim \mathcal{N}(a_t, Q_t)\)</span>.
Notice from our table above that <span
class="math inline">\(\alpha_t\)</span> is not the same size as <span
class="math inline">\(y_t\)</span>. Our observations <span
class="math inline">\(y_t\)</span> is a vector of length <span
class="math inline">\(p\)</span>, which will be 1 for our case here.
Conversely, <span class="math inline">\(\alpha_t\)</span> is of size
<span class="math inline">\(m\)</span>. This is because <span
class="math inline">\(\alpha_t\)</span> will act as the conduit for all
the effects and assumptions we impose on the model.</p>
<p>This is one of the best utilities provided by state space models. We
have the ability to add smaller effects together and extract them at the
end of the modeling process to further understand the structure of our
time series. For example, you might want to impose a 52 week seasonality
(1 year), positive industry trend growth, additive regression effects,
and maybe even a AR(n) process to account for autocorrelation. All of
this information is packed in <span
class="math inline">\(\alpha_t\)</span> which we’ll demonstrate
later.</p>
<p>The matrix <span class="math inline">\(Z_t\)</span> provides the
translation from <span class="math inline">\(\mathbb{R}^m\)</span> to
<span class="math inline">\(\mathbb{R}^p\)</span>, which our case is
just one dimension. Therefore, <span class="math inline">\(Z_t\)</span>
reduces down to a vector of length <span
class="math inline">\(m\)</span>, but this is not the case in general.
Some of the data that <span class="math inline">\(Z_t\)</span> might
hold is the <span class="math inline">\(x_t\)</span> for the regression
component of our time series. Usually, <span
class="math inline">\(Z_t\)</span> is filled with 1s and 0s to select
different components of <span class="math inline">\(\alpha_t\)</span>
that are used for <span class="math inline">\(y_t\)</span>. As we’ll see
later, there are many components of <span
class="math inline">\(\alpha_t\)</span> that don’t directly impact <span
class="math inline">\(y_t\)</span>, but are strategically placed to help
<span class="math inline">\(\alpha_t\)</span> itself evolve over
time.</p>
<p>This is a great segue to our transition matrix <span
class="math inline">\(T_t\)</span>. The transition matrix is what
evolves <span class="math inline">\(\alpha_t\)</span> to the next state
<span class="math inline">\(\alpha_{t+1}\)</span>. This again can take
multiple forms. It might be the stage in the process where a positive
industry trend is added to the overall series or where we move from
season to season.</p>
<p>Finally we have <span class="math inline">\(R_t\)</span>, the state
disturbance selection matrix. Notice that everything up to this point
has had a <span class="math inline">\(t\)</span> subscript. While we are
modeling a time series, it is not true that every element of <span
class="math inline">\(\alpha_t\)</span> has to vary over time. In fact,
most of the time the regression coefficients (if included) are usually
static with respect to time. The way we tell this model to account for
static elements of <span class="math inline">\(\alpha_t\)</span> is to
not add any disturbance to them. Lets consider a very simple local
linear trend model:</p>
<p><span class="math display">\[\begin{align}
y_t &amp;= \mu_t + \varepsilon_t\\
\mu_{t + 1} &amp;= \mu_{t} + \nu_{t} + \eta_{t}\\
\nu_{t+1} &amp;= \nu_{t}
\end{align}\]</span></p>
<p>Our cast of characters now wear the following costumes:</p>
<p><span class="math display">\[\alpha_t = \begin{bmatrix} \mu_t \\
\nu_t\end{bmatrix} \hspace{10mm} Z_t = \begin{bmatrix} 1 &amp; 0\\
\end{bmatrix}\]</span> <span class="math display">\[T_t =
\begin{bmatrix}1 &amp; 1\\ 0 &amp; 1 \end{bmatrix} \hspace{5mm} R_t =
\begin{bmatrix}1\\ 0\end{bmatrix}\]</span> Clearly, <span
class="math inline">\(\eta_t\)</span> is just a 1-D normal with variance
<span class="math inline">\(Q_t\)</span>, and thus as <span
class="math inline">\(\alpha_t\)</span> evolves over time its element
<span class="math inline">\(\nu_{n+1} = \nu_{n}\)</span> for all <span
class="math inline">\(n\in [1,t]\)</span>. The variable <span
class="math inline">\(\nu\)</span> here could be considered a static
industry trend and <span class="math inline">\(\mu_t\)</span> is the
random walk it perturbs. I’m sure you can already see the powerful
flexibility we’ve been afforded by this modeling system.</p>
<p>Now, the only variables I have yet to mention are <span
class="math inline">\(a_1\)</span> and <span
class="math inline">\(P_1\)</span>. These dictate the distribution of
our starting state. That is <span class="math inline">\(\alpha_1 \sim
\mathcal{N}(a_1, P_1)\)</span>. These initial values are assumed to be
known at the time of modeling. In reality these are rarely known and
there is a large amount of theory to approximate the best starting value
of these based on asymptotics. For more information, check out chapter 5
of <span class="citation">(James Durbin and Koopman 2012)</span>. But,
we are Bayesians after all are we not? If we don’t know something we
just chuck a prior at it. Although we will abide by the suggestion in
chapter 5 and set <span class="math inline">\(a_1 = \bf{0}\)</span>.</p>
<p>I don’t know if you can tell, but this notation is incredibly loaded,
and unfortunately it is only going to get worse before it gets better.
The most loaded of them all is understanding when we are talking about a
state vector <span class="math inline">\(\alpha_t\)</span> or the
expected value of a state, denoted by <span
class="math inline">\(a_t\)</span>. To hopefully alleviate some of
notation burden, here is a footnote that shows the definition for all
<span class="math inline">\(\alpha_t\)</span> or <span
class="math inline">\(a_t\)</span> variations we might encounter.<a
href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
</div>
<div id="i-prefer-autumn" class="section level2">
<h2>I prefer Autumn</h2>
<p>One of the key components we’ll utilize is seasonality. This means
that for some repeating point in time, we want to see the same, or
slightly modified effect. For example, we might observe seasonality in
consumer purchasing behavior. Big spikes occur during the holidays when
everyone is purchasing Tumbler v2s to give to their ice enthusiast
friends at Christmas, but then sales slowly die off. It’s a tale as old
as time that we’ll see again next Christmas.</p>
<p>We’ll go about this by considering a 4 season example. This might be
data that is collected every quarter for a company. We’ll define four
season parameters denoted by <span
class="math inline">\(\tau_j\)</span>, where <span
class="math inline">\(j \in [1,4]\)</span>. We’ll make the assumption
that all seasonal effects will add to zero. This makes sense if we
assume that a seasonal effect is just some perturbation against the
underlying trend. Consider the following:</p>
<pre class="r"><code>n_ex1 &lt;- 4*5
ss_comp_ex1 &lt;- cos(seq(0,2*pi, length.out=4))*2
ss_ex1 &lt;- rep(ss_comp_ex1 - mean(ss_comp_ex1),5)
trend_ex1 &lt;- cumsum(rnorm(n_ex1) + 0.5)

t_cex &lt;- 1.6

par(mfrow=c(3,1))
# layout_mat &lt;- matrix(c(1,2,3,3), nrow=2, byrow=T)
# layout(layout_mat)
par(mar=c(5,3,2,2)-0.5)
plot(ss_ex1, type=&#39;b&#39;, lwd=3, pch=16, xlab=&#39;&#39;, ylab=&#39;&#39;,
     main=&#39;Seasonality&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;,cex.main=t_cex)
box(lwd=2)
plot(trend_ex1, pch=16, type=&#39;b&#39;, lwd=3, xlab=&#39;&#39;, ylab=&#39;&#39;,
     main=&#39;Underlying Trend&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, cex.main=t_cex)
box(lwd=2)
plot(ss_ex1 + trend_ex1, lwd=3, pch=16, type=&#39;b&#39;, xlab=&#39;&#39;, ylab=&#39;&#39;, 
     main=&#39;Observed Trend&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, cex.main=t_cex)
box(lwd=2)</code></pre>
<p><img src="state_space_files/figure-html/unnamed-chunk-5-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>Let’s write our system in plain Greek and forget about the matrices
for a second:</p>
<p><span class="math display">\[\begin{align}
y_t &amp;= \mu_t + \tau_t + \varepsilon_t\\
\mu_{t + 1} &amp;= \mu_{t} + \eta_{t}^\mu\\
\tau_{t+1} &amp;= -\sum_{j=1}^{s-1}\tau_{t + 1 - j} + \eta_t^\tau
\end{align}\]</span></p>
<p>For some, that summation might have come out of left field. But
recall that we defined our seasonality components to sum to zero.
Therefore, for any <span class="math inline">\(s\)</span> length
seasonality trend, we only need <span class="math inline">\(s-1\)</span>
components to recover the full trend. For example if we have a seasonal
trend of length 4 and trying to find the expected seasonal component of
time <span class="math inline">\(t+1\)</span>, we can use the
following:</p>
<p><span class="math display">\[\begin{align}
0 &amp;= \tau_{t+1} + \tau_{t} + \tau_{t-1} + \tau_{t-2}\\
\tau_{t+1} &amp;= -(\tau_{t} + \tau_{t-1} + \tau_{t-2})\\
\tau_{t+1} &amp;= -\sum_{j=1}^{s-1}\tau_{t + 1 - j}\\
\end{align}\]</span></p>
<p>Since we want to allow our seasonality to vary over time, we add a
disturbance <span class="math inline">\(\eta_t^\tau\)</span> every time
the next expected seasonality component <span
class="math inline">\(\tau_{t+1}\)</span> is calculated. Note, since we
want both <span class="math inline">\(\mu_t\)</span> and <span
class="math inline">\(\tau_t\)</span> to vary over time, <span
class="math inline">\(\eta_t\)</span> will be a vector of length 2.
Finally, we can write our state space system matrices as:</p>
<p><span class="math display">\[\alpha_t = \begin{bmatrix} \mu_t \\
\tau_{t} \\ \tau_{t-1} \\ \tau_{t-2}\end{bmatrix} \hspace{10mm} Z_t =
\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0\\ \end{bmatrix}\]</span> <span
class="math display">\[T_t = \begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; -1 &amp; -1 &amp; -1\\0 &amp; 1 &amp; 0 &amp; 0\\0 &amp; 0 &amp;
1 &amp; 0\end{bmatrix} \hspace{5mm} R_t = \begin{bmatrix}1 &amp; 0\\ 0
&amp; 1\\0 &amp;0\\0&amp;0\end{bmatrix}\]</span> The 1s in the third and
fourth row of <span class="math inline">\(T_t\)</span> ensure that we
carry the <span class="math inline">\(s-2\)</span> most recent seasonal
components with us to the next state. The 0s in the third and fourth row
of <span class="math inline">\(R_t\)</span> <em>select</em> our
disturbance draw such that no variation will be applied to the <span
class="math inline">\(s-2\)</span> carryover seasonal states.</p>
</div>
<div id="who-got-a-call-back" class="section level2">
<h2>Who got a call back?</h2>
<p>So now that we’ve hosted the auditions, who should we call back to
comprise our final ensemble? Looking at the sales trend line, we see
that there are two main competitors: “Abominable Snowman” and “Bigfoot”.
We’ll use those two trends as covariates for our state space model. We
can omit the slope <span class="math inline">\(\nu_t\)</span> with our
reasoning being that any industry trend will already be captured by
including the covariates. Finally, a 7 day seasonality trend will be
considered to account for any within week variation. In practice, this
can usually be also be omitted if we assume that the covariates can
explain this variation as well, but we’ll keep seasonality for
demonstration.</p>
<p>Now, let me introduce the stars of the play:</p>
<p><span class="math display">\[\alpha_t = \begin{bmatrix} \mu_t \\
\tau_{t} \\ \tau_{t-1} \\ \tau_{t-2}\\ \tau_{t-3}\\ \tau_{t-4}\\
\tau_{t-5}\\ \beta_a\\ \beta_b\end{bmatrix} \hspace{10mm} Z_t =
\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;
x_{a} &amp; x_b\\ \end{bmatrix}\]</span> where <span
class="math inline">\((\beta_a, \beta_b)\)</span> and <span
class="math inline">\((x_a, x_b)\)</span> are the regression
coefficients and observed sales trend for “Abominable Snowman” and
“Bigfoot”, respectively.</p>
<p><span class="math display">\[T_t = \begin{bmatrix}1 &amp; 0 &amp; 0
&amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; -1 &amp; -1 &amp; -1 &amp; -1 &amp; -1 &amp; -1 &amp; 0 &amp;
0\\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;
1\\\end{bmatrix} \hspace{5mm}
R_t = \begin{bmatrix}1 &amp; 0\\ 0 &amp; 1\\0
&amp;0\\0&amp;0\\0&amp;0\\0&amp;0\\0&amp;0\\0&amp;0\\0&amp;0\end{bmatrix}\]</span></p>
<p>Since we are only varying the seasonality and <span
class="math inline">\(\mu\)</span> over time, our disturbance <span
class="math inline">\(\eta_t\)</span> covariance matrix is given by
<span class="math inline">\(Q_t = \text{diag}(\sigma_{\mu},
\sigma_{\tau})\)</span>. Furthermore, since <span
class="math inline">\(y_t\)</span> is univariate, our disturbance <span
class="math inline">\(\varepsilon_t\)</span> has scalar variance <span
class="math inline">\(H_t = \sigma_y\)</span>.</p>
</div>
</div>
<div id="setting-the-stage" class="section level1">
<h1>Setting the Stage</h1>
<div id="mine-is-a-brita" class="section level2">
<h2>Mine is a Brita</h2>
<p>The first step of our modeling process is
<strong>filtering</strong>.</p>
<p>For sake of completeness, let’s review the assumptions<a href="#fn2"
class="footnote-ref" id="fnref2"><sup>2</sup></a> we made about our data
generating process by imposing this system of equations:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(y_t\)</span> is a linear function of the
latent state <span class="math inline">\(\alpha_t\)</span></li>
<li>The disturbances of our data generating process are normally
distributed</li>
<li>The disturbances of our latent state are normally distributed.</li>
</ol>
<p>These unassuming<a href="#fn3" class="footnote-ref"
id="fnref3"><sup>3</sup></a> assumptions allow us to now leverage the
power of the <a
href="https://www.kalmanfilter.net/background.html">Kalman filter</a>.
Without going into too much detail, filtering is just the process of
updating what we expect given what we have observed. Assume that we have
seen the first 4 realizations of some time series process. We are not in
the dark about. We have <em>some</em> knowledge about what the 5th
observation might be. If the first 4 had values between 1 and 10, we can
be pretty sure that the 5th realization won’t be 1,000. This is of
course a gross oversimplification, but if our process follows the
assumptions outlined above, we can use the Kalman filter to quantify the
uncertainty and expected value of our 5th observation.</p>
<p>If we had the time I would write out the derivation for you, but I
already feel like I’m going to write too much. Instead, I refer you to
page 82-85 of <span class="citation">(James Durbin and Koopman
2012)</span> or this <a
href="https://nrotella.github.io/journal/kalman-filter-derivation-interpretation.html">cool
blog post</a>. Now, you’ve met the star cast but do you know their
origin story:</p>
<p><span class="math display">\[
\begin{align*}
v_t &amp;= y_t - Z_ta_t &amp; F_t &amp;= Z_t P_t Z^\prime_t + H_t \\
a_{t|t} &amp;= a_t + P_t Z^\prime_t F^{-1}_t v_t &amp; P_{t|t} &amp;=
P_t - P_t Z^\prime_t F^{-1}_t Z_t P_t \\
a_{t+1} &amp;= T_t a_t + K_t v_t &amp; P_{t+1} &amp;= T_t P_t (T_t - K_t
Z_t)^\prime + R_t Q_t R^\prime_t
\end{align*}
\]</span> Looks like we picked up some side characters on the way. The
matrix <span class="math inline">\(K_t\)</span> is referred to as the
Kalman Gain and is given by <span class="math inline">\(K_t =
T_tP_tZ^\prime_tF^{-1}_t\)</span>. It encodes how much we should trust a
given observation <span class="math inline">\(y_t\)</span>. If the gain
is high, then the observation uncertainty is low and the next predicted
state has a high dependence on the previous observation. If the gain is
low, there is high uncertainty in the observation and thus we put most
of our weight in the previous predicted state <span
class="math inline">\(a_t\)</span>. For a more detailed inerpretation,
take a look at this <a
href="https://dsp.stackexchange.com/questions/2347/how-to-understand-kalman-gain-intuitively">stackoverflow
post</a>.</p>
<p>The matrix <span class="math inline">\(F_t\)</span> is defined as
<span class="math inline">\(\textrm{Var}[v_t|Y_{t-1}]\)</span> and the
conditional expected value and variance of the state is defined as:</p>
<p><span class="math display">\[
\begin{align*}
a_{t|t}&amp;=\mathbb{E}[\alpha_t|Y_t] &amp;
P_{t|t}&amp;=\textrm{Var}[\alpha_t|Y_t] \\
\end{align*}
\]</span> where <span class="math inline">\(Y_t =
\{y_1,y_2,\dots,y_t\}\)</span>. In practice, you can actually ignore the
middle row (<span class="math inline">\(*_{t|t}\)</span>) but we include
it for completeness. Now, let’s apply the above equations to produce the
filtered expected states <span class="math inline">\(a_{t|t}\)</span>.
Since we are trying to understand the state of the system prior to our
launch of the Tumbler v2™, we will exclude the data post launch from our
analysis.</p>
<pre class="r"><code>y_vec_no_prod &lt;- y_vec[!is_new_prod_live]
k_obj &lt;- state_space_utils$forward_backward_pass(
  y_vec_no_prod, a_1=rep(0,9), 
  P_1 = state_space_utils$iden(9)*0.5,
  var_vec = c(mu_var, ss_var), 
  noise_var = c(noise_var),
  params = list(&#39;include_slope&#39;=F, x_mat=cbind(x1,x2)[!is_new_prod_live,], n_seasons=7)
)

filtered &lt;- y_vec_no_prod - (k_obj$forw$v |&gt; unlist())
smoothed &lt;- y_vec_no_prod - (k_obj$back$v_smth |&gt; unlist())

sim &lt;- state_space_utils$simulate_state(
  k_obj$back$alpha_smth, a_1=rep(0,9), 
  P_1 = state_space_utils$iden(9)*0.5,
  var_vec = c(mu_var, ss_var), noise_var = c(noise_var),
  params = list(&#39;include_slope&#39;=F, x_mat=cbind(x1,x2)[!is_new_prod_live,], n_seasons=7)
)

plot(y_vec_no_prod, type=&#39;n&#39;, lwd=3, cex.axis=1.2, cex=1.2, cex.main=1.5, 
     ylab=&#39;&#39;, xlab=&#39;&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, ylim=range(c(y_vec_no_prod, filtered, smoothed)))
axis(side=1, at=mean(seq_along(y_vec_no_prod)), labels=&#39;Time&#39;, cex.axis=1.3, lwd.ticks = 0)
axis(side=2, at=mean(y_vec_no_prod), labels=&#39;Sales ($)&#39;, cex.axis=1.3, lwd.ticks = 0)
lines(filtered, lwd=3, col=&#39;darkred&#39;)
lines(y_vec_no_prod, lwd=3, type=&#39;b&#39;, pch=16)
box(lwd=2)
legend(&#39;topleft&#39;, legend=c(&#39;LENI Sales&#39;, &#39;LENI Sales Filtered&#39;), lwd=3,col=c(&#39;black&#39;, &#39;darkred&#39;), pch=c(16,NA))</code></pre>
<p><img src="state_space_files/figure-html/unnamed-chunk-6-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="dont-forget-to-moisturize" class="section level2">
<h2>Don’t forget to Moisturize</h2>
<p>So you’ve done your filtering, but we don’t want dry flaky equations.
For that you’re going to need to moisturize, or as the mathematicians
call it: <strong>smoothing</strong>. You can think of filtering as the
forward pass and smoothing as the backward pass. Filtering helped us get
the finish line and find the next expected quantity given what we have
seen so far. But now that we are at the final time step, we can go
backwards and update our states with the entire series of observations,
<span class="math inline">\(Y_n\)</span>. There are smoothing equations
for the smoothed disturbances of <span
class="math inline">\(\varepsilon_t\)</span> and <span
class="math inline">\(\eta_t\)</span>, which we will denote <span
class="math inline">\(\hat{\varepsilon}_t\)</span> and <span
class="math inline">\(\hat{\eta}_t\)</span> respectively, but we will
omit those since we are only concerned with the smoothed state <span
class="math inline">\(\hat{\alpha}_t\)</span> inference.</p>
<p>We define <span class="math inline">\(\hat{\alpha}_t =
\mathbb{E}[\alpha_t|Y_n]\)</span>. Note, this is different from <span
class="math inline">\(a_{t|t}\)</span> in filtering due to the
condition. The series <span class="math inline">\(Y_t\)</span> is
defined as the series of observations up to time <span
class="math inline">\(t\)</span> given by <span
class="math inline">\(Y_t = \{y_1, y_2, \dots,y_t\}\)</span>, but <span
class="math inline">\(Y_n\)</span> is the entire series of <span
class="math inline">\(n\)</span> observations. As with filtering, I omit
the proof of the smoothing equations and instead refer you to pages
88-91 of <span class="citation">(James Durbin and Koopman 2012)</span>.
The smoothing equations are relatively simpler, given by</p>
<p><span class="math display">\[
\begin{align*}
r_{t-1} &amp;= Z_t^\prime F_t^{-1} v_t + L_t^\prime r_t &amp;
\hat{\alpha_t} &amp;= a_t + P_t r_{t-1}
\end{align*}
\]</span> What fresh hell is this? We just keep getting more and more
cast members? I promise, this was the last audition, they’ll be no more
late additions. The matrix <span class="math inline">\(L_t\)</span> is
present only for convenience and is defined as <span
class="math inline">\(L_t = T_t - K_t Z_t\)</span>. As I said before,
this is the backward pass. When we start to compute our smoothed states,
we will go in reverse following <span
class="math inline">\(t=n,\dots,1\)</span>, with <span
class="math inline">\(r_n = 0\)</span>. Let’s see how our smoothed trend
<span class="math inline">\(\hat{\alpha}_t\)</span> compares to our
filtered trend:</p>
<pre class="r"><code>plot(y_vec_no_prod, type=&#39;n&#39;, lwd=3, cex.axis=1.2, cex=1.2, cex.main=1.5,
     ylab=&#39;&#39;, xlab=&#39;&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, ylim=range(c(y_vec_no_prod, filtered, smoothed)))
axis(side=1, at=mean(seq_along(y_vec_no_prod)), labels=&#39;Time&#39;, cex.axis=1.3, lwd.ticks = 0)
axis(side=2, at=mean(y_vec_no_prod), labels=&#39;Sales ($)&#39;, cex.axis=1.3, lwd.ticks = 0)
lines(filtered, lwd=3, col=&#39;darkred&#39;)
lines(smoothed, lwd=3, col=&#39;darkblue&#39;)
lines(y_vec_no_prod, lwd=3, type=&#39;b&#39;, pch=16)
box(lwd=2)
legend(&#39;topleft&#39;, legend=c(&#39;LENI Sales&#39;, &#39;LENI Sales Filtered&#39;, &#39;LENI Sales Smoothed&#39;), 
       lwd=3,col=c(&#39;black&#39;, &#39;darkred&#39;, &#39;darkblue&#39;),
        bg=adjustcolor(&#39;white&#39;, 0.5), pch=c(16,NA,NA))</code></pre>
<p><img src="state_space_files/figure-html/unnamed-chunk-7-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>You’ll notice that the largest deviations from filtered to smooth are
towards the beginning of the series when the system has the least amount
of information.</p>
</div>
<div id="lots-and-lots-of-understudies" class="section level2">
<h2>Lots and Lots of Understudies</h2>
<p>Alright, so we went forward, we went backward, what’s left before
show time? <strong>Simulating</strong>. I’m not one for point estimates.
If given the option I’d rather work with a distribution, which works
nicely with our chosen Bayesian workflow. Our distribution of interest
is <span class="math inline">\(P(\alpha|Y_n)\)</span>, with individual
draws denoted as <span class="math inline">\(\tilde{\alpha}\)</span>.
How do we go about sampling this? Luckily, Durbin and Koopman have us
covered once again <span class="citation">(J. Durbin and Koopman
2002)</span>.</p>
<p>The first step in simulating is realizing that we have the blueprints
to play the system forward from some starting state <span
class="math inline">\(\alpha_1\)</span> if we are given <span
class="math inline">\(H_t\)</span>, <span
class="math inline">\(Q_t\)</span>, <span
class="math inline">\(a_1\)</span>, and <span
class="math inline">\(P_1\)</span> (we’ll get to estimating those
later). We’ll use the <span class="math inline">\(*^+\)</span> notation
for describing draws that have been generated from evolving our system
forward with random initializations. For example, <span
class="math inline">\(\alpha^+ = [\alpha_1^+,
\alpha_2^+,\dots,\alpha_n^+]\)</span>, where <span
class="math inline">\(\alpha_1^+\)</span> was drawn from <span
class="math inline">\(\mathcal{N}(a_1, P_1)\)</span> and the rest of the
states simulated from recursively applying equations <span
class="math inline">\(\text{(1)}\)</span> and <span
class="math inline">\(\text{(2)}\)</span>.<a href="#fn4"
class="footnote-ref" id="fnref4"><sup>4</sup></a> Here are the steps
fully laid out for simulating <span
class="math inline">\(*^+\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Draw <span class="math inline">\(\alpha^+_1\)</span> from the
distribution <span class="math inline">\(\mathcal{N}(a_1,
P_1)\)</span></li>
<li>Set <span class="math inline">\(t=1\)</span></li>
<li>Draw <span class="math inline">\(\varepsilon_t\)</span> from <span
class="math inline">\(\mathcal{N}(0, H_t)\)</span> and <span
class="math inline">\(\nu_t\)</span> from <span
class="math inline">\(\mathcal{N}(0, Q_t)\)</span></li>
<li>Simulate the observation <span class="math inline">\(y^+_t\)</span>
using equation <span class="math inline">\(\text{(1)}\)</span></li>
<li>Simulate the next state <span
class="math inline">\(\alpha^+_{t+1}\)</span> using equation <span
class="math inline">\(\text{(2)}\)</span></li>
<li>Set <span class="math inline">\(t = t+1\)</span></li>
<li>Return to step 3</li>
</ol>
<p>Now that we have <span class="math inline">\(y^+\)</span>, we can
calculate <span class="math inline">\(\hat{\alpha}^+\)</span> defined by
<span class="math inline">\(E[\alpha|y^+]\)</span>. The equations used
for finding <span class="math inline">\(\hat{\alpha}^+\)</span> are the
same ones we used for filtering and smoothing above. Finally, we may use
the culmination of the <span class="citation">(J. Durbin and Koopman
2002)</span> paper to yield <span
class="math inline">\(\tilde{\alpha}\)</span>, given by:</p>
<p><span class="math display">\[\tilde{\alpha} = \alpha^+ -
\hat{\alpha}^+ + \hat{\alpha}\]</span> Using the methodology outline
above, we can now plot the simulated states:</p>
<pre class="r"><code>y_sim &lt;- list()
alpha_sim &lt;- list()
z_list &lt;- state_space_utils$gen_Z_list(length(y_vec_no_prod), include_slope=F,
                                       x_mat=cbind(x1,x2)[!is_new_prod_live,], n_seasons=7)

for (i in 1:100){
  sim &lt;- state_space_utils$simulate_state(
    k_obj$back$alpha_smth, a_1=rep(0,9),
    P_1 = state_space_utils$iden(9)*0.5,
    var_vec = c(mu_var, ss_var), noise_var = c(noise_var),
    params = list(&#39;include_slope&#39;=F, x_mat=cbind(x1,x2)[!is_new_prod_live,], n_seasons=7)
  )
  alpha_sim[[i]] &lt;- sim
  y_sim[[i]] &lt;- lapply(1:length(y_vec_no_prod), \(x) z_list[[x]] %*% sim[[x]]) |&gt; unlist()
}

plot(y_vec_no_prod, type=&#39;n&#39;, lwd=3, cex.axis=1.2, cex=1.2, cex.main=1.5,
     ylab=&#39;&#39;, xlab=&#39;&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, ylim=range(c(y_vec_no_prod, filtered, smoothed)))
axis(side=1, at=mean(seq_along(y_vec_no_prod)), labels=&#39;Time&#39;, cex.axis=1.3, lwd.ticks = 0)
axis(side=2, at=mean(y_vec_no_prod), labels=&#39;Sales ($)&#39;, cex.axis=1.3, lwd.ticks = 0)
box(lwd=2)
y_sim |&gt; unlist() |&gt; matrix(ncol=length(y_vec_no_prod), byrow=T) |&gt; 
  state_space_utils$time_series_err()
lines(y_vec_no_prod, type=&#39;b&#39;, lwd=3, pch=16)
legend(&#39;topleft&#39;, legend=c(&#39;LENI Sales&#39;, &#39;Simulated State&#39;),
       col=c(&#39;black&#39;, NA),
       pch=c(16,22),
       pt.cex=c(1,2),
       lwd=c(3,NA),
       lty=c(1,NA),
       pt.bg=c(NA, adjustcolor(&#39;darkblue&#39;,0.2)),

               bg=adjustcolor(&#39;white&#39;, 0.5))</code></pre>
<p><img src="state_space_files/figure-html/unnamed-chunk-8-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="end-of-act-1" class="section level1">
<h1>End of Act 1</h1>
<p>The curtains have closed and you may now take a brief intermission.
There is still a lot of be done though. We have only talked about the
nuts and bolts of the state space model and haven’t even begun our
exciting applications outlined at the beginning. We may have a framework
that describes the data generating process, but there has been no work
on any parameter inference. Spoiler alert, this is where STAN comes into
play. In the next section we will:</p>
<ul>
<li>Use <a href="https://mc-stan.org/">STAN</a> as our inference engine
for the model outlined</li>
<li>Interrogate the posterior distribution of the model’s parameters to
validate the initial assumptions</li>
<li>Predict the next states and observations credible intervals to back
into a incremental estimate for our marketing campaign (the primary
objective)</li>
</ul>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.1.1 (2021-08-10)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4 cmdstanr_0.5.3  
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.1     xfun_0.27            bslib_0.3.1         
##  [4] purrr_0.3.4          V8_4.4.2             colorspace_2.0-2    
##  [7] vctrs_0.4.1          generics_0.1.1       htmltools_0.5.2     
## [10] viridisLite_0.4.0    yaml_2.2.1           utf8_1.2.2          
## [13] rlang_1.0.3          jquerylib_0.1.4      pillar_1.6.4        
## [16] glue_1.6.2           DBI_1.1.3            colormap_0.1.4      
## [19] distributional_0.3.1 lifecycle_1.0.1      stringr_1.4.0       
## [22] posterior_1.3.1      munsell_0.5.0        gtable_0.3.0        
## [25] rvest_1.0.2          evaluate_0.14        knitr_1.36          
## [28] fastmap_1.1.0        curl_4.3.2           parallel_4.1.1      
## [31] fansi_0.5.0          highr_0.9            Rcpp_1.0.7          
## [34] scales_1.1.1         backports_1.4.1      checkmate_2.0.0     
## [37] webshot_0.5.4        jsonlite_1.7.2       abind_1.4-5         
## [40] farver_2.1.0         systemfonts_1.0.4    tensorA_0.36.2      
## [43] ggplot2_3.3.5        digest_0.6.28        stringi_1.7.5       
## [46] dplyr_1.0.9          grid_4.1.1           cli_3.3.0           
## [49] tools_4.1.1          magrittr_2.0.1       sass_0.4.0          
## [52] tibble_3.1.6         crayon_1.4.2         pkgconfig_2.0.3     
## [55] MASS_7.3-54          ellipsis_0.3.2       xml2_1.3.3          
## [58] assertthat_0.2.1     rmarkdown_2.11       svglite_2.1.0       
## [61] httr_1.4.3           rstudioapi_0.13      R6_2.5.1            
## [64] compiler_4.1.1</code></pre>
<div id="references" class="section level2">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-book" class="csl-entry">
Durbin, James, and Siem Jan Koopman. 2012. <em><span class="nocase">Time
Series Analysis by State Space Methods</span></em>. Oxford University
Press. <a
href="https://doi.org/10.1093/acprof:oso/9780199641178.001.0001">https://doi.org/10.1093/acprof:oso/9780199641178.001.0001</a>.
</div>
<div id="ref-sim_paper" class="csl-entry">
Durbin, J., and S. J. Koopman. 2002. <span>“A Simple and Efficient
Simulation Smoother for State Space Time Series Analysis.”</span>
<em>Biometrika</em> 89 (3): 603–15. <a
href="http://www.jstor.org/stable/4140605">http://www.jstor.org/stable/4140605</a>.
</div>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p><span class="math inline">\(\alpha_t\)</span>: defined
as the state as time t <br /><span class="math inline">\(a_t =
\mathbb{E}[\alpha_t|Y_{t-1}]\)</span> <br /> <span
class="math inline">\(a_{t|t} = \mathbb{E}[\alpha_t|Y_t]\)</span> <br />
<span
class="math inline">\(\hat{\alpha}_t=\mathbb{E}[\alpha_t|Y_n]\)</span>.
This one irks me a bit. I’d prefer it be <span
class="math inline">\(a_{t|n}\)</span> or <span
class="math inline">\(\hat{a}_t\)</span>, but it makes sense if they
wanted to be consistent with the sampling notation. <br /> <span
class="math inline">\(\tilde{\alpha}_t\)</span>: a draw from the
distribution <span class="math inline">\(P(\alpha|Y_n)\)</span>, where
<span class="math inline">\(\alpha = \{\alpha_1^\prime,
\dots,\alpha_n^\prime\}^\prime\)</span>. Here <span
class="math inline">\(\prime\)</span> is used to denote a draw <br />
<span class="math inline">\(\alpha^+\)</span>: a draw from the
distribution <span class="math inline">\(P(\alpha)\)</span>, the
unconditional distribution <br /> <span
class="math inline">\(\hat{\alpha}^+ = \mathbb{E}[\alpha|y^+]\)</span>,
where <span class="math inline">\(y^+\)</span> are the observations from
evolving the system forward given a random initialization drawn from the
known disturbance variances and <span
class="math inline">\(\alpha_1\)</span> distribution.<a href="#fnref1"
class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>You can actually drop the normality assumptions of the
disturbances if you’re willing to work with the <em>minimum variance
linear unbiased estimate</em> (MVLUE) instead<a href="#fnref2"
class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Or incredibly assuming<a href="#fnref3"
class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Durbin &amp; Koopman actually using <span
class="math inline">\(\prime\)</span> when describing individual draws
within the <span class="math inline">\(*^+\)</span> vector, I have
chosen to use <span class="math inline">\(*^+_t\)</span> to keep the
notation less overwhelming<a href="#fnref4"
class="footnote-back">↩︎</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
