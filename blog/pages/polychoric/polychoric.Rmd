---
title: "Polychoric Correlation with Likert data in R and Stan"
output: html_document
---

## Motivation: It's likely likert
Look at you, you successful businessperson you! You own a company that sells two products: A and B. You run a short two question survey to determine whether your customers would recommend either products. For this example we'll assume that all customers are using both products. The question style is the commonly used "Net Promoter Score" or likert scale format: 

<div style='text-align:center'>
"**Provide your level of agreement to the following question:** I would recommend this product to a friend"\n\n
</div>

Where the available choices are:

1. Strongly Disagree
2. Somewhat Disagree
3. Neither Agree or Disagree
4. Somewhat Agree
5. Strongly Agree

There are of course limitations to this kind of survey design. For one, most people have a hard time discretizing their feelings or emotions into a single bucket. Perhaps the more appropriate question response would feature a slider that allows respondents to freely select their agreement on a continuous scale. Regardless, as this is the design chosen by thousands of companies and organizations, we'll choose it as well. Though, we'll recognize that agreement or sentiment in general is better categorized as a spectrum.

Enough philosophy, now to the actual data. I'm going to show how the data is generated further down, but for now let's say that we ran the survey and collected 1,000 responses. First, let's start by loading in all the packages we'll need for this analysis.
```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(cmdstanr)
library(knitr)
library(kableExtra)
```
```{r, echo=F, results='hide'}
sigma <- c(1,0.5,0.5,1) %>% matrix(.,nrow=2,byrow=T)
dat <- mvtnorm::rmvnorm(4e3, sigma=sigma)

c_points <- list(
  c(-2.69, -0.68, 0.07, 1.13),
  c(-1.29, 0.62, 1.38, 1.65)
)

discrete_data <- dat %>% apply(., 1, 
                               \(x) c(
                                 sum(x[1] > c_points[[1]]) + 1,
                                 sum(x[2] > c_points[[2]]) + 1)
                               ) %>% t()

sum(discrete_data[,1] == 3 & discrete_data[,2] == 2)
```

Next, we'll take our `discrete_data` data frame which holds our survey responses and visualize it as a table of all unique responses. For example, the third row and second column will be the number of customers that responded `3` to question 1 and `2` to question 2.

```{r}
table(discrete_data[,1], discrete_data[,2]) %>%
  kbl(row.names=1) %>%
  kable_styling() %>%
  column_spec(column = 1, bold=T)
```

From the table above, we can already see that there is a high degree of positive correlation between the questions. If we wanted to quantifying this correlation, we might naively use the `cor`, but this produces biased results as our provided data is not continuous, which is assumed by the default Pearson correlation measure. There are other measures of correlation such as Spearman or Kendall which are non-parametric, but neither take into account the data generating process that aligns with our philosophy.


## Data Generating Process
We assumed that our data was generated by customers that were forced to discretize their agreement with a given question. If we wanted to properly model this, we might want to assume that our data is first generated from some latent multivariate continuous distribution, and then discretized using a set of `p - 1` "cut-points" for each dimension of the latent space, where `p` is the number of possible choices in the questionnaire. The code to generate data from this process is shown below:


```{r}
sigma <- c(1,0.5,0.5,1) %>% matrix(.,nrow=2,byrow=T)
dat <- mvtnorm::rmvnorm(4e3, sigma=sigma)

c_points <- list(
  c(-2.69, -0.68, 0.07, 1.13),
  c(-1.29, 0.62, 1.38, 1.65)
)

discrete_data <- dat %>% apply(., 1, 
                               \(x) c(
                                 sum(x[1] > c_points[[1]]) + 1,
                                 sum(x[2] > c_points[[2]]) + 1)
                               ) %>% t()
```

We use a bivariate gaussian as our latent distribution with a mean of 0 and variance, or equivalently in this case, a correlation matrix of 

$$
\begin{bmatrix}
1 & 0.5\\
0.5 & 1
\end{bmatrix}
$$
If we plot the latent variables with the cut-points we obtain:
```{r, fig.align='center'}
plot(dat,pch='.', xlab = 'Question 1', ylab='Question 2', main='Latent Space Representation')
for (i in 1:length(c_points[[1]])){
  abline(v=c_points[[1]][i], lwd=2, col='darkred')
  abline(h=c_points[[2]][i], lwd=2, col='darkblue')
}
```
So a respondent that answered `3` for the first question and `2` for the second question would have had their latent representation in the below region: 

```{r, fig.align='center', echo=F}
plot(dat,pch='.', xlab = 'Question 1', ylab='Question 2', main='Latent Space Representation')
polygon(x=c(c_points[[1]][2], c_points[[1]][3], c_points[[1]][3], c_points[[1]][2]),
        y=c(c_points[[2]][1], c_points[[2]][1], c_points[[2]][2], c_points[[2]][2]),
        col=adjustcolor('purple', 0.5))
for (i in 1:length(c_points[[1]])){
  abline(v=c_points[[1]][i], lwd=2, col='darkred')
  abline(h=c_points[[2]][i], lwd=2, col='darkblue')
}
```

## Modeling
Our model is almost fully specified with the data generating process outlined above, but we still need to incorporate our priors. For the correlation matrix, we will reparametrize using the Cholesky decomposition and use a LKJ prior. The cut-points are a little trickier, but notice that the marginals of our latent distribution are standard normals. We can use the to our advantage by reparametrizing the cut-points as a vector of probabilities where each entry is the probability allocated to the interval on the standard normal distribution between two adjacent cut-points $c_i$ and $c_j$.


