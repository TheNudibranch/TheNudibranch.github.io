---
title: "Polychoric Correlation with Likert data in R and Stan"
output: html_document
---

## Motivation: It's likely likert
Look at you, you successful businessperson you! You own a company that sells two products: A and B. You run a short two question survey to determine whether your customers would recommend either products. For this example we'll assume that all customers are using both products. The question style is the commonly used "Net Promoter Score" or likert scale format: 

<div style='text-align:center'>
"**Provide your level of agreement to the following question:** I would recommend this product to a friend"\n\n
</div>

Where the available choices are:

1. Strongly Disagree
2. Somewhat Disagree
3. Neither Agree or Disagree
4. Somewhat Agree
5. Strongly Agree

There are of course limitations to this kind of survey design. For one, most people have a hard time discretizing their feelings or emotions into a single bucket. Perhaps the more appropriate question response would feature a slider that allows respondents to freely select their agreement on a continuous scale. Regardless, as this is the design chosen by thousands of companies and organizations, we'll choose it as well. Though, we'll recognize that agreement or sentiment in general is better categorized as a spectrum.

Enough philosophy, now to the actual data. I'm going to show how the data is generated further down, but for now let's say that we ran the survey and collected 1,000 responses. First, let's start by loading in all the packages we'll need for this analysis.
```{r, echo=F}
knitr::opts_chunk$set(cache = T)
```

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(cmdstanr)
library(knitr)
library(kableExtra)
library(ggridges)
```
```{css, echo=FALSE}
pre {
  max-height: 200px;
  overflow-y: auto;
}

pre[class] {
  max-height: 500px;
}

.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
```

```{r, echo=F, results='hide'}
sigma <- c(1,0.5,0.5,1) %>% matrix(.,nrow=2,byrow=T)
dat <- mvtnorm::rmvnorm(500, sigma=sigma)

c_points <- list(
  c(-2.69, -0.68, 0.07, 1.13),
  c(-1.29, 0.62, 1.38, 1.65)
)

discrete_data <- dat %>% apply(., 1, 
                               \(x) c(
                                 sum(x[1] > c_points[[1]]) + 1,
                                 sum(x[2] > c_points[[2]]) + 1)
                               ) %>% t()

sum(discrete_data[,1] == 3 & discrete_data[,2] == 2)
```

Next, we'll take our `discrete_data` data frame which holds our survey responses and visualize it as a table of all unique responses. For example, the third row and second column will be the number of customers that responded `3` to question 1 and `2` to question 2.

```{r}
table(discrete_data[,1], discrete_data[,2]) %>%
  kbl(row.names=1) %>%
  kable_styling() %>%
  column_spec(column = 1, bold=T)
```

From the table above, we can already see that there is a high degree of positive correlation between the questions. If we wanted to quantifying this correlation, we might naively use the `cor` function, but this produces biased results as our provided data is not continuous, which is assumed by the default Pearson correlation measure. There are other measures of correlation such as Spearman or Kendall which are non-parametric, but neither take into account the data generating process that aligns with our philosophy. For that, we will need to employ the polychoric correlation which we will further define below.


## Data Generating Process
We assumed that our data was generated by customers that were forced to discretize their agreement with a given question. If we wanted to properly model this, we might want to assume that our data is first generated from some latent multivariate continuous distribution, and then discretized using a set of `p - 1` "cut-points" for each dimension of the latent space, where `p` is the number of possible choices in the questionnaire. The code to generate data from this process is shown below:


```{r}
sigma <- c(1,0.5,0.5,1) %>% matrix(.,nrow=2,byrow=T)
dat <- mvtnorm::rmvnorm(500, sigma=sigma)

c_points <- list(
  c(-2.69, -0.68, 0.07, 1.13),
  c(-1.29, 0.62, 1.38, 1.65)
)

discrete_data <- dat %>% apply(., 1, 
                               \(x) c(
                                 sum(x[1] > c_points[[1]]) + 1,
                                 sum(x[2] > c_points[[2]]) + 1)
                               ) %>% t()
```

We use a bivariate gaussian as our latent distribution with a mean of 0 and variance, or equivalently in this case, a correlation matrix of 

$$
\begin{bmatrix}
1 & 0.5\\
0.5 & 1
\end{bmatrix}
$$
If we plot the latent variables with the cut-points we obtain:
```{r, fig.align='center'}
plot(dat,pch=16, xlab = 'Question 1', ylab='Question 2', main='Latent Space Representation')
for (i in 1:length(c_points[[1]])){
  abline(v=c_points[[1]][i], lwd=2, col='darkred')
  abline(h=c_points[[2]][i], lwd=2, col='darkblue')
}
```
So a customer that answered `3` for the first question and `2` for the second question would have had their latent representation in the below region: 

```{r, fig.align='center', echo=F}
plot(dat, pch=16, xlab = 'Question 1', ylab='Question 2', main='Latent Space Representation')
polygon(x=c(c_points[[1]][2], c_points[[1]][3], c_points[[1]][3], c_points[[1]][2]),
        y=c(c_points[[2]][1], c_points[[2]][1], c_points[[2]][2], c_points[[2]][2]),
        col=adjustcolor('purple', 0.5))
for (i in 1:length(c_points[[1]])){
  abline(v=c_points[[1]][i], lwd=2, col='darkred')
  abline(h=c_points[[2]][i], lwd=2, col='darkblue')
}
```

## Modeling
### Priors and Likelihood
Our model is almost fully specified with the data generating process outlined above, but we still need to incorporate our priors. For the correlation matrix, we will reparametrize using the Cholesky decomposition and use a LKJ prior. The cut-points are a little trickier, but notice that the marginals of our latent distribution are standard normals. We can use this to our advantage by reparametrizing the cut-points as a vector of probabilities where each entry is the probability allocated to the interval on the standard normal distribution between two adjacent cut-points $c_i$ and $c_j$. Note that for $p-1$ cutpoints, there will be $p$ entries in our probability vector. Thus, we can write:

$$
\begin{align}
\begin{bmatrix}
c_1\\
c_2 \\
\vdots\\
c_{p-1}
\end{bmatrix}
& \rightarrow \begin{bmatrix}
\theta_1\\
\theta_2\\
\vdots\\
\theta_{p}
\end{bmatrix} \\ \\
&= \begin{bmatrix}
\Phi(c_1) \\
\Phi(c_2) - \Phi(c_1) ] \\
\vdots \\
1 - \Phi(c_{p-1})
\end{bmatrix}
\end{align}
$$
Since our probability interval vector must sum to one, we can use a dirichlet distribution as the prior. The stan code to specify this prior and perform the reparametrization is below:

```{r, eval=F}
real induced_dirichlet_lpdf(vector c, vector alpha, real gamma){
    int K = num_elements(c) + 1;
    vector[K - 1] cuml = Phi(c - gamma);
    vector[K] p;
    matrix[K,K] J = rep_matrix(0,K,K);

    p[1] = cuml[1];
    for (k in 2:(K-1)){
        p[k] = cuml[k] - cuml[k-1];
    }
    p[K] = 1 - cuml[K-1];

    for (k in 1:K) J[k,1] = 1;

    for (k in 2:K){
        real rho = exp(std_normal_lpdf(c[k-1] - gamma));
        J[k,k] = -rho;
        J[k - 1, k] = rho;
    }
    return dirichlet_lpdf(p | alpha) + log_determinant(J);
}
```
The stan code is a bit more involved, and includes the Jacobian calculations since we are specifying a prior on the transformed parameters. For more detail about the reparametrization and Jacobian calculations I suggest reading [Michael Betancourt's ordinal regression tutorial](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html). Be aware that his model uses a latent logistic distribution (different than our standard normal).

Finally, we need to consider the likelihood of the data, conditioned on our parameters. To model this, we will extend the multivariate probit model to the case where an arbitrary number of ordinal categories are observed. Without going into too much detail, the multivariate probit is used to model a bernoulli distributed random vector, where the data is assumed to have been generated from a latent multivariate normal distribution. For example, if you consider our data generating process above but instead only have one cut-point per dimension, then the data generated would be a bernoulli random vector. The stan code used to define this extenstion of the multivariate probit likelihood is [here](https://github.com/TheNudibranch/TheNudibranch.github.io/blob/main/blog/pages/polychoric/model.stan), along with the full stan code for the model.

The full derivation for the likelihood, and therefore stan code, is beyond the scope of this blog post, but I refer you to these two other resources to learn more if you are interested:

- [Ben Goodrich's Truncated Normal Sampler in STAN](https://groups.google.com/g/stan-users/c/GuWUJogum1o/m/LvxjlUBnBwAJ)
- [GHK Algorithm](https://en.wikipedia.org/wiki/GHK_algorithm), which is what Ben's implementation is based on

The parameter, model, and generated quantities block is shown below:
```{r,  eval=F}
parameters {
  cholesky_factor_corr[D] L_Omega;
  array[N,D] real<lower=0, upper=1> u;
  array[D] ordered[n_cut] c_points;
}
model {
    L_Omega ~ lkj_corr_cholesky(4);
    for (d in 1:D) target += induced_dirichlet_lpdf(c_points[d] | rep_vector(1, n_cut + 1), 0);

    for (n in 1:N) target += trunc_norm(y[n], c_points, L_Omega, u[n], D, y_min, y_max);
}
generated quantities {
   corr_matrix[D] Omega;
   Omega = multiply_lower_tri_self_transpose(L_Omega);
}
```
The parameters of interest here are `L_Omega`, which will give us the correlation matrix for our latent gaussian and the `c_points` array which determines the cut-points that generated our data. Ignore the `u` parameter, as it is a nuisance parameter and is only used to help define the likelihood of our data.


### Sampling and Posterior Exploration
Now that are model is fully defined, we can used the `cmdstanr` package to sample our posterior. The full stan code can found [here](https://github.com/TheNudibranch/TheNudibranch.github.io/blob/main/blog/pages/polychoric/model.stan). Note, during model fitting we are expecting some rejected samples due to the highly constrained values of the correlation matrix. When fitting this model with a higher dimension latent 

```{r, echo=F}
fp <- file.path('C:\\Users\\rayia\\Documents\\Website\\Website_2.0\\blog\\pages\\polychoric\\model.stan')
```



```{r, eval=F}
# fp <- file.path('PATH TO YOUR STAN MODEL CODE')
mod <- cmdstan_model(fp)
data <- list(
  D = ncol(discrete_data),
  N = nrow(discrete_data),
  y = discrete_data,
  y_min = min(discrete_data),
  y_max = max(discrete_data)
)

poly_cor <- mod$sample(data = data, seed = 1234, chains = 4, parallel_chains = 2,
                       iter_warmup = 2000,iter_sampling = 2000)
```
```{r, results='hold', collapse=TRUE, echo=FALSE}
# fp <- file.path('PATH TO YOUR STAN MODEL CODE')
mod <- cmdstan_model(fp)
data <- list(
  D = ncol(discrete_data),
  N = nrow(discrete_data),
  y = discrete_data,
  y_min = min(discrete_data),
  y_max = max(discrete_data)
)

poly_cor <- mod$sample(data = data, seed = 1234, chains = 4, parallel_chains = 2,
                       iter_warmup = 2000,iter_sampling = 2000)
```

Let's take a look out some diagnotics to make sure we had adequate posterior sampling:
```{r}
poly_cor$summary(c(paste0('c_points[1,', 1:4,']'), paste0('c_points[2,', 1:4,']'), 'Omega[2,1]')) %>%
  select(-c(median, q5, q95, mad)) %>%
  map(., \(x) if(is.character(x)) x else round(x,2)) %>% as.data.frame() %>%
  kbl() %>% kable_styling()

```
Our `Rhat`, `ess_bulk`, and `ess_tail` look good! Let's take a look at our posteriors for the `c_points` parameter. The ggplot code that adds line segments to the ridge plot is adapted from this [stackoverflow post](https://stackoverflow.com/questions/52527229/draw-line-on-geom-density-ridges). 


```{r, results='hide',message=FALSE, warning=FALSE, fig.align='center', fig.width=15, fig.height=10}
# Function to extract Cut Points from stan model
get_cut_point <- function(dim, n){
  c_name <- paste0('c_points[', dim, ',', n, ']')
  poly_cor$draws(c_name) %>% 
    .[,,1,drop=T] %>%
    as.vector() %>% as.data.frame() %>%
    cbind(., c_name, dim)
}

cut_point_draws <- lapply(4:1, \(x) get_cut_point(1,x)) %>% 
  append(., lapply(4:1, \(x) get_cut_point(2,x))) %>%
  do.call('rbind', .) %>% set_names(c('value', 'name', 'dim')) %>% 
  mutate(dim=as.factor(dim))

# Plot the Cut Point Ridge plot
comp <- ggplot(cut_point_draws, aes(x=value, y=name)) + geom_density_ridges()
ingredients <- ggplot_build(comp) %>% purrr::pluck("data", 1)

density_lines <- ingredients %>% group_by(group) %>% 
  mutate(a = c_points[[floor(max(group)/5) + 1]][[((max(group) - 1) %% 4) + 1]]) %>% 
  slice(which.min(abs(x-a)))

ggplot(cut_point_draws, aes(x=value, y=name)) + 
  geom_density_ridges(aes(fill=dim), rel_min_height = 0.01, lwd=1.2, alpha=0.3) +
  scale_x_continuous(breaks = seq(-3.5,2.5,0.5),limits = c(-3.5,2), expand=c(0,0)) +
  geom_segment(data = density_lines, col='darkred',lwd=1.2,
               aes(x = x, y = ymin, xend = x, 
                   yend = ymin+density*scale*iscale, linetype='Actual Cut-Point Value')) +
  scale_linetype_manual("",values=c('Actual Cut-Point Value'=1)) +
  scale_fill_manual("Cut-point Dimension", 
                    values=c('darkblue', 'darkred', 'yellow'), 
                    labels=c('Dim. 1', 'Dim. 2')) +
  labs(title='Cut Point Parameter Estimates') +
  theme_ridges() + 
  theme(axis.title.y = element_blank(), axis.title.x = element_blank(), 
        axis.text = element_text(size=20), plot.title = element_text(size=30),
        legend.text = element_text(size=20), legend.title = element_text(size=30))
```

It looks like the actual value of each cut-point is captured within the range of each estimated parameter. Now we can look at our originally requested quantity; the correlation.

```{r, results='hide',message=FALSE, warning=FALSE, fig.align='center'}
poly_cor$draws('Omega[2,1]') %>% .[,,1,drop=T] %>% as.vector() %>% 
  {as.data.frame(list(x=.))} %>%
  ggplot(aes(x=x)) + geom_histogram(aes(y=..density..), fill='darkblue', col='black', alpha=0.3) + 
  geom_density(lwd=1.3) +     
  geom_vline(aes(xintercept=0.5, color='Actual Value'), size=1.5, linetype='twodash') +
  scale_colour_manual("", values="darkred") +
  labs(title='Correlation Parameter Estimate') +
  theme_minimal() +
  theme(axis.title.y = element_blank(), axis.title.x = element_blank(), legend.text = element_text(size=13),
        axis.text = element_text(size=13), plot.title = element_text(size=17))
```

## Conclusion
It looks like our model fits the data well and is able to adequately identify the parameters. While the polychoric correlation is a little more involved than a simple pearson correlation, it aligns more with our original data generating process philosophy. Since this model was fit using a bayesian framework we have samples from our posterior which we can use to perform decision analysis, generating pseudocustomers, or probabilistic PCA from the posterior of the correlation matrix. 

```{r}
sessionInfo()
```
