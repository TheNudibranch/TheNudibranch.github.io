<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Polychoric Correlation with Likert data in R and Stan</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
	<a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a>
  <div class="container">
  	<!-- <a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a> -->
    <div class="navbar-header">
      <!-- NOTE: add "navbar-inverse" class for an alternate navbar background -->
      <!-- <a class="navbar-brand" href="../../../index.html">Ian Costley Homepage</a> -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li><a href="https://github.com/TheNudibranch">GitHub</a></li>
        <li><a href="https://www.linkedin.com/in/ian-s-costley/">LinkedIn</a></li>
        <li><a href="../../blog.html">B<sub>natural</sub>LOG</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Polychoric Correlation with Likert data in R and Stan</h1>

</div>


<div id="motivation-its-likely-likert" class="section level2">
<h2>Motivation: It’s likely likert</h2>
<p>Look at you, you successful businessperson you! You own a company that sells two products: A and B. You run a short two question survey to determine whether your customers would recommend either products. For this example we’ll assume that all customers are using both products. The question style is the commonly used “Net Promoter Score” or likert scale format:</p>
<div style="text-align:center">
<p>“<strong>Provide your level of agreement to the following question:</strong> I would recommend this product to a friend”</p>
</div>
<p>Where the available choices are:</p>
<ol style="list-style-type: decimal">
<li>Strongly Disagree</li>
<li>Somewhat Disagree</li>
<li>Neither Agree or Disagree</li>
<li>Somewhat Agree</li>
<li>Strongly Agree</li>
</ol>
<p>There are of course limitations to this kind of survey design. For one, most people have a hard time discretizing their feelings or emotions into a single bucket. Perhaps the more appropriate question response would feature a slider that allows respondents to freely select their agreement on a continuous scale. Regardless, as this is the design chosen by thousands of companies and organizations, we’ll choose it as well. Though, we’ll recognize that agreement or sentiment in general is better categorized as a spectrum.</p>
<p>Enough philosophy, now to the actual data. I’m going to show how the data is generated further down, but for now let’s say that we ran the survey and collected 1,000 responses. First, let’s start by loading in all the packages we’ll need for this analysis.</p>
<pre class="r"><code>library(tidyverse)
library(cmdstanr)
library(knitr)
library(kableExtra)
library(ggridges)</code></pre>
<style type="text/css">
pre {
  max-height: 200px;
  overflow-y: auto;
}

pre[class] {
  max-height: 500px;
}

.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
</style>
<p>Next, we’ll take our <code>discrete_data</code> data frame which holds our survey responses and visualize it as a table of all unique responses. For example, the third row and second column will be the number of customers that responded <code>3</code> to question 1 and <code>2</code> to question 2.</p>
<pre class="r"><code>table(discrete_data[,1], discrete_data[,2]) %&gt;%
  kbl(row.names=1) %&gt;%
  kable_styling() %&gt;%
  column_spec(column = 1, bold=T)</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
2
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
75
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
3
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
97
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
4
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
117
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
14
</td>
</tr>
</tbody>
</table>
<p>From the table above, we can already see that there is a high degree of positive correlation between the questions. If we wanted to quantifying this correlation, we might naively use the <code>cor</code> function, but this produces biased results as our provided data is not continuous, which is assumed by the default Pearson correlation measure. There are other measures of correlation such as Spearman or Kendall which are non-parametric, but neither take into account the data generating process that aligns with our philosophy. For that, we will need to employ the polychoric correlation which we will further define below.</p>
</div>
<div id="data-generating-process" class="section level2">
<h2>Data Generating Process</h2>
<p>We assumed that our data was generated by customers that were forced to discretize their agreement with a given question. If we wanted to properly model this, we might want to assume that our data is first generated from some latent multivariate continuous distribution, and then discretized using a set of <code>p - 1</code> “cut-points” for each dimension of the latent space, where <code>p</code> is the number of possible choices in the questionnaire. The code to generate data from this process is shown below:</p>
<pre class="r"><code>sigma &lt;- c(1,0.5,0.5,1) %&gt;% matrix(.,nrow=2,byrow=T)
dat &lt;- mvtnorm::rmvnorm(500, sigma=sigma)

c_points &lt;- list(
  c(-2.69, -0.68, 0.07, 1.13),
  c(-1.29, 0.62, 1.38, 1.65)
)

discrete_data &lt;- dat %&gt;% apply(., 1, 
                               \(x) c(
                                 sum(x[1] &gt; c_points[[1]]) + 1,
                                 sum(x[2] &gt; c_points[[2]]) + 1)
                               ) %&gt;% t()</code></pre>
<p>We use a bivariate gaussian as our latent distribution with a mean of 0 and variance, or equivalently in this case, a correlation matrix of</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 0.5\\
0.5 &amp; 1
\end{bmatrix}
\]</span> If we plot the latent variables with the cut-points we obtain:</p>
<pre class="r"><code>plot(dat,pch=16, xlab = &#39;Question 1&#39;, ylab=&#39;Question 2&#39;, main=&#39;Latent Space Representation&#39;)
for (i in 1:length(c_points[[1]])){
  abline(v=c_points[[1]][i], lwd=2, col=&#39;darkred&#39;)
  abline(h=c_points[[2]][i], lwd=2, col=&#39;darkblue&#39;)
}</code></pre>
<p><img src="polychoric_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /> So a customer that answered <code>3</code> for the first question and <code>2</code> for the second question would have had their latent representation in the below region:</p>
<p><img src="polychoric_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
<div id="priors-and-likelihood" class="section level3">
<h3>Priors and Likelihood</h3>
<p>Our model is almost fully specified with the data generating process outlined above, but we still need to incorporate our priors. For the correlation matrix, we will reparametrize using the Cholesky decomposition and use a LKJ prior. The cut-points are a little trickier, but notice that the marginals of our latent distribution are standard normals. We can use this to our advantage by reparametrizing the cut-points as a vector of probabilities where each entry is the probability allocated to the interval on the standard normal distribution between two adjacent cut-points <span class="math inline">\(c_i\)</span> and <span class="math inline">\(c_j\)</span>. Note that for <span class="math inline">\(p-1\)</span> cutpoints, there will be <span class="math inline">\(p\)</span> entries in our probability vector. Thus, we can write:</p>
<p><span class="math display">\[
\begin{align}
\begin{bmatrix}
c_1\\
c_2 \\
\vdots\\
c_{p-1}
\end{bmatrix}
&amp; \rightarrow \begin{bmatrix}
\theta_1\\
\theta_2\\
\vdots\\
\theta_{p}
\end{bmatrix} \\ \\
&amp;= \begin{bmatrix}
\Phi(c_1) \\
\Phi(c_2) - \Phi(c_1) ] \\
\vdots \\
1 - \Phi(c_{p-1})
\end{bmatrix}
\end{align}
\]</span> Since our probability interval vector must sum to one, we can use a dirichlet distribution as the prior. The stan code to specify this prior and perform the reparametrization is below:</p>
<pre class="r"><code>real induced_dirichlet_lpdf(vector c, vector alpha, real gamma){
    int K = num_elements(c) + 1;
    vector[K - 1] cuml = Phi(c - gamma);
    vector[K] p;
    matrix[K,K] J = rep_matrix(0,K,K);

    p[1] = cuml[1];
    for (k in 2:(K-1)){
        p[k] = cuml[k] - cuml[k-1];
    }
    p[K] = 1 - cuml[K-1];

    for (k in 1:K) J[k,1] = 1;

    for (k in 2:K){
        real rho = exp(std_normal_lpdf(c[k-1] - gamma));
        J[k,k] = -rho;
        J[k - 1, k] = rho;
    }
    return dirichlet_lpdf(p | alpha) + log_determinant(J);
}</code></pre>
<p>The stan code is a bit more involved, and includes the Jacobian calculations since we are specifying a prior on the transformed parameters. For more detail about the reparametrization and Jacobian calculations I suggest reading <a href="https://betanalpha.github.io/assets/case_studies/ordinal_regression.html">Michael Betancourt’s ordinal regression tutorial</a>. Be aware that his model uses a latent logistic distribution (different than our standard normal).</p>
<p>Finally, we need to consider the likelihood of the data, conditioned on our parameters. To model this, we will extend the multivariate probit model to the case where an arbitrary number of ordinal categories are observed. Without going into too much detail, the multivariate probit is used to model a bernoulli distributed random vector, where the data is assumed to have been generated from a latent multivariate normal distribution. For example, if you consider our data generating process above but instead only have one cut-point per dimension, then the data generated would be a bernoulli random vector. The stan code used to define this extenstion of the multivariate probit likelihood is <a href="https://github.com/TheNudibranch/TheNudibranch.github.io/blob/main/blog/pages/polychoric/model.stan">here</a>, along with the full stan code for the model.</p>
<p>The full derivation for the likelihood, and therefore stan code, is beyond the scope of this blog post, but I refer you to these two other resources to learn more if you are interested:</p>
<ul>
<li><a href="https://groups.google.com/g/stan-users/c/GuWUJogum1o/m/LvxjlUBnBwAJ">Ben Goodrich’s Truncated Normal Sampler in STAN</a></li>
<li><a href="https://en.wikipedia.org/wiki/GHK_algorithm">GHK Algorithm</a>, which is what Ben’s implementation is based on</li>
</ul>
<p>The parameter, model, and generated quantities block is shown below:</p>
<pre class="r"><code>parameters {
  cholesky_factor_corr[D] L_Omega;
  array[N,D] real&lt;lower=0, upper=1&gt; u;
  array[D] ordered[n_cut] c_points;
}
model {
    L_Omega ~ lkj_corr_cholesky(4);
    for (d in 1:D) target += induced_dirichlet_lpdf(c_points[d] | rep_vector(1, n_cut + 1), 0);

    for (n in 1:N) target += trunc_norm(y[n], c_points, L_Omega, u[n], D, y_min, y_max);
}
generated quantities {
   corr_matrix[D] Omega;
   Omega = multiply_lower_tri_self_transpose(L_Omega);
}</code></pre>
<p>The parameters of interest here are <code>L_Omega</code>, which will give us the correlation matrix for our latent gaussian and the <code>c_points</code> array which determines the cut-points that generated our data. Ignore the <code>u</code> parameter, as it is a nuisance parameter and is only used to help define the likelihood of our data.</p>
</div>
<div id="sampling-and-posterior-exploration" class="section level3">
<h3>Sampling and Posterior Exploration</h3>
<p>Now that are model is fully defined, we can used the <code>cmdstanr</code> package to sample our posterior. The full stan code can found <a href="https://github.com/TheNudibranch/TheNudibranch.github.io/blob/main/blog/pages/polychoric/model.stan">here</a>. Note, during model fitting we are expecting some rejected samples due to the highly constrained values of the correlation matrix. When fitting this model with a higher dimension latent</p>
<pre class="r"><code># fp &lt;- file.path(&#39;PATH TO YOUR STAN MODEL CODE&#39;)
mod &lt;- cmdstan_model(fp)
data &lt;- list(
  D = ncol(discrete_data),
  N = nrow(discrete_data),
  y = discrete_data,
  y_min = min(discrete_data),
  y_max = max(discrete_data)
)

poly_cor &lt;- mod$sample(data = data, seed = 1234, chains = 4, parallel_chains = 2,
                       iter_warmup = 2000,iter_sampling = 2000)</code></pre>
<pre><code>## Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
## Chain 1 Exception: Exception: Phi: x is nan, but must be not nan! (in &#39;C:/Users/rayia/AppData/Local/Temp/RtmpEBrkY5/model-42dc7956511.stan&#39;, line 30, column 16 to column 86) (in &#39;C:/Users/rayia/AppData/Local/Temp/RtmpEBrkY5/model-42dc7956511.stan&#39;, line 84, column 19 to column 88)
## Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
## Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
## Chain 1
## Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
## Chain 1 Exception: Exception: Phi: x is nan, but must be not nan! (in &#39;C:/Users/rayia/AppData/Local/Temp/RtmpEBrkY5/model-42dc7956511.stan&#39;, line 30, column 16 to column 86) (in &#39;C:/Users/rayia/AppData/Local/Temp/RtmpEBrkY5/model-42dc7956511.stan&#39;, line 84, column 19 to column 88)
## Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
## Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
## Chain 1
## Chain 2 Rejecting initial value:
## Chain 2   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 2   Stan can&#39;t start sampling from this initial value.
## Chain 2 Rejecting initial value:
## Chain 2   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 2   Stan can&#39;t start sampling from this initial value.
## Chain 2 Rejecting initial value:
## Chain 2   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 2   Stan can&#39;t start sampling from this initial value.
## Chain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
## Chain 2 Exception: Exception: Phi: x is nan, but must be not nan! (in &#39;C:/Users/rayia/AppData/Local/Temp/RtmpEBrkY5/model-42dc7956511.stan&#39;, line 12, column 16 to column 89) (in &#39;C:/Users/rayia/AppData/Local/Temp/RtmpEBrkY5/model-42dc7956511.stan&#39;, line 84, column 19 to column 88)
## Chain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
## Chain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
## Chain 2
## Chain 3 Rejecting initial value:
## Chain 3   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 3   Stan can&#39;t start sampling from this initial value.
## Chain 3 Rejecting initial value:
## Chain 3   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 3   Stan can&#39;t start sampling from this initial value.
## Chain 3 Rejecting initial value:
## Chain 3   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 3   Stan can&#39;t start sampling from this initial value.
## Chain 3 Rejecting initial value:
## Chain 3   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 3   Stan can&#39;t start sampling from this initial value.
## Chain 4 Rejecting initial value:
## Chain 4   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 4   Stan can&#39;t start sampling from this initial value.
## Chain 4 Rejecting initial value:
## Chain 4   Log probability evaluates to log(0), i.e. negative infinity.
## Chain 4   Stan can&#39;t start sampling from this initial value.
## Chain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
## Chain 4 Exception: Exception: Phi: x is nan, but must be not nan! (in &#39;C:/Users/rayia/AppData/Local/Temp/RtmpEBrkY5/model-42dc7956511.stan&#39;, line 29, column 16 to column 90) (in &#39;C:/Users/rayia/AppData/Local/Temp/RtmpEBrkY5/model-42dc7956511.stan&#39;, line 84, column 19 to column 88)
## Chain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
## Chain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
## Chain 4
## Running MCMC with 4 chains, at most 2 in parallel...
## 
## Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup) 
## Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup) 
## Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) 
## Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) 
## Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) 
## Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) 
## Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) 
## Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) 
## Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) 
## Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) 
## Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) 
## Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) 
## Chain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) 
## Chain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) 
## Chain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) 
## Chain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) 
## Chain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) 
## Chain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) 
## Chain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) 
## Chain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) 
## Chain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) 
## Chain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) 
## Chain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) 
## Chain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) 
## Chain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) 
## Chain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) 
## Chain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) 
## Chain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) 
## Chain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) 
## Chain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) 
## Chain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) 
## Chain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) 
## Chain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) 
## Chain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) 
## Chain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) 
## Chain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) 
## Chain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) 
## Chain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) 
## Chain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) 
## Chain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) 
## Chain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) 
## Chain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) 
## Chain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) 
## Chain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) 
## Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) 
## Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) 
## Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) 
## Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) 
## Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) 
## Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) 
## Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) 
## Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) 
## Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) 
## Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) 
## Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) 
## Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) 
## Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) 
## Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) 
## Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) 
## Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) 
## Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) 
## Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) 
## Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) 
## Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) 
## Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) 
## Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) 
## Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) 
## Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) 
## Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) 
## Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) 
## Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) 
## Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) 
## Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) 
## Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) 
## Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) 
## Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) 
## Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) 
## Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) 
## Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) 
## Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) 
## Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) 
## Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) 
## Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) 
## Chain 2 finished in 202.8 seconds.
## Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) 
## Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup) 
## Chain 1 finished in 202.9 seconds.
## Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) 
## Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) 
## Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) 
## Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) 
## Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) 
## Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) 
## Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) 
## Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) 
## Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) 
## Chain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) 
## Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) 
## Chain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) 
## Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) 
## Chain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) 
## Chain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) 
## Chain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) 
## Chain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) 
## Chain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) 
## Chain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) 
## Chain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) 
## Chain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) 
## Chain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) 
## Chain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) 
## Chain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) 
## Chain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) 
## Chain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) 
## Chain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) 
## Chain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) 
## Chain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) 
## Chain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) 
## Chain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) 
## Chain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) 
## Chain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) 
## Chain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) 
## Chain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) 
## Chain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) 
## Chain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) 
## Chain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) 
## Chain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) 
## Chain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) 
## Chain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) 
## Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) 
## Chain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) 
## Chain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) 
## Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) 
## Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) 
## Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) 
## Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) 
## Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) 
## Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) 
## Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) 
## Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) 
## Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) 
## Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) 
## Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) 
## Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) 
## Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) 
## Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) 
## Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) 
## Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) 
## Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) 
## Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) 
## Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) 
## Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) 
## Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) 
## Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) 
## Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) 
## Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) 
## Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) 
## Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) 
## Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) 
## Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) 
## Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) 
## Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) 
## Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) 
## Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) 
## Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) 
## Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) 
## Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) 
## Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) 
## Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) 
## Chain 3 finished in 199.6 seconds.
## Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) 
## Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) 
## Chain 4 finished in 204.5 seconds.
## 
## All 4 chains finished successfully.
## Mean chain execution time: 202.4 seconds.
## Total execution time: 407.8 seconds.</code></pre>
<p>Let’s take a look out some diagnotics to make sure we had adequate posterior sampling:</p>
<pre class="r"><code>poly_cor$summary(c(paste0(&#39;c_points[1,&#39;, 1:4,&#39;]&#39;), paste0(&#39;c_points[2,&#39;, 1:4,&#39;]&#39;), &#39;Omega[2,1]&#39;)) %&gt;%
  select(-c(median, q5, q95, mad)) %&gt;%
  map(., \(x) if(is.character(x)) x else round(x,2)) %&gt;% as.data.frame() %&gt;%
  kbl() %&gt;% kable_styling()</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
rhat
</th>
<th style="text-align:right;">
ess_bulk
</th>
<th style="text-align:right;">
ess_tail
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
c_points[1,1]
</td>
<td style="text-align:right;">
-2.46
</td>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
9840.80
</td>
<td style="text-align:right;">
5852.00
</td>
</tr>
<tr>
<td style="text-align:left;">
c_points[1,2]
</td>
<td style="text-align:right;">
-0.70
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
10809.28
</td>
<td style="text-align:right;">
7019.21
</td>
</tr>
<tr>
<td style="text-align:left;">
c_points[1,3]
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.05
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
9600.36
</td>
<td style="text-align:right;">
7235.36
</td>
</tr>
<tr>
<td style="text-align:left;">
c_points[1,4]
</td>
<td style="text-align:right;">
1.17
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
11823.80
</td>
<td style="text-align:right;">
6400.34
</td>
</tr>
<tr>
<td style="text-align:left;">
c_points[2,1]
</td>
<td style="text-align:right;">
-1.31
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
10660.58
</td>
<td style="text-align:right;">
6513.94
</td>
</tr>
<tr>
<td style="text-align:left;">
c_points[2,2]
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8662.44
</td>
<td style="text-align:right;">
5863.82
</td>
</tr>
<tr>
<td style="text-align:left;">
c_points[2,3]
</td>
<td style="text-align:right;">
1.28
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
10177.36
</td>
<td style="text-align:right;">
6183.05
</td>
</tr>
<tr>
<td style="text-align:left;">
c_points[2,4]
</td>
<td style="text-align:right;">
1.59
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
11536.31
</td>
<td style="text-align:right;">
6388.87
</td>
</tr>
<tr>
<td style="text-align:left;">
Omega[2,1]
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
11065.00
</td>
<td style="text-align:right;">
6189.01
</td>
</tr>
</tbody>
</table>
<p>Our <code>Rhat</code>, <code>ess_bulk</code>, and <code>ess_tail</code> look good! Let’s take a look at our posteriors for the <code>c_points</code> parameter. The ggplot code that adds line segments to the ridge plot is adapted from this <a href="https://stackoverflow.com/questions/52527229/draw-line-on-geom-density-ridges">stackoverflow post</a>.</p>
<pre class="r"><code># Function to extract Cut Points from stan model
get_cut_point &lt;- function(dim, n){
  c_name &lt;- paste0(&#39;c_points[&#39;, dim, &#39;,&#39;, n, &#39;]&#39;)
  poly_cor$draws(c_name) %&gt;% 
    .[,,1,drop=T] %&gt;%
    as.vector() %&gt;% as.data.frame() %&gt;%
    cbind(., c_name, dim)
}

cut_point_draws &lt;- lapply(4:1, \(x) get_cut_point(1,x)) %&gt;% 
  append(., lapply(4:1, \(x) get_cut_point(2,x))) %&gt;%
  do.call(&#39;rbind&#39;, .) %&gt;% set_names(c(&#39;value&#39;, &#39;name&#39;, &#39;dim&#39;)) %&gt;% 
  mutate(dim=as.factor(dim))

# Plot the Cut Point Ridge plot
comp &lt;- ggplot(cut_point_draws, aes(x=value, y=name)) + geom_density_ridges()
ingredients &lt;- ggplot_build(comp) %&gt;% purrr::pluck(&quot;data&quot;, 1)

density_lines &lt;- ingredients %&gt;% group_by(group) %&gt;% 
  mutate(a = c_points[[floor(max(group)/5) + 1]][[((max(group) - 1) %% 4) + 1]]) %&gt;% 
  slice(which.min(abs(x-a)))

ggplot(cut_point_draws, aes(x=value, y=name)) + 
  geom_density_ridges(aes(fill=dim), rel_min_height = 0.01, lwd=1.2, alpha=0.3) +
  scale_x_continuous(breaks = seq(-3.5,2.5,0.5),limits = c(-3.5,2), expand=c(0,0)) +
  geom_segment(data = density_lines, col=&#39;darkred&#39;,lwd=1.2,
               aes(x = x, y = ymin, xend = x, 
                   yend = ymin+density*scale*iscale, linetype=&#39;Actual Cut-Point Value&#39;)) +
  scale_linetype_manual(&quot;&quot;,values=c(&#39;Actual Cut-Point Value&#39;=1)) +
  scale_fill_manual(&quot;Cut-point Dimension&quot;, 
                    values=c(&#39;darkblue&#39;, &#39;darkred&#39;, &#39;yellow&#39;), 
                    labels=c(&#39;Dim. 1&#39;, &#39;Dim. 2&#39;)) +
  labs(title=&#39;Cut Point Parameter Estimates&#39;) +
  theme_ridges() + 
  theme(axis.title.y = element_blank(), axis.title.x = element_blank(), 
        axis.text = element_text(size=20), plot.title = element_text(size=30),
        legend.text = element_text(size=20), legend.title = element_text(size=30))</code></pre>
<p><img src="polychoric_files/figure-html/unnamed-chunk-15-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>It looks like the actual value of each cut-point is captured within the range of each estimated parameter. Now we can look at our originally requested quantity; the correlation.</p>
<pre class="r"><code>poly_cor$draws(&#39;Omega[2,1]&#39;) %&gt;% .[,,1,drop=T] %&gt;% as.vector() %&gt;% 
  {as.data.frame(list(x=.))} %&gt;%
  ggplot(aes(x=x)) + geom_histogram(aes(y=..density..), fill=&#39;darkblue&#39;, col=&#39;black&#39;, alpha=0.3) + 
  geom_density(lwd=1.3) +     
  geom_vline(aes(xintercept=0.5, color=&#39;Actual Value&#39;), size=1.5, linetype=&#39;twodash&#39;) +
  scale_colour_manual(&quot;&quot;, values=&quot;darkred&quot;) +
  labs(title=&#39;Correlation Parameter Estimate&#39;) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), axis.title.x = element_blank(), legend.text = element_text(size=13),
        axis.text = element_text(size=13), plot.title = element_text(size=17))</code></pre>
<p><img src="polychoric_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>It looks like our model fits the data well and is able to adequately identify the parameters. While the polychoric correlation is a little more involved than a simple pearson correlation, it aligns more with our original data generating process philosophy. Since this model was fit using a bayesian framework we have samples from our posterior which we can use to perform decision analysis, generating pseudocustomers, or probabilistic PCA from the posterior of the correlation matrix.</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.1.1 (2021-08-10)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggridges_0.5.3   kableExtra_1.3.4 knitr_1.36       cmdstanr_0.5.3  
##  [5] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     
##  [9] readr_2.1.2      tidyr_1.2.0      tibble_3.1.6     ggplot2_3.3.5   
## [13] tidyverse_1.3.1 
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.4.3           sass_0.4.0           jsonlite_1.7.2      
##  [4] viridisLite_0.4.0    modelr_0.1.8         bslib_0.3.1         
##  [7] assertthat_0.2.1     posterior_1.3.1      distributional_0.3.1
## [10] highr_0.9            tensorA_0.36.2       cellranger_1.1.0    
## [13] yaml_2.2.1           pillar_1.6.4         backports_1.4.1     
## [16] glue_1.6.2           digest_0.6.28        checkmate_2.0.0     
## [19] rvest_1.0.2          colorspace_2.0-2     htmltools_0.5.2     
## [22] plyr_1.8.6           pkgconfig_2.0.3      broom_1.0.0         
## [25] haven_2.5.0          mvtnorm_1.1-3        scales_1.1.1        
## [28] webshot_0.5.4        processx_3.5.2       svglite_2.1.0       
## [31] tzdb_0.3.0           generics_0.1.1       farver_2.1.0        
## [34] ellipsis_0.3.2       withr_2.5.0          cli_3.3.0           
## [37] magrittr_2.0.1       crayon_1.4.2         readxl_1.4.0        
## [40] evaluate_0.14        ps_1.6.0             fs_1.5.2            
## [43] fansi_0.5.0          xml2_1.3.3           data.table_1.14.2   
## [46] tools_4.1.1          hms_1.1.1            matrixStats_0.61.0  
## [49] lifecycle_1.0.1      munsell_0.5.0        reprex_2.0.1        
## [52] compiler_4.1.1       jquerylib_0.1.4      systemfonts_1.0.4   
## [55] rlang_1.0.3          grid_4.1.1           rstudioapi_0.13     
## [58] labeling_0.4.2       rmarkdown_2.11       gtable_0.3.0        
## [61] codetools_0.2-18     abind_1.4-5          DBI_1.1.3           
## [64] R6_2.5.1             lubridate_1.8.0      fastmap_1.1.0       
## [67] utf8_1.2.2           stringi_1.7.5        Rcpp_1.0.7          
## [70] vctrs_0.4.1          dbplyr_2.2.1         tidyselect_1.1.1    
## [73] xfun_0.27</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
