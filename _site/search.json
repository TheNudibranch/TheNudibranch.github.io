[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 23, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Decision Analysis with Poisson Regression and Metropolis-Hastings\n\n\n\n\n\n\nBayes\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 26, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/poisson_metrop/index.html",
    "href": "posts/poisson_metrop/index.html",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "",
    "text": "Allow me to paint a picture:\nYou’re an avid salmon fisherman and statistician. You’ve fished 100 ponds in the surrounding area and have recorded the hourly rate at which you catch salmon while on the water. During your time on the water you also record a couple of measurable variables for each pond: pond depth and dissolved oxygen.\nNow, your friend Danny wants go fishing with you this week and suggests two ponds you haven’t fished a before. Also, since Danny is a good friend, they have kindly measured both depth and dissolved oxygen of each pond. Very thoughtful! The question now stands: which pond will you catch more salmon at?"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#a-fishy-situation",
    "href": "posts/poisson_metrop/index.html#a-fishy-situation",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "",
    "text": "Allow me to paint a picture:\nYou’re an avid salmon fisherman and statistician. You’ve fished 100 ponds in the surrounding area and have recorded the hourly rate at which you catch salmon while on the water. During your time on the water you also record a couple of measurable variables for each pond: pond depth and dissolved oxygen.\nNow, your friend Danny wants go fishing with you this week and suggests two ponds you haven’t fished a before. Also, since Danny is a good friend, they have kindly measured both depth and dissolved oxygen of each pond. Very thoughtful! The question now stands: which pond will you catch more salmon at?"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#data-generation",
    "href": "posts/poisson_metrop/index.html#data-generation",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "Data generation",
    "text": "Data generation\nLets assume that the number of salmon caught \\(Y\\) is poisson distributed with rate \\(\\lambda\\). \\[Y \\sim Poi(\\lambda)\\]\nLet us also assume that the log rate of catching salmon is a linear function of the two pond variables:\n\\[\\log(\\lambda) =  \\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0\\] Finally, we may write the probability of catching \\(y\\) salmon during one hour of fishing given the above parameters:\n\\[P(y|x_i;\\beta_{depth},\\beta_{oxy},\\beta_0)=\\frac{\\lambda^y}{y!}e^{-\\lambda}, \\hspace{4mm} \\lambda=\\exp(\\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0)\\] Now, let’s generate some data!\n\n\nCode\nlibrary(dplyr)\nlibrary(rstan)\n\n\n\n# Function to standardize data\nstandardize &lt;- function(x){\n  return((x - mean(x)) / sd(x))\n}\nset.seed(1234)\n\nN &lt;- 200 # Number of ponds fished at\nx_oxy &lt;- rnorm(N, 5, 1) # Dissolved oxygen in mg per Liter\nx_depth &lt;- abs(rnorm(N, 30, 10)) # Pond depth\n\nb_oxy &lt;- 0.8\nb_depth &lt;- -0.6\nb_int &lt;- 0.4\nlambda &lt;- exp(b_oxy*standardize(x_oxy) + b_depth*standardize(x_depth) + b_int)\ny &lt;- rpois(length(lambda), lambda = lambda)\n\npar(mfrow=c(1,2))\nplot(x_oxy, y, pch=16, col='darkblue', ylab='Fish Caught Per Hour', xlab='Dissolved Oxygen (mg/L)')\nplot(x_depth, y, pch=16, col='darkred', ylab='Fish Caught Per Hour', xlab='Pond Depth (m)')"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#mcmc-with-metropolis-hastings",
    "href": "posts/poisson_metrop/index.html#mcmc-with-metropolis-hastings",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "MCMC with Metropolis Hastings",
    "text": "MCMC with Metropolis Hastings\n\nTheory\n\nModel Definition\nFirst, we will assume that the parameters \\(\\beta_{depth}\\), \\(\\beta_{oxy}\\), and \\(\\beta_0\\) are independent of each other such that the joint probability distribution for the parameters can be expressed as:\n\\[P(\\beta_{depth}, \\beta_{oxy}, \\beta_0 ) = P(\\beta_{depth} ) \\cdot P(\\beta_{oxy})\\cdot P(\\beta_0)\\]\nNow, observing the data from the plots above, we can assume that \\(\\beta_{depth}\\) will be positive and \\(\\beta_{oxy}\\) to be negative. Thus, we can set out priors to reflect this observation:\n\\[\\begin{align}\nY &\\sim Poi(\\lambda) \\\\\n\\log(\\lambda) &=  \\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0 \\\\\n\\beta_{depth} &\\sim N(0.5, 0.5) \\\\\n\\beta_{oxy} &\\sim N(-0.5,0.5) \\\\\n\\beta_0 &\\sim N(0, 0.5)\n\\end{align}\\]\nLet’s now define our likelihood and priors in R:\n\nlog_poi_liklihood &lt;- function(params){\n  lam_cands &lt;- exp(params[['b_oxy_cand']]*standardize(x_oxy) + \n                     params[['b_depth_cand']]*standardize(x_depth) +\n                     params[['b_int_cand']])\n  return(sum(dpois(y, lam_cands, log=T)))\n}\n\nlog_prior &lt;- function(params){\n  return(\n    dnorm(params[['b_oxy_cand']], 0.5, 0.5, log=T) +\n    dnorm(params[['b_depth_cand']], -0.5, 0.5, log=T) +\n    dnorm(params[['b_int_cand']], 0, 0.5, log=T)\n  )\n}\n\nlog_posterior_prob &lt;- function(params){\n  return(log_prior(params) + log_poi_liklihood(params))\n}\n\nWhile defining our priors it is always good practice to make sure that they make sense by checking the prior predictive distribution. We can do this by taking some samples from our prior distributions and using them in place of our model.\n\nn_prior_samples &lt;- 1e3\nsample_priors &lt;- cbind(rnorm(n_prior_samples, 0.5, 0.5),\n                       rnorm(n_prior_samples, -0.5, 0.5),\n                       rnorm(n_prior_samples, 0,0.5))\n\nprior_predicitive &lt;- cbind(standardize(x_oxy), standardize(x_depth)) %&gt;% apply(., 1, function(x) \n  rpois(n=n_prior_samples, exp(x[1]*sample_priors[,1] + x[2]*sample_priors[,2] + sample_priors[,3]) ))\n\nhist(prior_predicitive %&gt;% log(),prob=T,ylim=c(0,0.8), col=adjustcolor('darkblue', alpha.f = 0.5),\n     main='Log of Prior Predictive and y', xlab='', ylab='')\nhist(y %&gt;% log(), prob=T, add=T, col=adjustcolor('darkred', alpha.f = 0.5))\nlegend('topright', c('Prior Predidictive','y'), fill=c(adjustcolor('darkblue', alpha.f = 0.5),\n                                                       adjustcolor('darkred', alpha.f = 0.5)))\n\n\n\n\n\n\n\n\n\n\nMCMC\nMetropolis Hastings is rejection sampler defined as the following.\nSuppose we have some sampled value \\(x_t\\) and some function \\(P(x)\\) the returns the probability of a given \\(x\\). We also have some proposal function that generates a new \\(x_{t+1}\\) given a previously sampled \\(x_t\\) defined by \\(g(x_{t+1}|x_t)\\).\nNow, we need some way to “reject” or “accept” some newly generated \\(x_{t+1}\\) value from our function \\(g\\). Define this probability of acceptance to be\n\\[a=\\frac{P(x_{t+1})g(x_t|x_{t+1})}{P(x_t)g(x_{t+1}|x_t)}\\] Usually (and for our case today), we’ll choose a function \\(g\\) such that \\(g\\) is symmetric, or \\(g(x_t|x_{t+1})=g(x_{t+1}|x_t)\\). A common choice to achieve this property would be to assume \\(g\\) is normal with mean equal to the given point. In other words \\[g(x_t|x_{t+1})\\sim N(x_{t+1},\\sigma)\\]\nNote, here \\(P(x)\\) will be the probability of our sampled \\(\\{\\beta_{depth}, \\beta_{oxy}, \\beta_0 \\}\\) given our data, as that is the posterior we which to rejection sample from.\nNow, let’s write our MCMC algorithm and sample from our posterior! We run 4 different chains to get the best estimate of our posterior.\n\nN_sim &lt;- 5e4\nN_chains &lt;- 4\n\nmcmc_chain &lt;- function(N_sim, explore_param){\n  curr_params &lt;-  list(\n    b_oxy_cand = rnorm(1, 0, 4),\n    b_depth_cand = rnorm(1, 0, 4),\n    b_int_cand = rnorm(1, 0, 4)\n  )\n  chain &lt;- matrix(NA, nrow=N_sim, ncol=3)\n  for (i in 1:N_sim){\n    cand_params &lt;- list(\n      b_oxy_cand = rnorm(1, curr_params[['b_oxy_cand']], explore_param),\n      b_depth_cand = rnorm(1, curr_params[['b_depth_cand']], explore_param),\n      b_int_cand = rnorm(1, curr_params[['b_int_cand']], explore_param)\n    )\n    a &lt;- min(1, exp(log_posterior_prob(cand_params) - \n                      log_posterior_prob(curr_params)))\n    u &lt;- runif(1)\n    if (u &lt;= a){\n      chain[i,] &lt;- unlist(cand_params)\n      curr_params &lt;- cand_params\n    }\n    else{\n      chain[i,] &lt;- unlist(curr_params)\n    }\n  }\n  return(chain)\n}\n\nsimulation &lt;- list()\nfor (i in 1:N_chains){\n  simulation[[paste0('chain_',i)]] &lt;- mcmc_chain(N_sim, explore_param = 0.01)\n}\n\nburn &lt;- 1e4\n\n\n\n\nAnalyzing our chains\nLet’s see how well our posteriors match the actual values:\n\n\nCode\npar(mfrow=c(1,3))\nposterior_oxy &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),1])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_oxy %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Dissolved Oxygen')\npolygon(posterior_oxy %&gt;% density(), col=adjustcolor('darkgreen', 0.5))\nabline(v=0.8, col='red', lwd=4)\n\n\nposterior_depth &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),2])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_depth %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Depth')\npolygon(posterior_depth %&gt;% density(), col=adjustcolor('darkblue', 0.5))\nabline(v=-0.6, col='red', lwd=4)\n\n\nposterior_int &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),3])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_int %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Beta_0')\npolygon(posterior_int %&gt;% density(), col=adjustcolor('darkred', 0.5))\nabline(v=0.4, col='red', lwd=4)\n\n\n\n\n\n\n\n\n\nFocusing on \\(\\beta_{oxy}\\), let’s see how well our chain converged using rank plots:\n\na &lt;- simulation %&gt;% lapply(., function(x) x[-seq(1,burn),1]) %&gt;% unlist() %&gt;% rank() %&gt;% \n  matrix(., ncol=4)\npar(mfrow=c(2,2))\nfor (i in 1:4) hist(a[,i], col=adjustcolor('darkblue', alpha.f = 0.5), main=paste0('Chain_',i),\n                    xlab='', ylab='')\n\n\n\n\n\n\n\n\nRank plots are calculated by combining all our MCMC samples and finding each samples respective rank. The resulting ranks are separated back into their respective chains and plotted as histograms. If the MCMC sampler converged and the chains mixed well without a high degree of autocorrelation, we can expected uniform distributions for each rank plot.\nThere a couple of metrics we can look at to assess the convergence of our MCMC sampling. One main metric is \\(\\hat{R}\\). It tells us how well all our chains converged and mixed. A good rule of thumb is to have \\(\\hat{R}\\) under 1.05.\nThe other metrics have to do with effective sample size (ESS). In MCMC sampling, we are assuming a level of independence for samples not directly adjacent. In other words, we are hoping for a low degree of autocorrelation. Simply put, if we have a high degree of autocorrelation in our samples then we effectively have less information describing our posterior. This is what ESS measures, the degree of autocorrelation in our chains. The first, bulk-ESS tells us how well the center or bulk of our posterior has been sampled. The second is tail-ESS, which tells us how well our posterior tails were sampled. A good rule of thumb is to have a bulk-ESS and tail-ESS greater than 400.\n\nmetric_mat &lt;- matrix(NA, nrow=3, ncol=3)\nfor (i in 1:3){\n  metric_mat[1,i] &lt;- Rhat(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                            as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,2)\n  metric_mat[2,i] &lt;- ess_bulk(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                                as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,1)\n  metric_mat[3,i] &lt;- ess_tail(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                                as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,1)\n}\ncolnames(metric_mat) &lt;- c('b_oxy', 'b_depth', 'b_0')\nrow.names(metric_mat) &lt;- c('r_hat', 'bulk_ess', 'tail_ess')\nknitr::kable(metric_mat, align = 'ccc')\n\n\n\n\n\nb_oxy\nb_depth\nb_0\n\n\n\n\nr_hat\n1.0\n1.0\n1.0\n\n\nbulk_ess\n1359.1\n1168.9\n834.9\n\n\ntail_ess\n2638.3\n2752.3\n1573.2\n\n\n\n\n\nA good measure to determine how well model fits our data is to plot the posterior predictive against the observed data.\n\nposterior_predictive &lt;- cbind(standardize(x_oxy), standardize(x_depth)) %&gt;% apply(., 1, function(x) \n  rpois(n=n_prior_samples, exp(x[1]*posterior_oxy + x[2]*posterior_depth + posterior_int))) %&gt;% c()\n\nhist(posterior_predictive,prob=T, col=adjustcolor('darkblue', alpha.f = 0.5),\n     breaks=length(unique(posterior_predictive)),\n     main='Posterior Predictive and y', ylab='', xlab='')\n\nhist(y, prob=T, add=T, col=adjustcolor('darkred', alpha.f = 0.5), breaks=34)\nlegend('topright', c('Posterior Predictive','y'), fill=c(adjustcolor('darkblue', alpha.f = 0.5),\n                                                       adjustcolor('darkred', alpha.f = 0.5)))"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#decision-analysis",
    "href": "posts/poisson_metrop/index.html#decision-analysis",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "Decision Analysis",
    "text": "Decision Analysis\nBack to the question at hand: what pond should you fish at? Let’s say your friend Danny has the following measurements for the two ponds in question.\n\nponds &lt;- matrix(c(8, 7, 40, 20), ncol=2) %&gt;% as.data.frame()\ncolnames(ponds) &lt;- c('Dissolved Oxygen', 'Pond Depth')\nrow.names(ponds) &lt;- c('Pond A', 'Pond B')\nknitr::kable(ponds, align='ccc')\n\n\n\n\n\nDissolved Oxygen\nPond Depth\n\n\n\n\nPond A\n8\n40\n\n\nPond B\n7\n20\n\n\n\n\n\nFrom our posterior samples, we can obtain distributions representing our uncertainty for the fish we will catch at each of the ponds in question.\n\npar(mfrow=c(1,2))\npond_a &lt;- ((ponds$`Dissolved Oxygen`[1] - mean(x_oxy)) / sd(x_oxy)) * posterior_oxy + \n  ((ponds$`Pond Depth`[1] - mean(x_depth)) / sd(x_depth)) * posterior_depth + posterior_int\npond_a &lt;- rpois(length(pond_a), exp(pond_a))\nhist(pond_a, breaks=length(unique(pond_a)), prob=T, col=adjustcolor('darkgreen', alpha.f = 0.5),\n     main='Pond A', xlab='Fish Caught', ylab='')\n\npond_b &lt;- ((ponds$`Dissolved Oxygen`[2] - mean(x_oxy)) / sd(x_oxy)) * posterior_oxy + \n  ((ponds$`Pond Depth`[2] - mean(x_depth)) / sd(x_depth)) * posterior_depth + posterior_int\npond_b &lt;- rpois(length(pond_b), exp(pond_b))\nhist(pond_b, breaks=length(unique(pond_b)), prob=T, col=adjustcolor('yellow', alpha.f = 0.5),\n     main='Pond B', xlab='Fish Caught', ylab='')\n\n\n\n\n\n\n\n\nNow, we can take the difference of the two distributions and come to our conclusion.\n\npond_diff &lt;- (pond_b - pond_a)\nhist(pond_diff, prob=T, main='Pond B - Pond A', xlab='Difference in fish caught between ponds', ylab='',\n      col=adjustcolor('purple', alpha.f = 0.5))\n\n\n\n\n\n\n\n\nIf we want to find the expected increase in fish in choosing pond B of pond A, it’s as simple as taking the average of our above distribution.\n\nmean(pond_b - pond_a)\n\n[1] 3.11315\n\n\nHence, after our extensive analysis we can come to the conclusion that it is best to choose Pond B over A. Although, maybe your friend Danny has left by now!\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] rstan_2.32.6        StanHeaders_2.32.10 dplyr_1.1.4        \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_1.8.9     compiler_4.4.2     tidyselect_1.2.1  \n [5] Rcpp_1.0.13-1      parallel_4.4.2     gridExtra_2.3      scales_1.3.0      \n [9] yaml_2.3.10        fastmap_1.2.0      ggplot2_3.5.1      R6_2.5.1          \n[13] generics_0.1.3     knitr_1.49         htmlwidgets_1.6.4  tibble_3.2.1      \n[17] munsell_0.5.1      pillar_1.9.0       rlang_1.1.4        utf8_1.2.4        \n[21] inline_0.3.20      xfun_0.49          RcppParallel_5.1.9 cli_3.6.3         \n[25] magrittr_2.0.3     digest_0.6.37      grid_4.4.2         rstudioapi_0.17.1 \n[29] lifecycle_1.0.4    vctrs_0.6.5        evaluate_1.0.1     glue_1.8.0        \n[33] QuickJSR_1.4.0     codetools_0.2-20   stats4_4.4.2       pkgbuild_1.4.5    \n[37] fansi_1.0.6        colorspace_2.1-1   rmarkdown_2.29     matrixStats_1.4.1 \n[41] tools_4.4.2        loo_2.8.0          pkgconfig_2.0.3    htmltools_0.5.8.1"
  }
]