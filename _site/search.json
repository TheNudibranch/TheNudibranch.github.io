[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ian Costley",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\nNov 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O’Malley\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPolychoric Correltion with Likert Data\n\n\n\nBayes\n\n\nR\n\n\nStan\n\n\n\nPolychoric correlation using a bayesian framework in Stan.\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Decision Analysis with Poisson Regression and Metropolis-Hastings\n\n\n\nBayes\n\n\nR\n\n\n\nBayesian poisson regression with a Metropolis Hastings sampler\n\n\n\n\n\n\nMar 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Beta-Binomial Model\n\n\n\nBayes\n\n\nJavascript\n\n\n\nProvide some priors/data and what you’re conjugate posteriors update in real-time!\n\n\n\n\n\n\nOct 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRock Paper Scissors Robots\n\n\n\nArduino\n\n\n\nRock paper scissors with a 3D printed wireless hand!\n\n\n\n\n\n\nMay 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow JS\n\n\n\nJavascript\n\n\n\nJust a simple webapp showing TF JS functionality for building simple models in the browser\n\n\n\n\n\n\nJul 27, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/poisson_metrop/index.html",
    "href": "posts/poisson_metrop/index.html",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "",
    "text": "Allow me to paint a picture:\nYou’re an avid salmon fisherman and statistician. You’ve fished 100 ponds in the surrounding area and have recorded the hourly rate at which you catch salmon while on the water. During your time on the water you also record a couple of measurable variables for each pond: pond depth and dissolved oxygen.\nNow, your friend Danny wants go fishing with you this week and suggests two ponds you haven’t fished a before. Also, since Danny is a good friend, they have kindly measured both depth and dissolved oxygen of each pond. Very thoughtful! The question now stands: which pond will you catch more salmon at?"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#a-fishy-situation",
    "href": "posts/poisson_metrop/index.html#a-fishy-situation",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "",
    "text": "Allow me to paint a picture:\nYou’re an avid salmon fisherman and statistician. You’ve fished 100 ponds in the surrounding area and have recorded the hourly rate at which you catch salmon while on the water. During your time on the water you also record a couple of measurable variables for each pond: pond depth and dissolved oxygen.\nNow, your friend Danny wants go fishing with you this week and suggests two ponds you haven’t fished a before. Also, since Danny is a good friend, they have kindly measured both depth and dissolved oxygen of each pond. Very thoughtful! The question now stands: which pond will you catch more salmon at?"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#data-generation",
    "href": "posts/poisson_metrop/index.html#data-generation",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "Data generation",
    "text": "Data generation\nLets assume that the number of salmon caught \\(Y\\) is poisson distributed with rate \\(\\lambda\\). \\[Y \\sim Poi(\\lambda)\\]\nLet us also assume that the log rate of catching salmon is a linear function of the two pond variables:\n\\[\\log(\\lambda) =  \\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0\\] Finally, we may write the probability of catching \\(y\\) salmon during one hour of fishing given the above parameters:\n\\[P(y|x_i;\\beta_{depth},\\beta_{oxy},\\beta_0)=\\frac{\\lambda^y}{y!}e^{-\\lambda}, \\hspace{4mm} \\lambda=\\exp(\\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0)\\] Now, let’s generate some data!\n\n\nCode\nlibrary(dplyr)\nlibrary(rstan)\n\n\n\n# Function to standardize data\nstandardize &lt;- function(x){\n  return((x - mean(x)) / sd(x))\n}\nset.seed(1234)\n\nN &lt;- 200 # Number of ponds fished at\nx_oxy &lt;- rnorm(N, 5, 1) # Dissolved oxygen in mg per Liter\nx_depth &lt;- abs(rnorm(N, 30, 10)) # Pond depth\n\nb_oxy &lt;- 0.8\nb_depth &lt;- -0.6\nb_int &lt;- 0.4\nlambda &lt;- exp(b_oxy*standardize(x_oxy) + b_depth*standardize(x_depth) + b_int)\ny &lt;- rpois(length(lambda), lambda = lambda)\n\npar(mfrow=c(1,2))\nplot(x_oxy, y, pch=16, col='darkblue', ylab='Fish Caught Per Hour', xlab='Dissolved Oxygen (mg/L)')\nplot(x_depth, y, pch=16, col='darkred', ylab='Fish Caught Per Hour', xlab='Pond Depth (m)')"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#mcmc-with-metropolis-hastings",
    "href": "posts/poisson_metrop/index.html#mcmc-with-metropolis-hastings",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "MCMC with Metropolis Hastings",
    "text": "MCMC with Metropolis Hastings\n\nTheory\n\nModel Definition\nFirst, we will assume that the parameters \\(\\beta_{depth}\\), \\(\\beta_{oxy}\\), and \\(\\beta_0\\) are independent of each other such that the joint probability distribution for the parameters can be expressed as:\n\\[P(\\beta_{depth}, \\beta_{oxy}, \\beta_0 ) = P(\\beta_{depth} ) \\cdot P(\\beta_{oxy})\\cdot P(\\beta_0)\\]\nNow, observing the data from the plots above, we can assume that \\(\\beta_{depth}\\) will be positive and \\(\\beta_{oxy}\\) to be negative. Thus, we can set out priors to reflect this observation:\n\\[\\begin{align}\nY &\\sim Poi(\\lambda) \\\\\n\\log(\\lambda) &=  \\beta_{depth}\\cdot x_{depth} + \\beta_{oxy}\\cdot x_{oxy} + \\beta_0 \\\\\n\\beta_{depth} &\\sim N(0.5, 0.5) \\\\\n\\beta_{oxy} &\\sim N(-0.5,0.5) \\\\\n\\beta_0 &\\sim N(0, 0.5)\n\\end{align}\\]\nLet’s now define our likelihood and priors in R:\n\nlog_poi_liklihood &lt;- function(params){\n  lam_cands &lt;- exp(params[['b_oxy_cand']]*standardize(x_oxy) + \n                     params[['b_depth_cand']]*standardize(x_depth) +\n                     params[['b_int_cand']])\n  return(sum(dpois(y, lam_cands, log=T)))\n}\n\nlog_prior &lt;- function(params){\n  return(\n    dnorm(params[['b_oxy_cand']], 0.5, 0.5, log=T) +\n    dnorm(params[['b_depth_cand']], -0.5, 0.5, log=T) +\n    dnorm(params[['b_int_cand']], 0, 0.5, log=T)\n  )\n}\n\nlog_posterior_prob &lt;- function(params){\n  return(log_prior(params) + log_poi_liklihood(params))\n}\n\nWhile defining our priors it is always good practice to make sure that they make sense by checking the prior predictive distribution. We can do this by taking some samples from our prior distributions and using them in place of our model.\n\nn_prior_samples &lt;- 1e3\nsample_priors &lt;- cbind(rnorm(n_prior_samples, 0.5, 0.5),\n                       rnorm(n_prior_samples, -0.5, 0.5),\n                       rnorm(n_prior_samples, 0,0.5))\n\nprior_predicitive &lt;- cbind(standardize(x_oxy), standardize(x_depth)) %&gt;% apply(., 1, function(x) \n  rpois(n=n_prior_samples, exp(x[1]*sample_priors[,1] + x[2]*sample_priors[,2] + sample_priors[,3]) ))\n\nhist(prior_predicitive %&gt;% log(),prob=T,ylim=c(0,0.8), col=adjustcolor('darkblue', alpha.f = 0.5),\n     main='Log of Prior Predictive and y', xlab='', ylab='')\nhist(y %&gt;% log(), prob=T, add=T, col=adjustcolor('darkred', alpha.f = 0.5))\nlegend('topright', c('Prior Predidictive','y'), fill=c(adjustcolor('darkblue', alpha.f = 0.5),\n                                                       adjustcolor('darkred', alpha.f = 0.5)))\n\n\n\n\n\n\n\n\n\n\nMCMC\nMetropolis Hastings is rejection sampler defined as the following.\nSuppose we have some sampled value \\(x_t\\) and some function \\(P(x)\\) the returns the probability of a given \\(x\\). We also have some proposal function that generates a new \\(x_{t+1}\\) given a previously sampled \\(x_t\\) defined by \\(g(x_{t+1}|x_t)\\).\nNow, we need some way to “reject” or “accept” some newly generated \\(x_{t+1}\\) value from our function \\(g\\). Define this probability of acceptance to be\n\\[a=\\frac{P(x_{t+1})g(x_t|x_{t+1})}{P(x_t)g(x_{t+1}|x_t)}\\] Usually (and for our case today), we’ll choose a function \\(g\\) such that \\(g\\) is symmetric, or \\(g(x_t|x_{t+1})=g(x_{t+1}|x_t)\\). A common choice to achieve this property would be to assume \\(g\\) is normal with mean equal to the given point. In other words \\[g(x_t|x_{t+1})\\sim N(x_{t+1},\\sigma)\\]\nNote, here \\(P(x)\\) will be the probability of our sampled \\(\\{\\beta_{depth}, \\beta_{oxy}, \\beta_0 \\}\\) given our data, as that is the posterior we which to rejection sample from.\nNow, let’s write our MCMC algorithm and sample from our posterior! We run 4 different chains to get the best estimate of our posterior.\n\nN_sim &lt;- 5e4\nN_chains &lt;- 4\n\nmcmc_chain &lt;- function(N_sim, explore_param){\n  curr_params &lt;-  list(\n    b_oxy_cand = rnorm(1, 0, 4),\n    b_depth_cand = rnorm(1, 0, 4),\n    b_int_cand = rnorm(1, 0, 4)\n  )\n  chain &lt;- matrix(NA, nrow=N_sim, ncol=3)\n  for (i in 1:N_sim){\n    cand_params &lt;- list(\n      b_oxy_cand = rnorm(1, curr_params[['b_oxy_cand']], explore_param),\n      b_depth_cand = rnorm(1, curr_params[['b_depth_cand']], explore_param),\n      b_int_cand = rnorm(1, curr_params[['b_int_cand']], explore_param)\n    )\n    a &lt;- min(1, exp(log_posterior_prob(cand_params) - \n                      log_posterior_prob(curr_params)))\n    u &lt;- runif(1)\n    if (u &lt;= a){\n      chain[i,] &lt;- unlist(cand_params)\n      curr_params &lt;- cand_params\n    }\n    else{\n      chain[i,] &lt;- unlist(curr_params)\n    }\n  }\n  return(chain)\n}\n\nsimulation &lt;- list()\nfor (i in 1:N_chains){\n  simulation[[paste0('chain_',i)]] &lt;- mcmc_chain(N_sim, explore_param = 0.01)\n}\n\nburn &lt;- 1e4\n\n\n\n\nAnalyzing our chains\nLet’s see how well our posteriors match the actual values:\n\n\nCode\npar(mfrow=c(1,3))\nposterior_oxy &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),1])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_oxy %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Dissolved Oxygen')\npolygon(posterior_oxy %&gt;% density(), col=adjustcolor('darkgreen', 0.5))\nabline(v=0.8, col='red', lwd=4)\n\n\nposterior_depth &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),2])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_depth %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Depth')\npolygon(posterior_depth %&gt;% density(), col=adjustcolor('darkblue', 0.5))\nabline(v=-0.6, col='red', lwd=4)\n\n\nposterior_int &lt;- lapply(simulation, function(x) return(x[-seq(1,burn),3])) %&gt;% \n  unlist() %&gt;% unname()\nplot(posterior_int %&gt;% density(), lwd = 4, xlab='', ylab='', main='Posterior for Beta_0')\npolygon(posterior_int %&gt;% density(), col=adjustcolor('darkred', 0.5))\nabline(v=0.4, col='red', lwd=4)\n\n\n\n\n\n\n\n\n\nFocusing on \\(\\beta_{oxy}\\), let’s see how well our chain converged using rank plots:\n\na &lt;- simulation %&gt;% lapply(., function(x) x[-seq(1,burn),1]) %&gt;% unlist() %&gt;% rank() %&gt;% \n  matrix(., ncol=4)\npar(mfrow=c(2,2))\nfor (i in 1:4) hist(a[,i], col=adjustcolor('darkblue', alpha.f = 0.5), main=paste0('Chain_',i),\n                    xlab='', ylab='')\n\n\n\n\n\n\n\n\nRank plots are calculated by combining all our MCMC samples and finding each samples respective rank. The resulting ranks are separated back into their respective chains and plotted as histograms. If the MCMC sampler converged and the chains mixed well without a high degree of autocorrelation, we can expected uniform distributions for each rank plot.\nThere a couple of metrics we can look at to assess the convergence of our MCMC sampling. One main metric is \\(\\hat{R}\\). It tells us how well all our chains converged and mixed. A good rule of thumb is to have \\(\\hat{R}\\) under 1.05.\nThe other metrics have to do with effective sample size (ESS). In MCMC sampling, we are assuming a level of independence for samples not directly adjacent. In other words, we are hoping for a low degree of autocorrelation. Simply put, if we have a high degree of autocorrelation in our samples then we effectively have less information describing our posterior. This is what ESS measures, the degree of autocorrelation in our chains. The first, bulk-ESS tells us how well the center or bulk of our posterior has been sampled. The second is tail-ESS, which tells us how well our posterior tails were sampled. A good rule of thumb is to have a bulk-ESS and tail-ESS greater than 400.\n\nmetric_mat &lt;- matrix(NA, nrow=3, ncol=3)\nfor (i in 1:3){\n  metric_mat[1,i] &lt;- Rhat(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                            as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,2)\n  metric_mat[2,i] &lt;- ess_bulk(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                                as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,1)\n  metric_mat[3,i] &lt;- ess_tail(simulation %&gt;% lapply(., function(x) x[-seq(1,burn), i]) %&gt;% \n                                as.data.frame() %&gt;% as.matrix()) %&gt;% round(.,1)\n}\ncolnames(metric_mat) &lt;- c('b_oxy', 'b_depth', 'b_0')\nrow.names(metric_mat) &lt;- c('r_hat', 'bulk_ess', 'tail_ess')\nknitr::kable(metric_mat, align = 'ccc')\n\n\n\n\n\nb_oxy\nb_depth\nb_0\n\n\n\n\nr_hat\n1.0\n1.0\n1.0\n\n\nbulk_ess\n1359.1\n1168.9\n834.9\n\n\ntail_ess\n2638.3\n2752.3\n1573.2\n\n\n\n\n\nA good measure to determine how well model fits our data is to plot the posterior predictive against the observed data.\n\nposterior_predictive &lt;- cbind(standardize(x_oxy), standardize(x_depth)) %&gt;% apply(., 1, function(x) \n  rpois(n=n_prior_samples, exp(x[1]*posterior_oxy + x[2]*posterior_depth + posterior_int))) %&gt;% c()\n\nhist(posterior_predictive,prob=T, col=adjustcolor('darkblue', alpha.f = 0.5),\n     breaks=length(unique(posterior_predictive)),\n     main='Posterior Predictive and y', ylab='', xlab='')\n\nhist(y, prob=T, add=T, col=adjustcolor('darkred', alpha.f = 0.5), breaks=34)\nlegend('topright', c('Posterior Predictive','y'), fill=c(adjustcolor('darkblue', alpha.f = 0.5),\n                                                       adjustcolor('darkred', alpha.f = 0.5)))"
  },
  {
    "objectID": "posts/poisson_metrop/index.html#decision-analysis",
    "href": "posts/poisson_metrop/index.html#decision-analysis",
    "title": "Bayesian Decision Analysis with Poisson Regression and Metropolis-Hastings",
    "section": "Decision Analysis",
    "text": "Decision Analysis\nBack to the question at hand: what pond should you fish at? Let’s say your friend Danny has the following measurements for the two ponds in question.\n\nponds &lt;- matrix(c(8, 7, 40, 20), ncol=2) %&gt;% as.data.frame()\ncolnames(ponds) &lt;- c('Dissolved Oxygen', 'Pond Depth')\nrow.names(ponds) &lt;- c('Pond A', 'Pond B')\nknitr::kable(ponds, align='ccc')\n\n\n\n\n\nDissolved Oxygen\nPond Depth\n\n\n\n\nPond A\n8\n40\n\n\nPond B\n7\n20\n\n\n\n\n\nFrom our posterior samples, we can obtain distributions representing our uncertainty for the fish we will catch at each of the ponds in question.\n\npar(mfrow=c(1,2))\npond_a &lt;- ((ponds$`Dissolved Oxygen`[1] - mean(x_oxy)) / sd(x_oxy)) * posterior_oxy + \n  ((ponds$`Pond Depth`[1] - mean(x_depth)) / sd(x_depth)) * posterior_depth + posterior_int\npond_a &lt;- rpois(length(pond_a), exp(pond_a))\nhist(pond_a, breaks=length(unique(pond_a)), prob=T, col=adjustcolor('darkgreen', alpha.f = 0.5),\n     main='Pond A', xlab='Fish Caught', ylab='')\n\npond_b &lt;- ((ponds$`Dissolved Oxygen`[2] - mean(x_oxy)) / sd(x_oxy)) * posterior_oxy + \n  ((ponds$`Pond Depth`[2] - mean(x_depth)) / sd(x_depth)) * posterior_depth + posterior_int\npond_b &lt;- rpois(length(pond_b), exp(pond_b))\nhist(pond_b, breaks=length(unique(pond_b)), prob=T, col=adjustcolor('yellow', alpha.f = 0.5),\n     main='Pond B', xlab='Fish Caught', ylab='')\n\n\n\n\n\n\n\n\nNow, we can take the difference of the two distributions and come to our conclusion.\n\npond_diff &lt;- (pond_b - pond_a)\nhist(pond_diff, prob=T, main='Pond B - Pond A', xlab='Difference in fish caught between ponds', ylab='',\n      col=adjustcolor('purple', alpha.f = 0.5))\n\n\n\n\n\n\n\n\nIf we want to find the expected increase in fish in choosing pond B of pond A, it’s as simple as taking the average of our above distribution.\n\nmean(pond_b - pond_a)\n\n[1] 3.11315\n\n\nHence, after our extensive analysis we can come to the conclusion that it is best to choose Pond B over A. Although, maybe your friend Danny has left by now!\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] rstan_2.32.6        StanHeaders_2.32.10 dplyr_1.1.4        \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_1.8.9     compiler_4.4.2     tidyselect_1.2.1  \n [5] Rcpp_1.0.13-1      parallel_4.4.2     gridExtra_2.3      scales_1.3.0      \n [9] yaml_2.3.10        fastmap_1.2.0      ggplot2_3.5.1      R6_2.5.1          \n[13] generics_0.1.3     knitr_1.49         htmlwidgets_1.6.4  tibble_3.2.1      \n[17] munsell_0.5.1      pillar_1.9.0       rlang_1.1.4        utf8_1.2.4        \n[21] inline_0.3.20      xfun_0.49          RcppParallel_5.1.9 cli_3.6.3         \n[25] magrittr_2.0.3     digest_0.6.37      grid_4.4.2         rstudioapi_0.17.1 \n[29] lifecycle_1.0.4    vctrs_0.6.5        evaluate_1.0.1     glue_1.8.0        \n[33] QuickJSR_1.4.0     codetools_0.2-20   stats4_4.4.2       pkgbuild_1.4.5    \n[37] fansi_1.0.6        colorspace_2.1-1   rmarkdown_2.29     matrixStats_1.4.1 \n[41] tools_4.4.2        loo_2.8.0          pkgconfig_2.0.3    htmltools_0.5.8.1"
  },
  {
    "objectID": "posts/tfjs/index.html",
    "href": "posts/tfjs/index.html",
    "title": "Test",
    "section": "",
    "text": "TF JS Simple Example\n\n\n\n\n\n\n\n\n\n\n\nSimple TF JS Webapp"
  },
  {
    "objectID": "posts/polychoric/index.html",
    "href": "posts/polychoric/index.html",
    "title": "Polychoric Correltion with Likert Data",
    "section": "",
    "text": "Look at you, you successful businessperson you! You own a company that sells two products: A and B. You run a short two question survey to determine whether your customers would recommend either products. For this example we’ll assume that all customers are using both products. The question style is the commonly used “Net Promoter Score” or likert scale format:\n\n“Provide your level of agreement to the following question: I would recommend this product to a friend”\n\nWhere the available choices are:\n\nStrongly Disagree\nSomewhat Disagree\nNeither Agree or Disagree\nSomewhat Agree\nStrongly Agree\n\nThere are of course limitations to this kind of survey design. For one, most people have a hard time discretizing their feelings or emotions into a single bucket. Perhaps the more appropriate question response would feature a slider that allows respondents to freely select their agreement on a continuous scale. Regardless, as this is the design chosen by thousands of companies and organizations, we’ll choose it as well. Though, we’ll recognize that agreement or sentiment in general is better categorized as a spectrum.\nEnough philosophy, now to the actual data. I’m going to show how the data is generated further down, but for now let’s say that we ran the survey and collected 1,000 responses. First, let’s start by loading in all the packages we’ll need for this analysis.\n\n\nCode\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggridges)\n\noptions(mc.cores=4)\n\nset.seed(1234)\n\nsigma &lt;- c(1,0.5,0.5,1) %&gt;% matrix(.,nrow=2,byrow=T)\ndat &lt;- mvtnorm::rmvnorm(500, sigma=sigma)\n\nc_points &lt;- list(\n  c(-2.69, -0.68, 0.07, 1.13),\n  c(-1.29, 0.62, 1.38, 1.65)\n)\n\ndiscrete_data &lt;- dat %&gt;% apply(., 1, \n                               \\(x) c(\n                                 sum(x[1] &gt; c_points[[1]]) + 1,\n                                 sum(x[2] &gt; c_points[[2]]) + 1)\n                               ) %&gt;% t()\n\n\nNext, we’ll take our discrete_data data frame which holds our survey responses and visualize it as a table of all unique responses. For example, the third row and second column will be the number of customers that responded 3 to question 1 and 2 to question 2.\n\ntable(discrete_data[,1], discrete_data[,2]) %&gt;%\n  kbl(row.names=1) %&gt;%\n  kable_styling() %&gt;%\n  column_spec(column = 1, bold=T)\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\n1\n2\n0\n0\n0\n\n\n2\n25\n102\n5\n0\n0\n\n\n3\n9\n108\n16\n3\n3\n\n\n4\n4\n96\n46\n9\n10\n\n\n5\n0\n29\n17\n5\n10\n\n\n\n\n\n\n\nFrom the table above, we can already see that there is a high degree of positive correlation between the questions. If we wanted to quantifying this correlation, we might naively use the cor function, but this produces biased results as our provided data is not continuous, which is assumed by the default Pearson correlation measure. There are other measures of correlation such as Spearman or Kendall which are non-parametric, but neither take into account the data generating process that aligns with our philosophy. For that, we will need to employ the polychoric correlation which we will further define below."
  },
  {
    "objectID": "posts/polychoric/index.html#motivation-its-likely-likert",
    "href": "posts/polychoric/index.html#motivation-its-likely-likert",
    "title": "Polychoric Correltion with Likert Data",
    "section": "",
    "text": "Look at you, you successful businessperson you! You own a company that sells two products: A and B. You run a short two question survey to determine whether your customers would recommend either products. For this example we’ll assume that all customers are using both products. The question style is the commonly used “Net Promoter Score” or likert scale format:\n\n“Provide your level of agreement to the following question: I would recommend this product to a friend”\n\nWhere the available choices are:\n\nStrongly Disagree\nSomewhat Disagree\nNeither Agree or Disagree\nSomewhat Agree\nStrongly Agree\n\nThere are of course limitations to this kind of survey design. For one, most people have a hard time discretizing their feelings or emotions into a single bucket. Perhaps the more appropriate question response would feature a slider that allows respondents to freely select their agreement on a continuous scale. Regardless, as this is the design chosen by thousands of companies and organizations, we’ll choose it as well. Though, we’ll recognize that agreement or sentiment in general is better categorized as a spectrum.\nEnough philosophy, now to the actual data. I’m going to show how the data is generated further down, but for now let’s say that we ran the survey and collected 1,000 responses. First, let’s start by loading in all the packages we’ll need for this analysis.\n\n\nCode\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggridges)\n\noptions(mc.cores=4)\n\nset.seed(1234)\n\nsigma &lt;- c(1,0.5,0.5,1) %&gt;% matrix(.,nrow=2,byrow=T)\ndat &lt;- mvtnorm::rmvnorm(500, sigma=sigma)\n\nc_points &lt;- list(\n  c(-2.69, -0.68, 0.07, 1.13),\n  c(-1.29, 0.62, 1.38, 1.65)\n)\n\ndiscrete_data &lt;- dat %&gt;% apply(., 1, \n                               \\(x) c(\n                                 sum(x[1] &gt; c_points[[1]]) + 1,\n                                 sum(x[2] &gt; c_points[[2]]) + 1)\n                               ) %&gt;% t()\n\n\nNext, we’ll take our discrete_data data frame which holds our survey responses and visualize it as a table of all unique responses. For example, the third row and second column will be the number of customers that responded 3 to question 1 and 2 to question 2.\n\ntable(discrete_data[,1], discrete_data[,2]) %&gt;%\n  kbl(row.names=1) %&gt;%\n  kable_styling() %&gt;%\n  column_spec(column = 1, bold=T)\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\n1\n2\n0\n0\n0\n\n\n2\n25\n102\n5\n0\n0\n\n\n3\n9\n108\n16\n3\n3\n\n\n4\n4\n96\n46\n9\n10\n\n\n5\n0\n29\n17\n5\n10\n\n\n\n\n\n\n\nFrom the table above, we can already see that there is a high degree of positive correlation between the questions. If we wanted to quantifying this correlation, we might naively use the cor function, but this produces biased results as our provided data is not continuous, which is assumed by the default Pearson correlation measure. There are other measures of correlation such as Spearman or Kendall which are non-parametric, but neither take into account the data generating process that aligns with our philosophy. For that, we will need to employ the polychoric correlation which we will further define below."
  },
  {
    "objectID": "posts/polychoric/index.html#data-generating-process",
    "href": "posts/polychoric/index.html#data-generating-process",
    "title": "Polychoric Correltion with Likert Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\nWe assumed that our data was generated by customers that were forced to discretize their agreement with a given question. If we wanted to properly model this, we might want to assume that our data is first generated from some latent multivariate continuous distribution, and then discretized using a set of p - 1 “cut-points” for each dimension of the latent space, where p is the number of possible choices in the questionnaire. The code to generate data from this process is shown below:\n\nsigma &lt;- c(1,0.5,0.5,1) %&gt;% matrix(.,nrow=2,byrow=T)\ndat &lt;- mvtnorm::rmvnorm(500, sigma=sigma)\n\nc_points &lt;- list(\n  c(-2.69, -0.68, 0.07, 1.13),\n  c(-1.29, 0.62, 1.38, 1.65)\n)\n\ndiscrete_data &lt;- dat %&gt;% apply(., 1, \n                               \\(x) c(\n                                 sum(x[1] &gt; c_points[[1]]) + 1,\n                                 sum(x[2] &gt; c_points[[2]]) + 1)\n                               ) %&gt;% t()\n\nWe use a bivariate gaussian as our latent distribution with a mean of 0 and variance, or equivalently in this case, a correlation matrix of\n\\[\n\\begin{bmatrix}\n1 & 0.5\\\\\n0.5 & 1\n\\end{bmatrix}\n\\] If we plot the latent variables with the cut-points we obtain:\n\nplot(dat,pch=16, xlab = 'Question 1', ylab='Question 2', main='Latent Space Representation')\nfor (i in 1:length(c_points[[1]])){\n  abline(v=c_points[[1]][i], lwd=2, col='darkred')\n  abline(h=c_points[[2]][i], lwd=2, col='darkblue')\n}\n\n\n\n\n\n\n\n\nSo a customer that answered 3 for the first question and 2 for the second question would have had their latent representation in the below region:\n\n\nCode\nplot(dat, pch=16, xlab = 'Question 1', ylab='Question 2', main='Latent Space Representation')\npolygon(x=c(c_points[[1]][2], c_points[[1]][3], c_points[[1]][3], c_points[[1]][2]),\n        y=c(c_points[[2]][1], c_points[[2]][1], c_points[[2]][2], c_points[[2]][2]),\n        col=adjustcolor('purple', 0.5))\nfor (i in 1:length(c_points[[1]])){\n  abline(v=c_points[[1]][i], lwd=2, col='darkred')\n  abline(h=c_points[[2]][i], lwd=2, col='darkblue')\n}"
  },
  {
    "objectID": "posts/polychoric/index.html#modeling",
    "href": "posts/polychoric/index.html#modeling",
    "title": "Polychoric Correltion with Likert Data",
    "section": "Modeling",
    "text": "Modeling\n\nPriors and Likelihood\nOur model is almost fully specified with the data generating process outlined above, but we still need to incorporate our priors. For the correlation matrix, we will reparametrize using the Cholesky decomposition and use a LKJ prior. The cut-points are a little trickier, but notice that the marginals of our latent distribution are standard normals. We can use this to our advantage by reparametrizing the cut-points as a vector of probabilities where each entry is the probability allocated to the interval on the standard normal distribution between two adjacent cut-points \\(c_i\\) and \\(c_j\\). Note that for \\(p-1\\) cutpoints, there will be \\(p\\) entries in our probability vector. Thus, we can write:\n\\[\n\\begin{align}\n\\begin{bmatrix}\nc_1\\\\\nc_2 \\\\\n\\vdots\\\\\nc_{p-1}\n\\end{bmatrix}\n& \\rightarrow \\begin{bmatrix}\n\\theta_1\\\\\n\\theta_2\\\\\n\\vdots\\\\\n\\theta_{p}\n\\end{bmatrix} \\\\ \\\\\n&= \\begin{bmatrix}\n\\Phi(c_1) \\\\\n\\Phi(c_2) - \\Phi(c_1) ] \\\\\n\\vdots \\\\\n1 - \\Phi(c_{p-1})\n\\end{bmatrix}\n\\end{align}\n\\] Since our probability interval vector must sum to one, we can use a dirichlet distribution as the prior. The stan code to specify this prior and perform the reparametrization is below:\nreal induced_dirichlet_lpdf(vector c, vector alpha, real gamma){\n    int K = num_elements(c) + 1;\n    vector[K - 1] cuml = Phi(c - gamma);\n    vector[K] p;\n    matrix[K,K] J = rep_matrix(0,K,K);\n\n    p[1] = cuml[1];\n    for (k in 2:(K-1)){\n        p[k] = cuml[k] - cuml[k-1];\n    }\n    p[K] = 1 - cuml[K-1];\n\n    for (k in 1:K) J[k,1] = 1;\n\n    for (k in 2:K){\n        real rho = exp(std_normal_lpdf(c[k-1] - gamma));\n        J[k,k] = -rho;\n        J[k - 1, k] = rho;\n    }\n    return dirichlet_lpdf(p | alpha) + log_determinant(J);\n}\nThe stan code is a bit more involved, and includes the Jacobian calculations since we are specifying a prior on the transformed parameters. For more detail about the reparametrization and Jacobian calculations I suggest reading Michael Betancourt’s ordinal regression tutorial. Be aware that his model uses a latent logistic distribution (different than our standard normal).\nFinally, we need to consider the likelihood of the data, conditioned on our parameters. To model this, we will extend the multivariate probit model to the case where an arbitrary number of ordinal categories are observed. Without going into too much detail, the multivariate probit is used to model a bernoulli distributed random vector, where the data is assumed to have been generated from a latent multivariate normal distribution. For example, if you consider our data generating process above but instead only have one cut-point per dimension, then the data generated would be a bernoulli random vector. The stan code used to define this extenstion of the multivariate probit likelihood is here, along with the full stan code for the model.\nThe full derivation for the likelihood, and therefore stan code, is beyond the scope of this blog post, but I refer you to these two other resources to learn more if you are interested:\n\nBen Goodrich’s Truncated Normal Sampler in STAN\nGHK Algorithm, which is what Ben’s implementation is based on\n\nThe parameter, model, and generated quantities block is shown below:\nparameters {\n  cholesky_factor_corr[D] L_Omega;\n  array[N,D] real&lt;lower=0, upper=1&gt; u;\n  array[D] ordered[n_cut] c_points;\n}\nmodel {\n    L_Omega ~ lkj_corr_cholesky(4);\n    for (d in 1:D) target += induced_dirichlet_lpdf(c_points[d] | rep_vector(1, n_cut + 1), 0);\n\n    for (n in 1:N) target += trunc_norm(y[n], c_points, L_Omega, u[n], D, y_min, y_max);\n}\ngenerated quantities {\n   corr_matrix[D] Omega;\n   Omega = multiply_lower_tri_self_transpose(L_Omega);\n}\nThe parameters of interest here are L_Omega, which will give us the correlation matrix for our latent gaussian and the c_points array which determines the cut-points that generated our data. Ignore the u parameter, as it is a nuisance parameter and is only used to help define the likelihood of our data.\n\n\nSampling and Posterior Exploration\nNow that are model is fully defined, we can used the cmdstanr package to sample our posterior. The full stan code can found here. Note, during model fitting we are expecting some rejected samples due to the highly constrained values of the correlation matrix. When fitting this model with a higher dimension latent\n\n# fp &lt;- file.path('PATH TO YOUR STAN MODEL CODE')\nmod &lt;- cmdstan_model(fp)\ndata &lt;- list(\n  D = ncol(discrete_data),\n  N = nrow(discrete_data),\n  y = discrete_data,\n  y_min = min(discrete_data),\n  y_max = max(discrete_data)\n)\n\npoly_cor &lt;- mod$sample(data = data, seed = 1234, chains = 4, parallel_chains = 2,\n                       iter_warmup = 2000,iter_sampling = 2000)\n\nLet’s take a look out some diagnotics to make sure we had adequate posterior sampling:\n\npoly_cor$summary(c(paste0('c_points[1,', 1:4,']'), paste0('c_points[2,', 1:4,']'), 'Omega[2,1]')) %&gt;%\n  select(-c(median, q5, q95, mad)) %&gt;%\n  map(., \\(x) if(is.character(x)) x else round(x,2)) %&gt;% as.data.frame() %&gt;%\n  kbl() %&gt;% kable_styling()\n\n\n\n\nvariable\nmean\nsd\nrhat\ness_bulk\ness_tail\n\n\n\n\nc_points[1,1]\n-2.44\n0.19\n1\n9841.39\n5775.21\n\n\nc_points[1,2]\n-0.70\n0.06\n1\n9717.82\n7102.99\n\n\nc_points[1,3]\n-0.02\n0.06\n1\n9485.35\n7512.77\n\n\nc_points[1,4]\n1.04\n0.07\n1\n11822.11\n6832.92\n\n\nc_points[2,1]\n-1.36\n0.08\n1\n12008.87\n6858.70\n\n\nc_points[2,2]\n0.66\n0.06\n1\n9789.17\n6537.07\n\n\nc_points[2,3]\n1.44\n0.08\n1\n10790.98\n6061.38\n\n\nc_points[2,4]\n1.71\n0.10\n1\n12190.28\n6611.51\n\n\nOmega[2,1]\n0.51\n0.04\n1\n10775.66\n6575.90\n\n\n\n\n\n\n\nOur Rhat, ess_bulk, and ess_tail look good! Let’s take a look at our posteriors for the c_points parameter. The ggplot code that adds line segments to the ridge plot is adapted from this stackoverflow post.\n\n# Function to extract Cut Points from stan model\nget_cut_point &lt;- function(dim, n){\n  c_name &lt;- paste0('c_points[', dim, ',', n, ']')\n  poly_cor$draws(c_name) %&gt;% \n    .[,,1,drop=T] %&gt;%\n    as.vector() %&gt;% as.data.frame() %&gt;%\n    cbind(., c_name, dim)\n}\n\ncut_point_draws &lt;- lapply(4:1, \\(x) get_cut_point(1,x)) %&gt;% \n  append(., lapply(4:1, \\(x) get_cut_point(2,x))) %&gt;%\n  do.call('rbind', .) %&gt;% set_names(c('value', 'name', 'dim')) %&gt;% \n  mutate(dim=as.factor(dim))\n\n# Plot the Cut Point Ridge plot\ncomp &lt;- ggplot(cut_point_draws, aes(x=value, y=name)) + geom_density_ridges()\ningredients &lt;- ggplot_build(comp) %&gt;% purrr::pluck(\"data\", 1)\n\ndensity_lines &lt;- ingredients %&gt;% group_by(group) %&gt;% \n  mutate(a = c_points[[floor(max(group)/5) + 1]][[((max(group) - 1) %% 4) + 1]]) %&gt;% \n  slice(which.min(abs(x-a)))\n\nggplot(cut_point_draws, aes(x=value, y=name)) + \n  geom_density_ridges(aes(fill=dim), rel_min_height = 0.01, lwd=1.2, alpha=0.3) +\n  scale_x_continuous(breaks = seq(-3.5,2.5,0.5),limits = c(-3.5,2), expand=c(0,0)) +\n  geom_segment(data = density_lines, col='darkred',lwd=1.2,\n               aes(x = x, y = ymin, xend = x, \n                   yend = ymin+density*scale*iscale, linetype='Actual Cut-Point Value')) +\n  scale_linetype_manual(\"\",values=c('Actual Cut-Point Value'=1)) +\n  scale_fill_manual(\"Cut-point Dimension\", \n                    values=c('darkblue', 'darkred', 'yellow'), \n                    labels=c('Dim. 1', 'Dim. 2')) +\n  labs(title='Cut Point Parameter Estimates') +\n  theme_ridges() + \n  theme(axis.title.y = element_blank(), axis.title.x = element_blank(), \n        axis.text = element_text(size=20), plot.title = element_text(size=30),\n        legend.text = element_text(size=20), legend.title = element_text(size=30))\n\n\n\n\n\n\n\n\nIt looks like the actual value of each cut-point is captured within the range of each estimated parameter. Now we can look at our originally requested quantity; the correlation.\n\npoly_cor$draws('Omega[2,1]') %&gt;% .[,,1,drop=T] %&gt;% as.vector() %&gt;% \n  {as.data.frame(list(x=.))} %&gt;%\n  ggplot(aes(x=x)) + geom_histogram(aes(y=..density..), fill='darkblue', col='black', alpha=0.3) + \n  geom_density(lwd=1.3) +     \n  geom_vline(aes(xintercept=0.5, color='Actual Value'), size=1.5, linetype='twodash') +\n  scale_colour_manual(\"\", values=\"darkred\") +\n  labs(title='Correlation Parameter Estimate') +\n  theme_minimal() +\n  theme(axis.title.y = element_blank(), axis.title.x = element_blank(), legend.text = element_text(size=13),\n        axis.text = element_text(size=13), plot.title = element_text(size=17))"
  },
  {
    "objectID": "posts/polychoric/index.html#conclusion",
    "href": "posts/polychoric/index.html#conclusion",
    "title": "Polychoric Correltion with Likert Data",
    "section": "Conclusion",
    "text": "Conclusion\nIt looks like our model fits the data well and is able to adequately identify the parameters. While the polychoric correlation is a little more involved than a simple pearson correlation, it aligns more with our original data generating process philosophy. Since this model was fit using a bayesian framework we have samples from our posterior which we can use to perform decision analysis, generating pseudocustomers, or probabilistic PCA from the posterior of the correlation matrix.\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggridges_0.5.6   kableExtra_1.4.0 knitr_1.49       cmdstanr_0.8.1  \n [5] lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4     \n [9] purrr_1.0.2      readr_2.1.5      tidyr_1.3.1      tibble_3.2.1    \n[13] ggplot2_3.5.1    tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] tensorA_0.36.2.1     utf8_1.2.4           generics_0.1.3      \n [4] xml2_1.3.6           stringi_1.8.4        hms_1.1.3           \n [7] digest_0.6.37        magrittr_2.0.3       evaluate_1.0.1      \n[10] grid_4.4.2           timechange_0.3.0     mvtnorm_1.3-2       \n[13] fastmap_1.2.0        jsonlite_1.8.9       processx_3.8.4      \n[16] backports_1.5.0      ps_1.8.1             fansi_1.0.6         \n[19] viridisLite_0.4.2    scales_1.3.0         abind_1.4-8         \n[22] cli_3.6.3            rlang_1.1.4          munsell_0.5.1       \n[25] withr_3.0.2          yaml_2.3.10          tools_4.4.2         \n[28] tzdb_0.4.0           checkmate_2.3.2      colorspace_2.1-1    \n[31] vctrs_0.6.5          posterior_1.6.0      R6_2.5.1            \n[34] matrixStats_1.4.1    lifecycle_1.0.4      htmlwidgets_1.6.4   \n[37] pkgconfig_2.0.3      pillar_1.9.0         gtable_0.3.6        \n[40] glue_1.8.0           systemfonts_1.1.0    xfun_0.49           \n[43] tidyselect_1.2.1     rstudioapi_0.17.1    farver_2.1.2        \n[46] htmltools_0.5.8.1    labeling_0.4.3       svglite_2.1.3       \n[49] rmarkdown_2.29       compiler_4.4.2       distributional_0.5.0"
  }
]